{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42be5164-710c-4298-bbaf-282e6d3dc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output', 'random',\n",
    "          'unicodedata','sys', 'glob', 'from datetime import datetime', 'from itertools import chain']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc88acd-e0d0-4e41-8fcc-c311be759377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# IMPORT AND CLEAN SCRAPED DATA\n",
    "########################################################################################\n",
    "###############\n",
    "# Set parameters / Functions \n",
    "###############\n",
    "cores =  os.cpu_count() - 10 \n",
    "def import_wrapper(index):\n",
    "    file_path = raw_files[index]\n",
    "    file = pd.read_csv(file_path)\n",
    "    if file_path.find('_time') == -1:\n",
    "        file = file.assign(siren = (match := re.search(r'results(\\d+)\\.csv', file_path)) and match.group(1) or None,\n",
    "                           scrape_method = 'siren')\n",
    "    else:\n",
    "        file = file.assign(scrape_method = 'time')\n",
    "        if file_path.find(\"tm_\")!= -1:\n",
    "            dates = re.findall(r'\\d{8}', file_path)\n",
    "            file = file.assign(application_year = datetime.strptime(str(dates[0]), \"%Y%m%d\").year)\n",
    "    return(file)\n",
    "\n",
    "def clean_with_commas(chunks,index, name_column):\n",
    "    df = chunks[index]\n",
    "    df['name_col'] = df[name_column]\n",
    "    split_names = df['name_col'].str.split(',', expand=True)\n",
    "    split_names.columns = [f'name_col_{i}' for i in range(split_names.shape[1])]\n",
    "    df = pd.concat([df, split_names], axis=1)\n",
    "    \n",
    "    for index in range(len([col for col in df.columns if \"name_col_\" in col])):\n",
    "        col = f\"name_col_{index}\"\n",
    "        df.loc[lambda c: c[col].notna(), col] = (\n",
    "            hf.clean_firm_names(df.loc[lambda c: c[col].notna()],col,False)[col + \"_cleaned\"])\n",
    "    df = df.assign(**{f\"{name_column}_cleaned\": df.filter(like='name_col_').apply(\n",
    "            lambda row: \",\".join(sorted(set(filter(None, row.dropna().astype(str))))), axis=1)})\n",
    "    return(df.filter(like=name_column))\n",
    "\n",
    "###############\n",
    "# Initial Import and Clean Trademarks \n",
    "###############\n",
    "raw_files =  (glob.glob('data/3_IP_data/2_working/tm_time/*') + \n",
    "              glob.glob('data/3_IP_data/2_working/tm_siren/*'))\n",
    "\n",
    "tm_raw = (\n",
    "    ## import all the files \n",
    "    pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "    (delayed(import_wrapper)(index) \n",
    "     for index in range(len(raw_files))), ignore_index = True)\n",
    "    .assign(trademark_type= lambda df: df['ukey'].str.split('|').str[0],\n",
    "            trademark_name = lambda df: df['Mark'].astype(str),\n",
    "            applicant_name = lambda df: df['DEPOSANT'].astype(str),\n",
    "            application_number = lambda df: df['ApplicationNumber'].astype(str),\n",
    "            trademark_status = lambda df: df['MarkCurrentStatusCode'])\n",
    "    [['application_number','trademark_name', 'trademark_type', 'applicant_name',\n",
    "      'trademark_status', 'scrape_method', 'application_year', 'siren' ]]) \n",
    " \n",
    "###############\n",
    "# Initial Import / Clean Patents\n",
    "###############\n",
    "raw_files =  (glob.glob('data/3_IP_data/2_working/patent_time/*')\n",
    "              + glob.glob('data/3_IP_data/2_working/patent_siren/*'))\n",
    "patent_raw = (\n",
    "    ## import all the files \n",
    "    pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "    (delayed(import_wrapper)(index) \n",
    "     for index in range(len(raw_files))), ignore_index = True)\n",
    "    \n",
    "    ## clean \n",
    "    .assign(\n",
    "        application_year = lambda df:pd.to_datetime(df['DEPD'].astype(str), format='%Y%m%d', errors='coerce').dt.year,\n",
    "        applicant_name = lambda df: df['DENE'].apply(str),\n",
    "        type = lambda df: np.where(df['NAT'].str.strip() != \"\", df['NAT'], np.nan),\n",
    "        collection = lambda df: df['PUBN'].str[9:11],\n",
    "        publication_number = lambda df: df['PUBN'].str.extract(r'<doc-number>([0-9]+(?:\\.[0-9]+)?)</doc-number>')[0],\n",
    "        application_number = lambda df: df['DEPN'].str.extract(r'<doc-number>([0-9]+(?:\\.[0-9]+)?)</doc-number>')[0])\n",
    "    .rename(columns={'IPCR': 'ipcr', 'TIT': 'title'})\n",
    "    [['application_number', 'publication_number', 'type', 'collection',\n",
    "      'siren', 'application_year', 'applicant_name', 'title', 'ipcr', 'scrape_method']])\n",
    "\n",
    "###############\n",
    "# Generate Cleaned Versions of the Names  \n",
    "###############\n",
    "names = pd.concat([patent_raw[['applicant_name']], tm_raw[['applicant_name']]], ignore_index = True).drop_duplicates()\n",
    "names_cleaned = (pd.concat(\n",
    "    Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "    (delayed(clean_with_commas)(np.array_split(names,cores), index, 'applicant_name')\n",
    "    for index in range(cores)), ignore_index = True))\n",
    "\n",
    "patent_raw = pd.merge(patent_raw, names_cleaned)\n",
    "tm_raw = pd.merge(tm_raw, names_cleaned)\n",
    "\n",
    "###############\n",
    "# Merge and export  \n",
    "###############\n",
    "wd = 'data/3_IP_data/2_working/'\n",
    "\n",
    "patent_time = patent_raw.loc[patent_raw['scrape_method'].eq('time')].drop('siren',axis = 1)\n",
    "patent_time.to_parquet(wd + \"patent_time_init.parquet\") \n",
    "\n",
    "patent_siren = patent_raw.loc[patent_raw['scrape_method'].eq('siren')]\n",
    "patent_siren.to_parquet(wd + \"patent_siren_init.parquet\") \n",
    "\n",
    "tm_time = tm_raw.loc[tm_raw['scrape_method'].eq('time')].drop('siren',axis = 1)\n",
    "tm_time.to_parquet(wd + \"tm_time_init.parquet\") \n",
    "\n",
    "tm_siren = tm_raw.loc[tm_raw['scrape_method'].eq('siren')].drop('application_year',axis = 1)\n",
    "tm_siren.to_parquet(wd + \"tm_siren_init.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2bd4c-d736-4d25-acbd-28b8a67fa451",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## Retrieve names from Trademarks / Patents for use\n",
    "##############################################################\n",
    "###############\n",
    "# Import / set parameters \n",
    "###############\n",
    "cores =  os.cpu_count() - 10 \n",
    "wd = 'data/3_IP_data/2_working/'\n",
    "tm_siren = pd.read_parquet(wd+ 'tm_siren_init.parquet')\n",
    "tm_time =  pd.read_parquet(wd+ 'tm_time_init.parquet')\n",
    "patent_siren = pd.read_parquet(wd+ 'patent_siren_init.parquet')\n",
    "patent_time = pd.read_parquet(wd+ 'patent_time_init.parquet')\n",
    "\n",
    "###############\n",
    "# Import and Process The Sirens \n",
    "###############\n",
    "#######\n",
    "### prepare the siren numbers from the admin data \n",
    "#######\n",
    "siren_numbers = (\n",
    "    ##import\n",
    "    pd.read_csv('data/3_IP_data/1_raw/1_StockUniteLegaleHistorique_utf8.csv',\n",
    "                usecols=['denominationUniteLegale', 'siren', 'dateDebut', 'dateFin', 'etatAdministratifUniteLegale'],\n",
    "                dtype = {'siren': 'str'})\n",
    "    \n",
    "    #rename columns\n",
    "    .rename(columns={'denominationUniteLegale': 'admin_name', 'dateDebut': 'start_year',\n",
    "                     'dateFin': 'end_year', 'etatAdministratifUniteLegale': 'status'}) \n",
    "   \n",
    "    # fix date variables \n",
    "     .assign(start_year=lambda df: pd.to_datetime(df['start_year'], errors='coerce').dt.year,\n",
    "             end_year=lambda df: pd.to_datetime(df['end_year'], errors='coerce').dt.year)\n",
    "    # filter \n",
    "    .loc[lambda df: df['admin_name'].notna() & ~df['admin_name'].eq('[ND]') & ~df['status'].eq('C')]\n",
    ")\n",
    "siren_numbers.loc[siren_numbers['end_year'].isna(), 'end_year'] = 2024\n",
    "\n",
    "siren_chunks = np.array_split(siren_numbers,cores); \n",
    "def cleaning_wrapper(index):\n",
    "    return(hf.clean_firm_names(siren_chunks[index],'admin_name',False))\n",
    "siren_numbers = (\n",
    "    pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "              (delayed(cleaning_wrapper)(index) for index in range(cores)), ignore_index = True)\n",
    "[['siren', 'admin_name_cleaned','start_year','end_year']])\n",
    "\n",
    "\n",
    "#######\n",
    "### prepare the siren numbers from scraped data\n",
    "#######\n",
    "tm_combined = (\n",
    "    pd.merge(pd.read_parquet(wd + \"tm_time_init.parquet\").drop('scrape_method', axis = 1),\n",
    "             pd.read_parquet(wd + \"tm_siren_init.parquet\").drop('scrape_method', axis = 1),\n",
    "             how = 'left')\n",
    "    [['application_year', 'siren', 'applicant_name_cleaned']]\n",
    "    .drop_duplicates())\n",
    "    \n",
    "patent_combined = (\n",
    "    patent_siren.loc[lambda c: ~c['applicant_name_cleaned'].str.contains(\",\", na=False)]\n",
    "    [['siren', 'application_year', 'applicant_name_cleaned']]\n",
    ").drop_duplicates()\n",
    "\n",
    "#######\n",
    "## MERGE AND GENERATE COVERAGE YEARS\n",
    "#######\n",
    "siren_numbers = pd.concat([siren_numbers,\n",
    "                           pd.concat([tm_combined, patent_combined],ignore_index = True)\n",
    "                           .assign(start_year = lambda df: df['application_year'])\n",
    "                           .rename(columns = {'application_year': 'end_year',\n",
    "                                              'applicant_name_cleaned': 'admin_name_cleaned'})],\n",
    "                          ignore_index = True)\n",
    "\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name_cleaned']\n",
    "\n",
    "start_dates = (siren_numbers.loc[lambda c: c['start_year'].notna()]\n",
    "               .sort_values(['combo','start_year'])\n",
    "               .groupby(['combo']).head(1)[['siren','admin_name_cleaned','start_year']])\n",
    "\n",
    "end_dates = (siren_numbers.loc[lambda c: c['end_year'].notna()]\n",
    "            .sort_values(['combo','end_year'], ascending = [True, False])\n",
    "            .groupby('combo').head(1)[['siren','admin_name_cleaned','end_year']])\n",
    "\n",
    "siren_numbers = pd.merge(start_dates,end_dates).loc[lambda x: x['end_year'].gt(1989)]\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "# generate list of names to match\n",
    "###############\n",
    "names_to_match = (\n",
    "    pd.concat([\n",
    "        patent_siren[['application_year', 'applicant_name_cleaned']],\n",
    "        patent_time[['application_year', 'applicant_name_cleaned']],\n",
    "        tm_time[['application_year', 'applicant_name_cleaned']]],\n",
    "        ignore_index = True)\n",
    "    .drop_duplicates()\n",
    "    .assign(applicant_name_cleaned = lambda df: df['applicant_name_cleaned'].str.split(','))\n",
    "    .explode('applicant_name_cleaned').drop_duplicates()\n",
    ")\n",
    "\n",
    "###############\n",
    "# use admin_names to generate a list of common words to strip \n",
    "###############\n",
    "def strip_wrapper(df,index,name): return(hf.strip_words(df[index], name, common_words)) \n",
    "names_vec = ['andre', 'bernard', 'claude', 'jacques', 'jean','louis', 'marie', 'martin', 'michel','paul', 'pierre', 'philippe']\n",
    "cut_off = .01\n",
    "#siren_numbers = pd.read_parquet(wd + 'matching_dictionary.parquet')\n",
    "#names_to_match = pd.read_parquet(wd + 'names_to_match.parquet')\n",
    "\n",
    "word_counts = siren_numbers['admin_name_cleaned'].str.split(expand = True).stack().value_counts()\n",
    "common_words = set(word_counts[word_counts > np.max(word_counts) * cut_off].index)\n",
    "common_words = {word for word in common_words if not word.isnumeric() and word not in names_vec}\n",
    "\n",
    "names_to_match = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "                           (delayed(strip_wrapper)(np.array_split(names_to_match,cores),index, 'applicant_name_cleaned') \n",
    "                            for index in range(cores)), ignore_index = True)\n",
    "siren_numbers = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')\n",
    "                           (delayed(strip_wrapper)(np.array_split(siren_numbers,cores),index, 'admin_name_cleaned') \n",
    "                            for index in range(cores)), ignore_index = True)\n",
    "    \n",
    "names_to_match.to_parquet(wd + 'names_to_match.parquet')\n",
    "siren_numbers.to_parquet(wd + 'matching_dictionary.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73c604-cd66-4f43-870b-0e3642546aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## Match Firms Name to Administrative Data \n",
    "##############################################################\n",
    "###############\n",
    "# Import / set parameters /functions \n",
    "###############\n",
    "def matching_wrapper(index):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{yr}: {round((index+1)/chunks*100, 2)}%\")\n",
    "    temp_firms = firm_chunks[index]\n",
    "    if temp_firms.empty:\n",
    "        return pd.DataFrame()\n",
    "    #run first version of matcher on all words \n",
    "    matches = matcher.match_names(to_be_matched=temp_firms, column_matching='applicant_name_cleaned')\n",
    "    results = (pd.wide_to_long(matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "               .reset_index()[['original_name', 'match_name', 'score']]\n",
    "               .rename(columns={'original_name': 'applicant_name_cleaned', 'match_name': 'admin_name_cleaned', 'score': 'raw_score'})\n",
    "               .merge(temp_firms[['applicant_name_cleaned', 'applicant_name_stripped']], how = 'left', on = 'applicant_name_cleaned')\n",
    "               .merge(dictionary[['admin_name_cleaned', 'admin_name_stripped']], how = 'left', on = 'admin_name_cleaned'))\n",
    "    company_chunks = [group for _, group in results.groupby('applicant_name_cleaned')]\n",
    "\n",
    "    ### run the second version of matcher only on words from initial list \n",
    "    results = []\n",
    "    temp_matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "    temp_matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "\n",
    "    for chunk in company_chunks:\n",
    "        chunk = chunk.reset_index()\n",
    "        try:\n",
    "            temp_matcher.load_and_process_master_data(column='admin_name_stripped', df_matching_data=chunk, transform=True)\n",
    "            temp_matches = temp_matcher.match_names(to_be_matched=chunk.iloc[0], column_matching='applicant_name_stripped')\n",
    "            temp_results = (pd.wide_to_long(temp_matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "                            .reset_index()[['original_name', 'match_name', 'score']]\n",
    "                            .rename(columns={'original_name': 'applicant_name_stripped', 'match_name': 'admin_name_stripped', 'score': 'stripped_score'})\n",
    "                            .drop_duplicates()\n",
    "                            .merge(chunk, how = 'right').sort_values(by = 'stripped_score', ascending = False)\n",
    "                            .head(final_matches)\n",
    "                            [['applicant_name_cleaned', 'admin_name_cleaned','raw_score', 'stripped_score']]\n",
    "                            .assign(application_year = yr)\n",
    "                           )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing company: {chunk.loc[0,'applicant_name_cleaned']} in index {index}\") \n",
    "            temp_results = (chunk.sort_values(by = 'raw_score', ascending = False)\n",
    "                            .head(final_matches)\n",
    "                            [['applicant_name_cleaned', 'admin_name_cleaned','raw_score']])\n",
    "        results.append(temp_results)\n",
    "    \n",
    "    results = (pd.concat(results, ignore_index = True)\n",
    "               .assign(stripped_score = lambda df: round(df['stripped_score']) if 'stripped_score' in df.columns else None)    \n",
    "               .loc[lambda df: df['stripped_score'].ge(min_score)])\n",
    "    return(results)\n",
    "\n",
    "\n",
    "wd = 'data/3_IP_data/2_working/'\n",
    "cores =  os.cpu_count() - 10; cut_off = .01\n",
    "siren_numbers = pd.read_parquet(wd + 'matching_dictionary.parquet')\n",
    "names_to_match = pd.read_parquet(wd + 'names_to_match.parquet')\n",
    "init_matches = 50; final_matches = 1; min_score = 90; cores =  os.cpu_count() - 10; \n",
    "chunks = cores*10\n",
    "\n",
    "###############\n",
    "# Carry out fuzzy matching  \n",
    "###############\n",
    "for yr in range(1990,2024):\n",
    "    print(f'preparing matcher for {yr}')\n",
    "    ## prepare list of words \n",
    "    matching = names_to_match.loc[names_to_match['application_year'].eq(yr)].copy()\n",
    "    dictionary = siren_numbers.loc[lambda df: df['start_year'].le(yr) & df['end_year'].ge(yr)].copy()\n",
    "\n",
    "    direct_matches = (dictionary[['admin_name_cleaned', 'siren']]\n",
    "                      .loc[dictionary['admin_name_cleaned'].isin(matching['applicant_name_cleaned'])]\n",
    "                      .assign(applicant_name_cleaned = lambda df: df['admin_name_cleaned'],\n",
    "                            raw_score = 100, stripped_score = 100, application_year = yr))\n",
    "\n",
    "    remaining_to_match = matching.loc[lambda df: ~df['applicant_name_cleaned'].isin(direct_matches['admin_name_cleaned']) &\n",
    "                                          df['applicant_name_stripped'].isin(dictionary['admin_name_stripped']) &\n",
    "                                          ~df['applicant_name_stripped'].eq(\"\")]\n",
    "    \n",
    "    ### Run Fuzzy Matching\n",
    "    firm_chunks = np.array_split(remaining_to_match, chunks)\n",
    "    matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "    matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "    matcher.load_and_process_master_data(column='admin_name_cleaned', df_matching_data=dictionary, transform=True)\n",
    "    matching_output = Parallel(n_jobs=cores, backend='multiprocessing')(delayed(matching_wrapper)(index) for index in range(chunks))\n",
    "\n",
    "\n",
    "    ### Perform Necessary Cleaning\n",
    "    matching_output =  pd.concat(matching_output, ignore_index = True).merge(dictionary[['siren','admin_name_cleaned']])\n",
    "    matching_output = (\n",
    "        ## add back in the direct matches \n",
    "        pd.concat([matching_output, direct_matches], ignore_index = True) \n",
    "\n",
    "        ## note the number of sirens each applicant / admin name is matched to \n",
    "        .assign(num_matches=lambda c: c.groupby(['applicant_name_cleaned', 'siren'])['raw_score'].transform('size'))\n",
    "\n",
    "        ## only keep the best stripped score performance for each applicant name \n",
    "        .sort_values(by='stripped_score', ascending=False).groupby('applicant_name_cleaned').head(1)\n",
    "\n",
    "        ## only keep the match if it was unique (only matched to one siren)\n",
    "        .loc[lambda df: df['num_matches'].eq(1)]\n",
    "    )\n",
    "    matching_output.to_parquet(wd + f'dictionary_complete_{yr}.parquet')\n",
    "\n",
    "dictionary = []\n",
    "for file in glob.glob('data/3_IP_data/2_working/dictionary_complete_*'):\n",
    "    dictionary.append(pd.read_parquet(file))\n",
    "    dictionary = pd.concat(dictionary, ignore_index = True)\n",
    "\n",
    "###############################\n",
    "## Do quality Assurance to determine cutoffs  \n",
    "###############################\n",
    "## Rule would be that we would have to hit a 75% likely match rate in sample \n",
    "## to allow that group in. Did this based on needing a 100% matched when stripped\n",
    "## and then relaxing the raw score cutoff. Rule eneded up with was raw score of at least 90\n",
    "## as checks of 85 yielded only 67% likely match rate \n",
    "checking_quality = False \n",
    "if checking_quality:\n",
    "    def matching_performance_check(raw_score_cutoff,sample_size):\n",
    "        random.seed(42)\n",
    "        test_sample = dictionary.loc[lambda df: ~df['raw_score'].eq(100) & df['stripped_score'].eq(100) & df['raw_score'].ge(raw_score_cutoff)]\n",
    "        num_obs = len(test_sample)\n",
    "        test_sample = (test_sample\n",
    "                       .loc[test_sample['applicant_name_cleaned'].isin(random.sample(list(test_sample['applicant_name_cleaned'].unique()), sample_size))]\n",
    "                       .drop_duplicates(subset = 'applicant_name_cleaned')\n",
    "                       .assign(match_likelihood = -10)).reset_index()\n",
    "        for i in range(len(test_sample)):\n",
    "            print(f\"{i +1})\")\n",
    "            display(HTML(f\"<span style='font-size:20px;'>Name to match: {test_sample['applicant_name_cleaned'].iloc[i]}</span>\"))\n",
    "            print(\"\")\n",
    "            display(HTML(f\"<span style='font-size:20px;'>Proposed Match: {test_sample['admin_name_cleaned'].iloc[i]}</span>\"))\n",
    "            user_input = input(\"rating = \")\n",
    "            while user_input not in ['1','2','3','break']:\n",
    "                user_input = input(\"best fit = \")\n",
    "            clear_output(wait=True)\n",
    "            if user_input == 'break':\n",
    "                break\n",
    "            else:\n",
    "                test_sample.loc[i, 'match_likelihood'] = int(user_input)-2\n",
    "        output = pd.DataFrame({'raw_score_cutoff': [raw_score_cutoff],\n",
    "                               'num_obs': num_obs,\n",
    "                               'match_rate': len(test_sample.loc[test_sample['match_likelihood'].eq(1)])/sample_size,\n",
    "                              'accepted_average': [np.mean(test_sample.loc[test_sample['match_likelihood'].eq(1)]['raw_score'])],\n",
    "                              'failed_average': [np.mean(test_sample.loc[~test_sample['match_likelihood'].eq(1)]['raw_score'])]})\n",
    "        return(output)\n",
    "    ninety_cutoff = matching_performance_check(90, 100)\n",
    "    eighty_five_cutoff = matching_performance_check(85,100)\n",
    "    \n",
    "    \n",
    "############ \n",
    "### Use Results of Quality Assurance, export data\n",
    "############\n",
    "dictionary = []\n",
    "for file in glob.glob('data/3_IP_data/2_working/dictionary_complete_*'):\n",
    "    dictionary.append(pd.read_parquet(file))\n",
    "dictionary = pd.concat(dictionary, ignore_index = True)\n",
    "dictionary = dictionary.loc[lambda df: df['raw_score'].ge(90) & df['stripped_score'].eq(100)]\n",
    "dictionary.to_parquet('data/3_IP_data/3_final/dictionary_complete.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f2dc1b-430d-4b0a-af8a-5253d9154f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## USE NEW FIRM NAME - SIREN DICTIONARY TO GENERATE FINAL DATASETS \n",
    "##############################################################\n",
    "wd = 'data/3_IP_data/2_working/'\n",
    "outd = 'data/3_IP_data/3_final/'\n",
    "dictionary = pd.read_parquet(outd + 'dictionary_complete.parquet')\n",
    "\n",
    "siren_final = []\n",
    "for version in ['patent', 'tm']:\n",
    "    print(version)\n",
    "    merge_method = 'left'\n",
    "    if version == 'patent': merge_method = 'outer'\n",
    "    df_combined = (\n",
    "        pd.merge(pd.read_parquet(wd + f\"{version}_time_init.parquet\").drop('scrape_method', axis = 1),\n",
    "                 pd.read_parquet(wd + f\"{version}_siren_init.parquet\").drop('scrape_method', axis = 1),\n",
    "                 how = merge_method)\n",
    "        .drop_duplicates()\n",
    "        .assign(index_num = lambda df: range(1, len(df) + 1)))\n",
    "\n",
    "    def_matched = (df_combined\n",
    "                   .loc[lambda df: ~df['siren'].isna() &  ~df['applicant_name_cleaned'].str.contains(\",\", na=False)]\n",
    "                   .assign(method = 'scraping'))\n",
    "    not_scrape_matched =  df_combined.loc[~df_combined['index_num'].isin(def_matched['index_num'])].drop('siren',axis = 1)\n",
    "    record_grouped = (\n",
    "        not_scrape_matched\n",
    "        .assign(applicant_name_cleaned = lambda df: df['applicant_name_cleaned'].str.split(','))\n",
    "        .explode('applicant_name_cleaned')\n",
    "        .merge(dictionary[['applicant_name_cleaned', 'siren', 'application_year']])\n",
    "        .groupby('index_num').agg(siren=pd.NamedAgg(column='siren', aggfunc=lambda x: ','.join(x)))\n",
    "        .reset_index()\n",
    "        .assign(method = 'matching')\n",
    "        .merge(not_scrape_matched, how = 'outer')\n",
    "    )\n",
    "    record_grouped = pd.concat([record_grouped, def_matched], ignore_index = True)\n",
    "    record_grouped.to_parquet(outd + version + \"_record_level_final.parquet\")\n",
    "    \n",
    "    siren_grouped = (\n",
    "        record_grouped.loc[record_grouped['siren'].notna()]\n",
    "        .assign(siren = lambda df: df['siren'].str.split(','))\n",
    "        .explode('siren')\n",
    "        .groupby(['siren', 'application_year'])\n",
    "        .size()\n",
    "        .reset_index(name=f\"num_{version}\")\n",
    "    )\n",
    "    siren_final.append(siren_grouped)\n",
    "pd.merge(siren_final[0], siren_final[1],how= 'outer').to_parquet(outd + \"siren_level_patent_and_tm_final.parquet\")\n",
    "\n",
    "########\n",
    "## Generate Summaries of Match Rate by patent vs. tm and \"collection\" \n",
    "########\n",
    "patent_match_shares = (\n",
    "    pd.read_parquet(outd + 'patent_record_level_final.parquet')\n",
    "    .groupby('collection')['method']\n",
    "    .value_counts(normalize=True, dropna=False)\n",
    "    .reset_index(name='share')\n",
    ")\n",
    "tm_match_shares = (\n",
    "    pd.read_parquet(outd + 'tm_record_level_final.parquet')\n",
    "    .groupby('trademark_type')['method']\n",
    "    .value_counts(normalize=True, dropna=False)\n",
    "    .reset_index(name='share')\n",
    ")\n",
    "\n",
    "patent_match_shares.to_csv(outd + \"patent_match_shares.csv\")\n",
    "tm_match_shares.to_csv(outd + \"tm_match_shares.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
