{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccfd671-5d10-48dd-ac45-3a11040ee46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os','math', 'glob',\n",
    "           're', 'pickle', 'numpy as np',\n",
    "           'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed',\n",
    "          'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata', 'sys', 'numpy as np', 'shutil', 'itertools']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f0218-c6d8-4bbb-93d0-22632af2dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate the list of SIREN / Firm Names\n",
    "########################################################################################\n",
    "## set parameters and define the wrapper \n",
    "chunks =  os.cpu_count() - 10; cut_off = .01\n",
    "def wrapper(index, function):\n",
    "    if function == \"clean\":\n",
    "        return(hf.clean_firm_names(siren_chunks[index], \"admin_name\", False))\n",
    "    else:\n",
    "        return(hf.strip_words(siren_chunks[index], 'admin_name_cleaned', common_words))          \n",
    "\n",
    "################\n",
    "### IMPORT THE SIREN NUMBERS \n",
    "################\n",
    "siren_numbers = (\n",
    "    ##import\n",
    "    pd.read_csv('../1_IWH/data/2_patent_tm_scraping/1_raw/1_StockUniteLegaleHistorique_utf8.csv',\n",
    "                usecols=['denominationUniteLegale', 'siren', 'dateDebut', 'dateFin', \n",
    "                         'activitePrincipaleUniteLegale','nomenclatureActivitePrincipaleUniteLegale', 'etatAdministratifUniteLegale'],\n",
    "                dtype = {'siren': 'str'})\n",
    "    \n",
    "    #rename columns\n",
    "    .rename(columns={'denominationUniteLegale': 'admin_name', 'dateDebut': 'start_date', 'dateFin': 'end_date', 'etatAdministratifUniteLegale': 'status',\n",
    "                     'activitePrincipaleUniteLegale' : 'industry', 'nomenclatureActivitePrincipaleUniteLegale' : 'industry_system'}) \n",
    "   \n",
    "    # fix date variables \n",
    "     .assign(start_date=lambda df: pd.to_datetime(df['start_date'], errors='coerce'),\n",
    "             end_date=lambda df: pd.to_datetime(df['end_date'], errors='coerce'))\n",
    "    # filter \n",
    "    .loc[lambda df: df['admin_name'].notna() & ~df['admin_name'].eq('[ND]') & ~df['status'].eq('C')]\n",
    ")\n",
    "################\n",
    "### NOTE THE INDUSTRIES OF EACH SIREN FOR THE PERIOD OF INTEREST \n",
    "################\n",
    "industry_year_dta = []\n",
    "for year in range(2008,2024):\n",
    "    industry_year_dta.append(\n",
    "        siren_numbers.loc[lambda c: c['start_date'].dt.year.le(year) & c['end_date'].dt.year.ge(year)]\n",
    "        .sort_values(by = ['siren','end_date'], ascending = [True,False])\n",
    "        .groupby('siren').head(1)\n",
    "       .assign(year = year)\n",
    "       [['siren','year', 'industry', 'industry_system']]\n",
    "    )\n",
    "pd.concat(industry_year_dta, ignore_index = True).to_parquet('data/2_processed/admin/siren_industry_year.parquet')\n",
    "\n",
    "################\n",
    "### RETRIEVE THE START AND END DATE OF THE SIREN / Name Combo (this method is orders of magnitudes faster than aggregating)\n",
    "################\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "start_dates = (siren_numbers.loc[lambda c: c['start_date'].notna()]\n",
    "               .sort_values(['combo','start_date'])\n",
    "               .groupby(['combo']).head(1)[['siren','admin_name','start_date']])\n",
    "\n",
    "na_end_dates = (siren_numbers.loc[lambda c: c['end_date'].isna()]\n",
    "                .drop_duplicates(subset = 'combo')[['siren','admin_name','combo','end_date']])\n",
    "\n",
    "end_dates = (siren_numbers.loc[lambda c: ~c['combo'].isin(na_end_dates['combo'])]\n",
    "            .sort_values(['combo','end_date'], ascending = [True, False])\n",
    "            .groupby('combo').head(1))\n",
    "siren_numbers = (pd.merge(start_dates, pd.concat([na_end_dates,end_dates])[['siren','admin_name','end_date']], how = 'outer')\n",
    "                .loc[lambda x: x['end_date'].dt.year.gt(2007) | x['end_date'].isna()])\n",
    "\n",
    "################\n",
    "## Generate Cleaned Names \n",
    "################\n",
    "siren_chunks = np.array_split(siren_numbers,chunks); \n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'clean') for index in range(chunks)), ignore_index = True)\n",
    "\n",
    "#establish / remove the list of common words \n",
    "word_counts = siren_numbers['admin_name_cleaned'].str.split(expand = True).stack().value_counts()\n",
    "common_words = set(word_counts[word_counts > np.max(word_counts) * cut_off].index)\n",
    "common_words = {word for word in common_words if not word.isnumeric()}\n",
    "with open('data/2_processed/admin/common_words.txt', 'w') as file: file.write('\\n'.join(common_words))\n",
    "\n",
    "\n",
    "### use the common words to clean the firms: \n",
    "siren_chunks = np.array_split(siren_numbers,chunks)\n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'strip') for index in range(chunks)), ignore_index = True)\n",
    "siren_numbers.to_parquet('data/2_processed/admin/siren_admin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2976f879-2b77-477a-809f-82a468a56a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# Generate our role dictionary \n",
    "########################################################################################\n",
    "to_drop = ['role_k1000', 'role_k500', 'role_k300', 'role_k150', 'role_k50', 'job_category', 'onet_code', 'onet_title','role_id']\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "roles = (db.raw_sql(\"select * from revelio.individual_role_lookup\")\n",
    "              .applymap(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "\n",
    "\n",
    "roles['role_id'] = roles.index\n",
    "roles['total'] = True\n",
    "roles['engineer'] = roles['job_category'] == 'engineer'\n",
    "roles['data'] = (\n",
    "    (roles['role_k50'] == 'data analyst') |\n",
    "    (roles['role_k150'].str.contains('data', na=False)) |\n",
    "    (roles['onet_title'].str.contains('database', na=False)) |\n",
    "    (roles['role_k1500'].str.contains('data center', na=False))\n",
    ")\n",
    "roles['data_analyst'] = (roles['data'] & \n",
    "                         roles['role_k50'].str.contains('analyst', na=False) |\n",
    "                         roles['role_k1500'].str.contains('intelligence', na=False))\n",
    "roles['data_engineer'] = (roles['data'] & ~roles['data_analyst'])\n",
    "                         \n",
    "                         \n",
    "rnd = pd.read_excel(\"data/1_raw_data/admin/ONET_RandD_roles.xlsx\").assign(rnd=True)[['Code', 'rnd']]\n",
    "stem = pd.read_excel(\"data/1_raw_data/admin/ONET_stem_roles.xlsx\").assign(stem=True)[['Code', 'stem']]\n",
    "roles = (roles.merge(rnd, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "           .merge(stem, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "            .assign(rnd=lambda x: x['rnd'].fillna(False))\n",
    "           .assign(stem=lambda x: x['stem'].fillna(False))\n",
    "         .applymap(lambda x: int(x) if isinstance(x, bool) else x))\n",
    "roles['non_data_rnd'] = roles['rnd'] & ~roles['data'];\n",
    "roles.drop(to_drop, axis = 1)\n",
    "roles.to_csv('data/2_processed/linkedin/revelio_role_dict.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb40e9-3dc5-4437-a0ad-c56e300b40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate our list of french factset ids\n",
    "########################################################################################\n",
    "(db.raw_sql(\"select factset_entity_id \"\n",
    "        \"from factset.edm_standard_address \"\n",
    "         \"where iso_country = 'FR'\" ).\n",
    "      to_parquet('data/2_processed/admin/factset_french_domiciled.parquet'))\n",
    "db.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a569d01c-fbfc-4932-9fb6-73222edfe389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# Observe the number of data  workers in each market \n",
    "########################################################################################\n",
    "# set parameters  \n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "year_range = range(2008,2024)\n",
    "data_roles = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv') \\\n",
    "    .loc[lambda x: x['data'].eq(1), 'role_k1500'] \\\n",
    "    .tolist()\n",
    "temp_direct = 'data/2_processed/linkedin/temp_ctry_output'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "linkedin_to_iso_cross_walk = pd.read_csv('data/2_processed/admin/linkedin_to_iso_crosswalk.csv')\n",
    "    \n",
    "\n",
    "# define helper functions \n",
    "def collapse_wrapper(year, data_output):\n",
    "    data_output['startdate'] = pd.to_datetime(data_output['startdate'], errors='coerce')\n",
    "    data_output['enddate'] = pd.to_datetime(data_output['enddate'], errors='coerce')\n",
    "    data_output = (\n",
    "        data_output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            comp =lambda x: x['total_compensation']*x['weight']).\n",
    "        loc[lambda x: x['valid']].\n",
    "        groupby('ctry').agg(\n",
    "            ctry_data_empl=('weight', 'sum'),\n",
    "            ctry_data_comp=('comp', 'sum')\n",
    "        ).reset_index().assign(year = year))\n",
    "    return(data_output)\n",
    "\n",
    "\n",
    "## GENERATE THE NUMBER / COMPENSATION FOR ALL DATA ROLES IN EACH COUNTRY \n",
    "possible_combos = pd.DataFrame(itertools.product(linkedin_to_iso_cross_walk['ctry'].unique(), year_range), columns=['ctry', 'year'])\n",
    "data_roles_output = (\n",
    "    db.raw_sql(\n",
    "        \"\"\"\n",
    "       SELECT country, weight, total_compensation, startdate, enddate \n",
    "       FROM revelio.individual_positions \n",
    "       WHERE role_k1500 IN %(data_roles)s\n",
    "       \"\"\", \n",
    "        params= {\"data_roles\": tuple(data_roles)})\n",
    "    .merge(linkedin_to_iso_cross_walk))\n",
    "\n",
    "data_roles_output = (\n",
    "    pd.concat([collapse_wrapper(year, data_roles_output) for year in year_range])\n",
    "    .merge(possible_combos, how = 'right')\n",
    "    .assign(ctry_data_empl=lambda x: x['ctry_data_empl'].fillna(0),\n",
    "            ctry_data_comp=lambda x: x['ctry_data_comp'].fillna(0))\n",
    ")\n",
    "data_roles_output.to_parquet('data/2_processed/linkedin/data_roles_in_all_countries.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5805d6-8958-418e-9ee8-18adada2876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Observe the Origin Universities /graduation dates of French Data Scientists \n",
    "########################################################################################\n",
    "\n",
    "year_range = range(2008,2024)\n",
    "cores =  os.cpu_count() - 10; \n",
    "data_roles = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv').loc[lambda x: x['data'].eq(1), 'role_k1500'].tolist()\n",
    "params = {\"data_roles\": tuple(data_roles)}\n",
    "role_output = db.raw_sql(\n",
    "    \"\"\"\n",
    "    SELECT user_id, weight, total_compensation, startdate, enddate \n",
    "    FROM revelio.individual_positions \n",
    "    WHERE role_k1500 IN %(data_roles)s AND country = 'France'\n",
    "    \"\"\", \n",
    "    params=params)\n",
    "\n",
    "params = {\"data_ids\": tuple(role_output['user_id'].tolist())}\n",
    "user_output = (\n",
    "    db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM revelio.individual_user_education \n",
    "        where user_id IN %(data_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "        ) \n",
    "    .loc[lambda x: ~x['rsid'].isna() & ~x['enddate'].isna()] \n",
    "    .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "            enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "    .assign(grad_year=lambda x: x['enddate'].dt.year))\n",
    "\n",
    "def university_collapse_wrapper(year):\n",
    "    temp = (\n",
    "        role_output.assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                              enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign(valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "                comp =lambda x: x['total_compensation']*x['weight'])\n",
    "        .loc[lambda x: x['valid']] \n",
    "        .merge(user_output.rename(columns = {'startdate': 'uni_start_date','enddate': 'uni_end_date'})) \n",
    "        .loc[lambda x: x['uni_end_date'].le(x['startdate'])]\n",
    "        .loc[lambda x: x.groupby('user_id')['uni_end_date'].idxmax()]\n",
    "        .loc[lambda x: x['university_country'].eq('France')]\n",
    "        .groupby(['university_name', 'grad_year', 'rsid', 'university_location', 'university_country'])\n",
    "        .agg(data_grads =('weight', 'sum'),\n",
    "             comp_weighted_data_grads=('comp', 'sum'))\n",
    "        .assign(observation_year = year)\n",
    "    ).reset_index()\n",
    "    return(temp)\n",
    "\n",
    "yr_lvl_dta_uni = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')(delayed(university_collapse_wrapper)(year) for year in year_range),ignore_index = True)\n",
    "yr_lvl_dta_uni.to_parquet('data/2_processed/linkedin/data_grads_across_france.parquet')\n",
    "\n",
    "### GENERATE A LIST OF LOCATIONS SO WE CAN START TRYING TO MATCH TO FIRM DATA \n",
    "simple_uni_x_location = (\n",
    "    pd.merge(yr_lvl_dta_uni,\n",
    "             yr_lvl_dta_uni.groupby('university_location', as_index=False)['data_grads'].max())\n",
    "    [['university_name', 'university_location']].drop_duplicates()\n",
    "    .merge(yr_lvl_dta_uni[['university_name','university_location']]\n",
    "           .drop_duplicates()\n",
    "           .assign(num_unis = lambda df: df.groupby('university_location')['university_location'].transform('count'))\n",
    "           [['university_location', 'num_unis']].drop_duplicates())\n",
    ")\n",
    "simple_uni_x_location.to_excel('data/2_processed/admin/uni_x_location_raw.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c024-00a0-41f4-9959-458e0289b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate A CROSS BETWEEN LEI AND SIREN CODES \n",
    "########################################################################################\n",
    "#When INSEE is the managing Local Operating Unit (LOU) or the firm is french, it identifies firms with SIREN codes.\n",
    "##The initial list of LEI codes is provided by the GLIEF\n",
    "##(https://search.gleif.org/#/search/simpleSearch=France&fulltextFilterId=LEIREC_FULLTEXT&currentPage=1&perPage=15&expertMode=false).\n",
    "\n",
    "# File path\n",
    "file_path = 'data/1_raw_data/admin/20241105-0000-gleif-goldencopy-lei2-golden-copy.csv'\n",
    "\n",
    "# Columns of interest and their new names\n",
    "interest_cols = ['LEI','Entity.LegalName','Entity.LegalAddress.Country', 'Entity.RegistrationAuthority.RegistrationAuthorityEntityID','Registration.ManagingLOU']\n",
    "new_names = [\"lei\", \"lei_name\", 'lei_country', 'lei_siren',\"managing_lou\"]\n",
    "crosswalk = pd.read_csv(file_path, usecols=interest_cols, low_memory=False)\n",
    "crosswalk.columns = new_names\n",
    "\n",
    "crosswalk = (crosswalk.assign(insee_registered = lambda c: c['managing_lou'] == '969500Q2MA9VBQ8BG884',\n",
    "             lei_siren = lambda c: c['lei_siren'].fillna('').astype(str).apply(lambda x: re.sub(r'[^a-zA-Z0-9]', '',x)))\n",
    "             .loc[lambda x: (x['lei_country'].eq(\"FR\") | x['insee_registered'])])\n",
    "\n",
    "crosswalk.to_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dfb9b-5d62-47f1-96f6-5c57702812cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES LOCATED IN FRANCE OR ASSIGNED TO A FRENCH PERSON \n",
    "########################################################################################\n",
    "\n",
    "####\n",
    "# FIND THE ROLES OF ALL FRENCH USERS \n",
    "####\n",
    "command = \"select user_id from revelio.individual_user where user_country = France\"\n",
    "french_users = db.raw_sql(command)\n",
    "\n",
    "\n",
    "\n",
    "all_roles = []\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100000\n",
    "user_ids = tuple(french_users['user_id'].unique())\n",
    "\n",
    "# Split user_ids into smaller batches and execute the query for each batch\n",
    "for i in range(0, len(user_ids), batch_size):\n",
    "    batch_user_ids = user_ids[i:i + batch_size]\n",
    "    params = {\"user_ids\": batch_user_ids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    roles_batch = db.raw_sql(\n",
    "        \"SELECT rcid, ultimate_parent_rcid \"\n",
    "        \"FROM revelio.individual_positions \"\n",
    "        \"WHERE user_id IN %(user_ids)s \"\n",
    "        ,\n",
    "        params=params,\n",
    "    )\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_roles.append(roles_batch)\n",
    "\n",
    "french_roles = db.raw_sql(\n",
    "        \"SELECT rcid, ultimate_parent_rcid \"\n",
    "        \"FROM revelio.individual_positions \"\n",
    "        \"WHERE country = 'France' \",\n",
    "        params=params,\n",
    "    )\n",
    "all_roles.append(french_roles)\n",
    "\n",
    "# Concatenate all batches into a single DataFrame\n",
    "roles = pd.concat(all_roles, ignore_index=True)\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# FIND ALL COMPANIES THAT HAVE HIRED AT LEAST ONE OF THESE ROLES OR HAD A RELATION DO SO\n",
    "########################################################################################\n",
    "\n",
    "rcids = tuple(roles['rcid'].unique());\n",
    "parent_rcids = tuple(map(str, roles['ultimate_parent_rcid'].unique()));\n",
    "\n",
    "all_companies = []\n",
    "## FIND ALL COMPANIES BASED ON THEIR RCID \n",
    "for i in range(0, len(rcids), batch_size):\n",
    "    batch_rcids = rcids[i:i + batch_size]\n",
    "    params = {\"rcids\": batch_rcids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    companies_batch = db.raw_sql(\"SELECT * \"\n",
    "                      \"FROM  revelio.company_mapping \"\n",
    "                       \"WHERE rcid IN %(rcids)s \",\n",
    "                     params = params)\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_companies.append(companies_batch)\n",
    "\n",
    "#### FIND ALL COMPANIES BASED ON THEIR PARENT RCID\n",
    "for i in range(0, len(parent_rcids), batch_size):\n",
    "    batch_rcids = rcids[i:i + batch_size]\n",
    "    params = {\"rcids\": batch_rcids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    companies_batch = db.raw_sql(\"SELECT * \"\n",
    "                      \"FROM  revelio.company_mapping \"\n",
    "                       \"WHERE ultimate_parent_rcid IN %(rcids)s \",\n",
    "                     params = params)\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_companies.append(companies_batch)\n",
    "  \n",
    "companies = pd.concat(all_companies, ignore_index=True).drop_duplicates()\n",
    "companies['year_founded'] = pd.to_numeric(companies['year_founded'], errors='coerce')\n",
    "\n",
    "\n",
    "### CLEAN THE NAMES OF THOSE FIRMS \n",
    "with open('data/2_processed/admin/common_words.txt', 'r') as file:\n",
    "    common_words = set(file.read().splitlines())\n",
    "chunks =  os.cpu_count() - 10\n",
    "company_chunks = np.array_split(companies,chunks); \n",
    "def wrapper(index):\n",
    "    temp = hf.clean_firm_names(company_chunks[index], \"company\", True)\n",
    "    return(hf.strip_words(temp, 'company_cleaned', common_words))\n",
    "       \n",
    "companies = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index) for index in range(chunks)), ignore_index = True)\n",
    "companies = companies.loc[~companies['company_cleaned'].eq(\"\")]\n",
    "companies.to_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a5ce2-9704-4f5c-a536-7c4de1890c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES TIED TO A COMPANY WITH SOME CONNECTION TO FRANCE\n",
    "########################################################################################\n",
    "year_range = range(2008,2024)\n",
    "int_vars = ['french', 'total', 'engineer', 'data','rnd','stem']\n",
    "role_dict_vars = ['role_id'] + int_vars[1:]\n",
    "chunks = 1000\n",
    "cores =  os.cpu_count() - 10; \n",
    "export_path = 'data/2_processed/linkedin/temp_collapsed_roles'\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "\n",
    "role_to_id = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv').set_index('role_k1500')['role_id'].to_dict()\n",
    "country_dict = pd.read_csv('data/2_processed/linkedin/revelio_country_dict.csv')\n",
    "france_id = country_dict.loc[country_dict['country'] == 'France', 'country_id'].values[0]\n",
    "french_users = pd.read_parquet('data/2_processed/linkedin/user_components_by_country/users_'+ str(france_id)+\".parquet\")['user_id']\n",
    "role_dict = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv')\n",
    "\n",
    "companies = pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')[['rcid']].drop_duplicates()\n",
    "company_chunks = np.array_split(companies, chunks)\n",
    "\n",
    "for index in reversed(range(chunks)):\n",
    "    export_file = export_path + f'/chunk_{index}.parquet'\n",
    "    rcid_list = tuple(map(str, company_chunks[index]['rcid']))\n",
    "    if not os.path.isfile(export_file): \n",
    "        print(f'starting import {index}')\n",
    "        df = db.raw_sql(\n",
    "                \"SELECT user_id, position_id, country AS role_country, startdate, enddate, role_k1500, \"\n",
    "                \"weight, seniority, total_compensation, rcid \"\n",
    "                \"FROM revelio.individual_positions WHERE rcid IN %(rcids)s \", \n",
    "                params={\"rcids\": rcid_list})\n",
    "        \n",
    "        print(f'starting processing {index}')\n",
    "        df_rcid_chunks = np.array_split(df[['rcid']].drop_duplicates(), cores)\n",
    "        df_chunks = []\n",
    "        for chunk in range(cores):\n",
    "            df_chunks.append(df.loc[df['rcid'].isin(df_rcid_chunks[chunk]['rcid'])])\n",
    "\n",
    "        def collapse_and_clean(chunk):\n",
    "            chunk = (chunk\n",
    "                  .assign(role_id = chunk['role_k1500'].map(role_to_id),\n",
    "                          french = chunk['role_country'].eq('France') | chunk['user_id'].isin(french_users),\n",
    "                          startdate = chunk['startdate'].apply(pd.to_datetime),\n",
    "                          enddate = chunk['enddate'].apply(pd.to_datetime))\n",
    "                  .merge(role_dict[role_dict_vars], on='role_id', how='left'))\n",
    "\n",
    "            output_list = []\n",
    "            for year in year_range:\n",
    "                # Create a temporary DataFrame with 'valid' column indicating if 'startdate' <= year <= 'enddate' or 'enddate' is NA\n",
    "                temp = chunk.copy()\n",
    "                temp['valid'] = ((temp['startdate'].dt.year <= year) & \\\n",
    "                                 ((temp['enddate'].dt.year >= year) | temp['enddate'].isna())).astype(int)\n",
    "\n",
    "                # Pre compute interest columns\n",
    "                for col in int_vars:\n",
    "                    temp[f'emp_{col}'] = temp[col] * temp['valid'] * temp['weight']\n",
    "                    temp[f'comp_{col}'] = temp[col] * temp['valid'] * temp['weight']*temp['total_compensation'] / 1000\n",
    "\n",
    "                output = temp.groupby('rcid').agg({\n",
    "                    **{f'emp_{col}': 'sum' for col in int_vars},\n",
    "                    **{f'comp_{col}': 'sum' for col in int_vars}\n",
    "                }).reset_index()\n",
    "\n",
    "                for col in int_vars: \n",
    "                    output[f'emp_{col}'] = round(output[f'emp_{col}'])\n",
    "\n",
    "                output['share_emp_french'] = output['emp_french'] /output['emp_total'] \n",
    "                output['share_comp_french'] = output['comp_french'] / output['comp_total']\n",
    "                output['year'] = year\n",
    "                columns_to_round = ['comp_french', 'comp_total', 'comp_engineer', 'comp_data', 'comp_rnd', 'comp_stem']\n",
    "                output[columns_to_round] = output[columns_to_round].apply(lambda x: x.round(0))\n",
    "\n",
    "                output_list.append(output)\n",
    "            return(pd.concat(output_list).sort_values(by=['rcid']))\n",
    "        roles_yr_level = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')(delayed(collapse_and_clean)(chunk) for chunk in df_chunks),ignore_index = True)\n",
    "\n",
    "        roles_yr_level.to_parquet(export_file)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "output_list =[]\n",
    "file_list = sorted(glob.glob(export_path + '/*'))\n",
    "for file in file_list:\n",
    "    clear_output(wait=True)\n",
    "    print(file)\n",
    "    output_list.append(pd.read_parquet(file))\n",
    "    \n",
    "output_list = pd.concat(output_list)\n",
    "output_list.to_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "shutil.rmtree(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252b6eb-b0ac-48bc-a606-f30ac0e5b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# USE ROLE DATA TO DETERMINE WHICH FIRMS ARE LIKELY FRENCH \n",
    "########################################################################################\n",
    "\n",
    "french_leis = pd.read_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet')['lei'].unique()\n",
    "non_french_country_domains = (pd.read_excel('data/2_processed/admin/domain_names_by_country.xlsx')\n",
    "                              .assign(name=lambda x: x['name'].str.replace('.', '', regex=False))\n",
    "                              .query('include != include')['name'])\n",
    "french_factset_ids = pd.read_parquet('data/2_processed/admin/factset_french_domiciled.parquet')['factset_entity_id'].unique()\n",
    "roles_data = pd.read_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "\n",
    "companies = (\n",
    "    ## determine the firm's max total / french / data values and shares \n",
    "    roles_data\n",
    "    .assign(cost_per_worker = lambda df: df['comp_total'] / df['emp_total'])\n",
    "    .groupby('rcid', as_index=False)\n",
    "    .agg({'cost_per_worker':'max','emp_total': 'max','emp_french': 'max','emp_data': 'max',\n",
    "          'comp_total': 'max','comp_data': 'max','share_emp_french': 'max', 'share_comp_french': 'max'})\n",
    "    .assign(french_eligible = lambda c: c['emp_french'].gt(0))\n",
    "    [['rcid', 'french_eligible','cost_per_worker', 'share_emp_french', 'share_comp_french','emp_total', 'emp_data' ,'comp_total', 'comp_data']].\n",
    "    merge(pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet'))\n",
    "    \n",
    "    ### determine whether the firm likely french or not \n",
    "    .assign(url_ending = lambda c: c['url'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) else None))\n",
    "    .assign(\n",
    "        admin_score=lambda c: 0\n",
    "        ## TOP LEVEL DOMAIN \n",
    "        + c['url_ending'].eq('fr')  \n",
    "        - c['url_ending'].isin(non_french_country_domains)  \n",
    "\n",
    "        ## FACTSET\n",
    "        + c['factset_entity_id'].isin(french_factset_ids)\n",
    "        - (~c['factset_entity_id'].isin(french_factset_ids) & c['factset_entity_id'].notna())\n",
    "\n",
    "        # LEI CHECK \n",
    "        +  c['lei'].isin(french_leis) \n",
    "        -  (~c['lei'].isin(french_leis) & c['lei'].notna()) \n",
    "\n",
    "        # ISIN \n",
    "        + c['isin'].str[:2].eq(\"FR\") # add if french isin\n",
    "        - (~c['isin'].str[:2].eq(\"FR\") & c['isin'].notna())\n",
    "\n",
    "        # CUSIP \n",
    "        + c['cusip'].str[:1].eq(\"F\") # add if french cusip\n",
    "        - (~c['cusip'].str[:1].eq(\"F\") & c['cusip'].notna())\n",
    "\n",
    "        # Firm type \n",
    "        + c['firm_type_french_likelihood'].eq(\"likely french\") \n",
    "        - c['firm_type_french_likelihood'].eq(\"unlikely french\"))\n",
    "     .assign(\n",
    "         likely_french = lambda c: \n",
    "         c['french_eligible'] & (\n",
    "         c['admin_score'].gt(0) | \n",
    "         (c['admin_score'].eq(0) & (c['share_emp_french'].gt(.5) | c['share_comp_french'].gt(.5)))))\n",
    "    \n",
    "    #### mark whether it's a subsidiary \n",
    "    .assign(subsidiary = lambda c: c['rcid'] != c['ultimate_parent_rcid'],\n",
    "            public = lambda df: ~df['ticker'].isna(),\n",
    "            has_lei = lambda df: ~df['lei'].isna())\n",
    "    [['rcid','lei','company','company_cleaned', 'company_stripped', 'year_founded', 'ultimate_parent_rcid',\n",
    "      'likely_french', 'subsidiary', 'public', 'has_lei','share_emp_french', 'share_comp_french',\n",
    "      'emp_total', 'emp_data' ,'comp_total', 'comp_data','cost_per_worker']]\n",
    ")\n",
    "companies.to_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
