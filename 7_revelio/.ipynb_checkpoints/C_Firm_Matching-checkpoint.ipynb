{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8dc807-d2fd-4080-bdca-178e83dda239",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output', 'random',\n",
    "          'unicodedata','sys']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ec1bc-209b-42e3-be3d-ec43eff821ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Run Matching \n",
    "########################################################################################\n",
    "############  \n",
    "### DEFINE PARAMETERS AND IMPORT DATA / Matching Function\n",
    "############  \n",
    "init_matches = 50; final_matches = 5; cores =  os.cpu_count() - 10; \n",
    "chunks = cores*10\n",
    "\n",
    "def matching_wrapper(index):\n",
    "    #output progress \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{round(index/chunks*100, 2)}%\")\n",
    "    \n",
    "    temp_firms = firm_chunks[index]\n",
    "    #run first version of matcher on all words \n",
    "    matches = matcher.match_names(to_be_matched=temp_firms, column_matching='company_cleaned')\n",
    "    results = (pd.wide_to_long(matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "               .reset_index()[['original_name', 'match_name', 'score']]\n",
    "               .rename(columns={'original_name': 'company_cleaned', 'match_name': 'admin_name_cleaned', 'score': 'raw_score'})\n",
    "               .merge(temp_firms[['company_cleaned', 'company_stripped']], how = 'left', on = 'company_cleaned')\n",
    "               .merge(sirens_to_match[['admin_name_cleaned', 'admin_name_stripped']], how = 'left', on = 'admin_name_cleaned'))\n",
    "    company_chunks = [group for _, group in results.groupby('company_cleaned')]\n",
    "\n",
    "    ### reun the second version of matcher only on words from initial list \n",
    "    results = []\n",
    "    temp_matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "    temp_matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "    for chunk in company_chunks:\n",
    "        chunk = chunk.reset_index()\n",
    "        try:\n",
    "            temp_matcher.load_and_process_master_data(column='admin_name_stripped', df_matching_data=chunk, transform=True)\n",
    "            temp_matches = temp_matcher.match_names(to_be_matched=chunk.iloc[0], column_matching='company_stripped')\n",
    "            temp_results = (pd.wide_to_long(temp_matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "                            .reset_index()[['original_name', 'match_name', 'score']]\n",
    "                            .rename(columns={'original_name': 'company_stripped', 'match_name': 'admin_name_stripped', 'score': 'stripped_score'})\n",
    "                            .drop_duplicates()\n",
    "                            .merge(chunk, how = 'right')\n",
    "                            .assign(match_index = lambda df: df.groupby(['stripped_score']).ngroup()))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing company: {chunk.loc[0,'company_cleaned']} in index {index}\") \n",
    "            temp_results = chunk.assign(match_index = lambda df: df.groupby(['raw_score']).ngroup())\n",
    "\n",
    "        temp_results = (temp_results\n",
    "                        .assign(match_index = lambda df: df['match_index'].max() - df['match_index'] + 1)\n",
    "                        .sort_values(['match_index','raw_score'], ascending = [True, False])\n",
    "                        .loc[lambda df: df['match_index'].le(final_matches)])\n",
    "        results.append(temp_results)\n",
    "    results = pd.concat(results, ignore_index = True)[['company_cleaned', 'admin_name_cleaned', 'raw_score', 'stripped_score', 'match_index']]\n",
    "    return(results)\n",
    "\n",
    "############  \n",
    "### Prepare the lists of firm names / check for exact matches \n",
    "############ \n",
    "firms_to_match = (pd.read_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')\n",
    "                 .loc[lambda c: c['likely_french']] # & c['data_eligible']\n",
    "                 .drop_duplicates(subset='company_cleaned')\n",
    "                 [['company_cleaned','company_stripped']])\n",
    "\n",
    "\n",
    "sirens_to_match = (pd.read_parquet('data/2_processed/admin/siren_admin.parquet')\n",
    "                   .drop_duplicates(subset='admin_name_cleaned')\n",
    "                  [['admin_name_cleaned', 'admin_name_stripped']])\n",
    "\n",
    "initial_matches = (\n",
    "    pd.merge(firms_to_match[['company_cleaned']], sirens_to_match[['admin_name_cleaned']], how = 'inner',\n",
    "             left_on = 'company_cleaned', right_on = 'admin_name_cleaned')\n",
    "    .assign(raw_score = 100, stripped_score = 100, match_index = 1))\n",
    "\n",
    "############  \n",
    "### match remaining firms \n",
    "############ \n",
    "remaining_to_match = firms_to_match.loc[lambda c: ~c['company_cleaned'].isin(initial_matches['company_cleaned']) &\n",
    "                                        c['company_stripped'].isin(sirens_to_match['admin_name_stripped']) &\n",
    "                                       ~c['company_stripped'].eq(\"\")]\n",
    "firm_chunks = np.array_split(remaining_to_match, chunks)\n",
    "matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "matcher.load_and_process_master_data(column='admin_name_cleaned', df_matching_data=sirens_to_match, transform=True)\n",
    "\n",
    "############  \n",
    "### output results \n",
    "############ \n",
    "matching_output = Parallel(n_jobs=cores, backend='multiprocessing')(delayed(matching_wrapper)(index) for index in range(chunks))\n",
    "matching_output = pd.concat(matching_output, ignore_index = True)\n",
    "matching_output = pd.concat([initial_matches, matching_output], ignore_index=True)\n",
    "matching_output = (\n",
    "    matching_output\n",
    "    .merge(pd.read_parquet('data/2_processed/admin/siren_admin.parquet')[['admin_name','admin_name_cleaned', 'siren']],\n",
    "             how = 'left', on = 'admin_name_cleaned')\n",
    "    .merge(pd.read_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')[['rcid', 'company', 'company_cleaned']],\n",
    "           how = 'left', on = 'company_cleaned')\n",
    "    [['company','admin_name','company_cleaned', 'admin_name_cleaned', 'rcid', 'siren', 'raw_score', 'stripped_score', 'match_index']]\n",
    "    .assign(match_group_size = lambda df: df.groupby(['company', 'match_index'])['match_index'].transform('size')))\n",
    "\n",
    "matching_output.to_parquet('data/2_processed/admin/fuzzy_matching_output_raw.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5540c5-4cac-406e-8d57-127f3443bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "##### Check Performance of Matching Metrics\n",
    "########################################################################################\n",
    "def matching_performance_check(df):\n",
    "    if not 'match_likelihood' in df.columns:\n",
    "        df = (df[df['rcid'].isin(random.sample(list(df['rcid'].unique()), sample_size))]\n",
    "                   .assign(match_likelihood = -10))\n",
    "    for rcid in df['rcid'].unique():\n",
    "        temp = df.loc[lambda c:c['rcid'].eq(rcid)]\n",
    "        if max(temp['match_likelihood'])== -10:\n",
    "            display(HTML(f\"<span style='font-size:20px;'>Name to match: {temp['company'].iloc[0]}</span>\"))\n",
    "            print('')\n",
    "            for j in range(len(temp)):\n",
    "                display(HTML(f\"<span style='font-size:15px;'> {temp['admin_name'].iloc[j]} </span>\"))\n",
    "            user_input = input(\"best fit = \")\n",
    "            while user_input not in ['1','2','3','break']:\n",
    "                user_input = input(\"best fit = \")\n",
    "            clear_output(wait=True)\n",
    "            if user_input == 'break':\n",
    "                break\n",
    "            else:\n",
    "                df.loc[lambda c: c['rcid'].eq(rcid), 'match_likelihood'] = int(user_input)-2\n",
    "    return(df)\n",
    "    \n",
    "firms_to_match = (pd.read_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')\n",
    "                 .loc[lambda c: c['likely_french']]) # & c['data_eligible']\n",
    "\n",
    "############  \n",
    "### Identify firms matched by LEI / exact matches / fuzzy matches  \n",
    "############ \n",
    "lei_matched = (\n",
    "    pd.merge(pd.read_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet'),\n",
    "             pd.read_parquet('data/2_processed/admin/siren_admin.parquet')[['siren','admin_name']],\n",
    "             left_on = 'lei_siren', right_on = 'siren')\n",
    "    [['lei', 'lei_country', 'siren', 'admin_name']]\n",
    "    .merge(firms_to_match[['rcid', 'company', 'lei']], on = 'lei')\n",
    "    .assign(method = 'lei'))\n",
    "\n",
    "\n",
    "clean_matched = (\n",
    "    pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_raw.parquet')\n",
    "    .loc[lambda c:\n",
    "         ~c['rcid'].isin(lei_matched['rcid']) \n",
    "        & c['raw_score'].eq(100)\n",
    "        & c['match_group_size'].eq(1)]\n",
    "    .assign(method = 'clean'))\n",
    "\n",
    "strip_matched = (\n",
    "    pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_raw.parquet')\n",
    "   .loc[lambda c: ~c['rcid'].isin(lei_matched['rcid'])\n",
    "        & ~c['raw_score'].eq(100)\n",
    "        & c['stripped_score'].eq(100)]\n",
    "    .loc[lambda c: c['match_group_size'].eq(1)]\n",
    "    .assign(method = 'strip'))\n",
    "\n",
    "############  \n",
    "### Test their match rates \n",
    "### all have match rates above 75% which we set as the cutoff so \n",
    "### just use all three to make the final dictionary \n",
    "############ \n",
    "testing_matches = False\n",
    "if testing_matches:\n",
    "    random.seed(42); sample_size = 100;\n",
    "    lei_matched_sample = matching_performance_check(lei_matched)\n",
    "    clean_matched_sample = matching_performance_check(clean_matched)\n",
    "    strip_matched_sample = matching_performance_check(strip_matched)\n",
    "\n",
    "    lei_matched_sample.to_parquet('data/2_processed/admin/lei_matching_performance.parquet')\n",
    "    clean_matched_sample.to_parquet('data/2_processed/admin/clean_matching_performance.parquet')\n",
    "    strip_matched_sample.to_parquet('data/2_processed/admin/strip_matching_performance.parquet')\n",
    "\n",
    "dictionary_complete = (\n",
    "    pd.concat([lei_matched,clean_matched, strip_matched], ignore_index = True)[['rcid', 'siren','method']]\n",
    "    .drop_duplicates())\n",
    "dictionary_complete.to_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968707d0-2da1-4d5f-9911-cb0e6060152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_firmids = dictionary_complete.assign(firmid = dictionary_complete['siren'])[['firmid']].drop_duplicates().reset_index()\n",
    "matched_firmids.to_parquet('data/2_processed/admin/all_linkedin_matched_firmids_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581bdd6-bcc8-4292-b0e8-5d63a0f51e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### \n",
    "### Generate Descriptive stats about \n",
    "### matching \n",
    "##########\n",
    "matching_descriptives_base = (\n",
    "    pd.read_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')\n",
    "    .loc[lambda c: c['likely_french']]\n",
    "    .merge(dictionary_complete, how = 'left')\n",
    "    .assign(method=lambda df: df['method'].apply(lambda x: \"unmatched\" if pd.isna(x) else x))\n",
    "    .assign(matched =lambda df: ~df['method'].eq('unmatched'), \n",
    "            has_lei = lambda df: ~df['lei'].isna())\n",
    ")\n",
    "\n",
    "match_performance = (\n",
    "    matching_descriptives_base.groupby(['method'])\n",
    "    .size().reset_index(name='count')\n",
    "    .assign(percent = lambda df: df['count'] / df['count'].sum()*100))\n",
    "\n",
    "mean_descriptives = (matching_descriptives_base.groupby(['matched'])\n",
    " .agg({'share_comp_french':'mean','emp_total':'mean','emp_data': 'mean',\n",
    "       'comp_total':'mean','comp_data': 'mean', 'cost_per_worker':'mean', 'subsidiary': 'mean',\n",
    "       'year_founded':'mean', 'has_lei': 'mean'}))\n",
    "\n",
    "\n",
    "median_descriptives = (matching_descriptives_base.groupby(['matched'])\n",
    " .agg({'share_comp_french':'median','emp_total':'median','emp_data': 'median',\n",
    "       'comp_total':'median','comp_data': 'median', 'cost_per_worker':'median',\n",
    "       'year_founded':'median'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6ed4482-f100-4534-bafc-d048efde8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## Generate Final Datasets \n",
    "##############################################################\n",
    "french_firm_info = (pd.read_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')\n",
    "                    [['rcid', 'likely_french', 'subsidiary', 'public', 'has_lei']]\n",
    "                   .assign(rcid = lambda df: df['rcid'].astype(str)))\n",
    "fuzzy_matching_output = (pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')\n",
    "                         .rename(columns = {'siren': 'firmid'})\n",
    "                        .assign(has_siren = True, rcid = lambda df: df['rcid'].astype(str)))\n",
    "non_matched =  (french_firm_info[['rcid']]\n",
    "                .loc[lambda df: ~df['rcid'].isin(fuzzy_matching_output['rcid'])]\n",
    "                .assign(firmid = lambda df: df['rcid'].astype(str).str.zfill(10), \n",
    "                        has_siren = False)\n",
    "               )\n",
    "fuzzy_matching_output = (pd.concat([fuzzy_matching_output, non_matched])\n",
    "                        .assign(rcid_count = lambda df: df.groupby('firmid')['firmid'].transform('count'))\n",
    "                        .assign(needs_collapse = lambda df: ~df['rcid_count'].eq(1))\n",
    "                        )\n",
    "french_firm_info = pd.merge(french_firm_info, fuzzy_matching_output)\n",
    "french_firm_info = pd.concat([french_firm_info.loc[french_firm_info['needs_collapse']]\n",
    "                .groupby('firmid', as_index = False)\n",
    "                .agg({'rcid': lambda x: ','.join(x), 'has_lei': 'any', 'public': 'any',\n",
    "                      'likely_french': 'any', 'subsidiary': 'any', 'has_siren': 'any',\n",
    "                      'rcid_count':'max', 'needs_collapse':'any'}),\n",
    "               french_firm_info.loc[~french_firm_info['needs_collapse']]])\n",
    "\n",
    "french_roles = (pd.read_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "                .assign(rcid = lambda df: df['rcid'].astype(str))\n",
    "                .merge(fuzzy_matching_output[['rcid', 'firmid', 'needs_collapse']])\n",
    "                .drop(columns=['rcid', 'share_comp_french','share_emp_french']))\n",
    "\n",
    "french_roles = pd.concat([french_roles.loc[french_roles['needs_collapse']]\n",
    "                .groupby(['firmid','year'], as_index=False).sum()\n",
    "                .drop(columns='needs_collapse'), \n",
    "                french_roles.loc[~french_roles['needs_collapse']]\n",
    "                .drop(columns='needs_collapse')])\n",
    "\n",
    "\n",
    "\n",
    "pd.merge(french_firm_info,french_roles).to_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_clean.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
