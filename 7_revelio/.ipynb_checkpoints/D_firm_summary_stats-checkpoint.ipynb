{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a31cfdc-8581-4b19-9773-2684989864cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata','sys', 'matplotlib.pyplot as plt', 'glob', 'shutil','from sklearn.decomposition import PCA']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc2122-b58c-41d4-ac37-e31d83860c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Find Ancilliary Information Associated with our matched companies \n",
    "########################################################################################\n",
    "matching_output = pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')\n",
    "num_chunks = 50\n",
    "temp_direct = 'data/2_processed/linkedin/temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(matching_output['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):  \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = (\n",
    "            db.raw_sql(\n",
    "                \"\"\"\n",
    "                SELECT rcid, child_rcid, ultimate_parent_rcid, ticker, lei,  hq_zip_code, hq_metro_area, hq_state, hq_country\n",
    "                FROM  revelio.company_mapping \n",
    "                WHERE rcid IN %(rcid_list)s\n",
    "                \"\"\",\n",
    "                params = {'rcid_list': tuple(chunks[index].tolist())})\n",
    "            .assign(has_subsid = lambda x: ~x['rcid'].eq(x['child_rcid']),\n",
    "                    is_subsid = lambda x: ~x['rcid'].eq(x['ultimate_parent_rcid']),\n",
    "                    is_public = lambda x: ~x['ticker'].isna(),\n",
    "                    has_lei = lambda x: ~x['lei'].isna(),\n",
    "                    french_hq = lambda x: ~x['hq_country'].isna(),\n",
    "                    hq_metro_area =lambda x: x['hq_metro_area']\n",
    "                    .str.replace('france nonmetropolitan area', 'non_metro', regex=False)\n",
    "                    .str.replace('metropolitan area', '', regex=False)\n",
    "                    .str.strip()))\n",
    "\n",
    "        parent_temp = (\n",
    "            db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT ultimate_parent_rcid, hq_country\n",
    "            FROM  revelio.company_mapping \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\",\n",
    "            params = {'rcid_list':  tuple(temp.loc[temp['is_subsid']]['ultimate_parent_rcid'].unique().tolist())})\n",
    "            .assign(parent_non_french_hq = lambda x: (~x['hq_country'].eq('France')).where(x['hq_country'].notna()))\n",
    "            .rename(columns={\"hq_country\": \"parent_hq_country\"}))\n",
    "        temp = pd.merge(temp, parent_temp, how = 'left')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    " .to_parquet('data/2_processed/linkedin/matched_firm_base_info.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa829d8-1239-4afa-8133-dbdd8e3745ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES ASSOCIATED WITH OUR MATCHED COMPANIES\n",
    "########################################################################################\n",
    "matching_output = pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')\n",
    "num_chunks = 50\n",
    "temp_direct = 'data/2_processed/linkedin/temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(matching_output['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = role_output = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,country, weight, total_compensation, startdate, enddate, role_k1500 \n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        for col in ['startdate', 'enddate']:\n",
    "            temp[col] = pd.to_datetime(temp[col], errors='coerce')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],\n",
    "           ignore_index = True)\n",
    " .to_parquet('data/2_processed/linkedin/matched_firm_role_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d5e83-2ac8-4581-b709-6ce6afbe3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Perform PCA to generate metrics of differentiation \n",
    "########################################################################################\n",
    "temp_direct = 'data/2_processed/linkedin/temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=('wgted_comp', 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet('data/2_processed/linkedin/matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren': 'firmid'})\n",
    "output = pd.merge(long_data, matching_output)\n",
    "\n",
    "# run pca analysis\n",
    "pca_model = collapse_year_level(sample_year, making_pca=True)\n",
    "[collapse_year_level(year,False,pca_model) for year in range(2008, 2024)]\n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    " .to_parquet('data/2_processed/linkedin/matched_firm_pca_output.parquet'))\n",
    "shutil.rmtree(temp_direct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed5f53-cc63-482b-8965-48253856358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.8%\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# FIND THE PRESTIGE / education OF ALL EMPLOYEES ASSOCIATED WITH OUR MATCHED COMPANIES \n",
    "########################################################################################\n",
    "num_chunks = 50\n",
    "temp_direct = 'data/2_processed/linkedin/temp_user_prestige'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "role_output = pd.read_parquet('data/2_processed/linkedin/matched_firm_role_output.parquet')\n",
    "chunks = np.array_split(role_output['user_id'].unique(), num_chunks)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path): \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            select user_id, prestige, highest_degree \n",
    "            from revelio.individual_user \n",
    "            where user_id IN %(user_ids)s\n",
    "            \"\"\",\n",
    "            params= {'user_ids': tuple(chunks[index].tolist())})\n",
    "        temp.to_parquet(file_path)\n",
    "    \n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],\n",
    "           ignore_index = True)\n",
    " .to_parquet('data/2_processed/linkedin/matched_firm_user_prestige.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437db715-baa5-4688-83c2-9541a919da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE AMOUNT OF WORKERS CURRENTLY WORKING OR WITH EXPERIENCE ABROAD PER COMPANY \n",
    "########################################################################################\n",
    "\n",
    "### SET PARAMETERS\n",
    "num_chunks = 500\n",
    "temp_direct = 'data/2_processed/linkedin/temp_user_output'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "matching_output = pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "role_output = pd.read_parquet('data/2_processed/linkedin/matched_firm_role_output.parquet').merge(matching_output)\n",
    "chunks = np.array_split(role_output['firmid'].unique(), num_chunks)\n",
    "linkedin_to_iso_cross_walk = (pd.read_csv('data/2_processed/admin/linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "\n",
    "### DEFINE FUNCTIONS \n",
    "def run_subsection(index):\n",
    "    clear_output(wait=True)\n",
    "    print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "\n",
    "    role_subset = role_output.loc[lambda x: x['firmid'].isin(chunks[index])]\n",
    "    params = {\"user_ids\":  tuple(role_subset['user_id'].unique())}\n",
    "    ever_role_subset = db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id, rcid, country, startdate, enddate, weight, total_compensation, seniority\n",
    "        FROM revelio.individual_positions \n",
    "        where user_id IN %(user_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "        )\n",
    "    print('finished scraping')\n",
    "    subset_output = pd.concat([collapse_year_level(year, role_subset, ever_role_subset) for year in range(2008, 2024)],\n",
    "                            ignore_index=True)\n",
    "    subset_output.to_parquet(temp_direct + \"/temp\"+str(index)+\".parquet\")\n",
    "\n",
    "    \n",
    "def collapse_year_level(year, role_subset, ever_role_subset):\n",
    "    valid_cols = ['valid_now', 'valid_ever', 'valid_l5'] \n",
    "    temp = (\n",
    "         ## Determine which users are active in a given year for a given firm \n",
    "         role_subset\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign(valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .loc[lambda x: x['valid'], ['firmid','user_id']].drop_duplicates()\n",
    "\n",
    "        ## add all the roles those users have ever held \n",
    "        .merge(ever_role_subset)\n",
    "\n",
    "        # drop all roles that start after the year of interest\n",
    "        .drop_duplicates()\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce'),\n",
    "                comp =lambda x: x['total_compensation']*x['weight'],\n",
    "               )\n",
    "         .loc[lambda x: x['startdate'].dt.year.le(year)]\n",
    "\n",
    "        ## Mark if the the position occured in the year of interest, within 5 years of the year of interest or ever.\n",
    "        ## Second assign step restricts to only the highest compensation value over the validity period. Stops us from double counting promotions etc. \n",
    "        .assign(valid_ever = lambda x: x['comp'] == x.groupby(['user_id', 'firmid', 'country'])['comp'].transform('max'),\n",
    "                valid_now = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "                valid_l5 = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year-5)))\n",
    "        .assign(valid_now = lambda x: x['valid_now'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid_now'])['comp'].transform('max')),\n",
    "                valid_l5 = lambda x: x['valid_l5'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid_l5'])['comp'].transform('max')))\n",
    "\n",
    "        ## now melt the data frame \n",
    "        .melt(\n",
    "        id_vars=['country', 'firmid', 'comp', 'weight'],\n",
    "        value_vars=valid_cols,\n",
    "        var_name='valid_type',\n",
    "        value_name='valid_flag')\n",
    "        .loc[lambda x: x['valid_flag']]\n",
    "\n",
    "        ## collapse by country firmid valid_type \n",
    "        .groupby(['country', 'firmid', 'valid_type'])\n",
    "        .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "        .reset_index()\n",
    "\n",
    "        ## reshape back to wide \n",
    "        .pivot(index=['firmid', 'country'], columns='valid_type', values=['empl', 'comp'])\n",
    "    )\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns = temp.columns.str.replace('valid_', '', regex=True)\n",
    "    temp['year'] = year\n",
    "    return(temp)\n",
    "\n",
    "### EXECUTE SCRAPING AND INITIAL COLLAPSE TO YEAR-firmid-ctry LEVEL \n",
    "[run_subsection(index) for index in range(num_chunks)]\n",
    "\n",
    "### Match to ISO-2 CODES AND EXPORT \n",
    "combined_output = (\n",
    "    pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "    .merge(linkedin_to_iso_cross_walk, how = 'left'))\n",
    "\n",
    "columns_to_sum = [col for col in combined_output.columns if 'comp' in col or 'empl' in col]\n",
    "columns_to_keep = ['firmid', 'year','ctry'] + columns_to_sum\n",
    "\n",
    "combined_output = pd.concat(\n",
    "    [combined_output.loc[lambda x: ~x['needs_collapse'], columns_to_keep],\n",
    "    combined_output.loc[lambda x: x['needs_collapse']].groupby(['firmid', 'year', 'ctry'])[columns_to_sum].sum().reset_index()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "combined_output.to_parquet('data/2_processed/linkedin/matched_firm_foreign_employment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9242ac-cb67-4e46-a929-d78a422f279f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c5bfc8bec7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Generate our compensation / employment datasets at the year level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrole_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2_processed/linkedin/revelio_role_dict.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mint_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'role_k1500'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'french_data'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmatching_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2_processed/admin/fuzzy_matching_output_final.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rcid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'siren'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'siren'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'firmid'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# Generate our compensation / employment datasets at the year level \n",
    "########################################################################################\n",
    "role_dict = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv')\n",
    "int_cols = list((set(role_dict.columns) - {'role_k1500', 'Unnamed: 0'}) | {'french_data'})\n",
    "matching_output = pd.read_parquet('data/2_processed/admin/fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "\n",
    "### Get the firm info at the siren level \n",
    "base_vars = ['has_subsid', 'is_subsid', 'is_public', 'has_lei', 'french_hq', 'parent_non_french_hq']\n",
    "matched_firm_base_info = (\n",
    "    pd.read_parquet('data/2_processed/linkedin/matched_firm_base_info.parquet').merge(matching_output)\n",
    "    .assign(needs_collapse = lambda df: df.groupby('firmid')['firmid'].transform('count') > 1)\n",
    ")\n",
    "matched_firm_base_info = pd.concat(\n",
    "    [matched_firm_base_info.loc[lambda x: ~x['needs_collapse'], ['firmid'] + base_vars],\n",
    "     matched_firm_base_info.loc[lambda x: x['needs_collapse']].groupby('firmid', as_index=False)[base_vars].max()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "### generate the base for the role data collapse \n",
    "role_data = (\n",
    "    ## merge together all the component datasets \n",
    "    pd.read_parquet('data/2_processed/linkedin/matched_firm_role_output.parquet')\n",
    "    .merge(pd.read_parquet('data/2_processed/linkedin/matched_firm_user_prestige.parquet'), how = 'left')\n",
    "    .merge(matching_output)\n",
    "    .merge(role_dict, how = 'left')\n",
    "    \n",
    "    ## generate necessary variables  \n",
    "     .assign(french= lambda x: x['country'].eq(\"France\"),\n",
    "         comp =  lambda x: x['total_compensation']*x['weight'])\n",
    "    .assign(french_data = lambda x: x['data'] & x['french'])\n",
    ")\n",
    "\n",
    "### Carry out the collapse \n",
    "def collapse_wrapper(year):\n",
    "    clear_output(wait=True) \n",
    "    print(year)\n",
    "    temp = (\n",
    "        role_data\n",
    "        .assign(valid = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .assign(valid = lambda x: x['valid'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid'])['comp'].transform('max')))\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .assign(comp_weighted_prestige =lambda x: x['prestige']*x['comp']/ x.groupby(['firmid'])['comp'].transform('sum'),\n",
    "                weighted_prestige = lambda x: x['prestige']*x['weight'])\n",
    "    )\n",
    "    ## collapse the prestige variables since we don't need those broken out    \n",
    "    temp_prestige = temp.groupby('firmid', as_index=False)[['comp_weighted_prestige', 'weighted_prestige']].max()\n",
    "\n",
    "    temp = (temp\n",
    "     .melt(id_vars=['firmid', 'comp', 'weight'],\n",
    "            value_vars=int_cols,\n",
    "            var_name='type',\n",
    "            value_name='valid_flag')\n",
    "     .loc[lambda x: x['valid_flag'].eq(1)]\n",
    "     .groupby(['firmid', 'type'])\n",
    "     .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "     .reset_index()\n",
    "     .pivot(index=['firmid'], columns='type', values=['empl', 'comp']))\n",
    "\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp = pd.merge(temp, temp_prestige,  how='outer')\n",
    "    temp = temp.assign(year = year)\n",
    "    return(temp)\n",
    "role_annual_collapsed = pd.concat([collapse_wrapper(year) for year in range(2008, 2024)], ignore_index=True)\n",
    "cols_to_fill = [col for col in role_annual_collapsed.columns if col.startswith('empl') or col.startswith('comp')]\n",
    "role_annual_collapsed[cols_to_fill] = role_annual_collapsed[cols_to_fill].fillna(0)\n",
    "\n",
    "final_output = pd.merge(role_annual_collapsed, matched_firm_base_info, how = 'left')\n",
    "for var in int_cols[1:]:\n",
    "    final_output[f'share_comp_{var}'] = final_output[f'comp_{var}'] / final_output['comp_total']\n",
    "final_output.to_parquet('data/2_processed/linkedin/matched_firm_empl_and_linkedin_characteristics.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
