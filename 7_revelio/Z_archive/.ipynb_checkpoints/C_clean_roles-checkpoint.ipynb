{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9bf7bd-db99-4661-8444-2f502aee559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata', 'sys']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb36b2-be29-4a97-8579-ac8389edaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate Firm-Year Dataset \n",
    "########################################################################################\n",
    "collapsed_dir = 'data/2_processed/linkedin/role_components_collapsed/'\n",
    "country_dict = pd.read_csv('data/2_processed/linkedin/revelio_country_dict.csv')\n",
    "france_id = country_dict.loc[country_dict['country'] == 'France', 'country_id'].values[0]\n",
    "french_users = pd.read_parquet('data/2_processed/linkedin/user_components_by_country/users_'+ str(france_id)+\".parquet\" )\n",
    "\n",
    "role_dict = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv')\n",
    "role_files = os.listdir('data/2_processed/linkedin/role_components')\n",
    "year_range = range(2008,2024)\n",
    "int_vars = ['french', 'total', 'engineer', 'data','rnd','stem']\n",
    "role_dict_vars = ['role_id'] + int_vars[1:]\n",
    "\n",
    "### COLLAPSE ALL THE ROLE DATA TO THE FIRM-YEAR LEVEL \n",
    "os.mkdir(collapsed_dir)\n",
    "for i, role_file in enumerate(role_files):\n",
    "    output_file = collapsed_dir + role_file\n",
    "    if not os.path.exists(output_file):\n",
    "        print(i / len(role_files))\n",
    "        roles = (pd.read_parquet('data/2_processed/linkedin/role_components/' + role_file)\n",
    "                 .rename(columns={'role_k1500': 'role_id'}))\n",
    "        roles = pd.merge(roles, role_dict[role_dict_vars], on='role_id', how='left')\n",
    "        roles['french'] = ((roles['role_country'] == 'France') | (roles['user_id'].isin(french_users['user_id']))).astype(int)\n",
    "        roles[['startdate', 'enddate']] = roles[['startdate', 'enddate']].apply(pd.to_datetime)\n",
    "        \n",
    "        output_list = []\n",
    "        for year in year_range:\n",
    "            # Create a temporary DataFrame with 'valid' column indicating if 'startdate' <= year <= 'enddate' or 'enddate' is NA\n",
    "            temp = roles.copy()\n",
    "            temp['valid'] = ((temp['startdate'].dt.year <= year) & \\\n",
    "                             ((temp['enddate'].dt.year >= year) | temp['enddate'].isna())).astype(int)\n",
    "            \n",
    "            # Pre compute interest columns\n",
    "            for col in int_vars:\n",
    "                temp[f'emp_{col}'] = temp[col] * temp['valid'] * temp['weight']\n",
    "                temp[f'comp_{col}'] = temp[col] * temp['valid'] * temp['weight']*temp['total_compensation'] / 1000\n",
    "            \n",
    "            output = temp.groupby('rcid').agg({\n",
    "                **{f'emp_{col}': 'sum' for col in int_vars},\n",
    "                **{f'comp_{col}': 'sum' for col in int_vars}\n",
    "            }).reset_index()\n",
    "\n",
    "            for col in int_vars: \n",
    "                output[f'emp_{col}'] = round(output[f'emp_{col}'])\n",
    "\n",
    "            output['share_emp_french'] = output['emp_french'] /output['emp_total'] \n",
    "            output['share_comp_french'] = output['comp_french'] / output['comp_total']\n",
    "            # add year\n",
    "            output['year'] = year\n",
    "            output_list.append(output)\n",
    "        pd.concat(output_list).sort_values(by=['rcid']).to_parquet(output_file)\n",
    "\n",
    "### CONCATENATE ALL THE CLEANED FILES AND THEN DELETE COMPONENTS \n",
    "output_list = []\n",
    "for i, role_file in enumerate(role_files):\n",
    "    print(i / len(role_files))\n",
    "    output_file = collapsed_dir + role_file\n",
    "    output_list.append(pd.read_parquet(output_file))\n",
    "\n",
    "output_list = pd.concat(output_list)\n",
    "output_list.to_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "shutil.rmtree(collapsed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b695f1f-822a-4b03-b8c3-623140f66c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# USE ROLE DATA TO DETERMINE WHICH FIRMS ARE LIKELY FRENCH \n",
    "########################################################################################\n",
    "\n",
    "french_leis = pd.read_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet')['lei'].unique()\n",
    "non_french_country_domains = (pd.read_excel('data/2_processed/admin/domain_names_by_country.xlsx')\n",
    "                              .assign(name=lambda x: x['name'].str.replace('.', '', regex=False))\n",
    "                              .query('include != include')['name'])\n",
    "french_factset_ids = pd.read_parquet('data/2_processed/admin/factset_french_domiciled.parquet')['factset_entity_id'].unique()\n",
    "\n",
    "companies = (\n",
    "    ## determine the firm's max total / french / data values and shares \n",
    "    pd.read_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "    .groupby('rcid', as_index=False)\n",
    "    .agg({'emp_total': 'max','emp_french': 'max', 'share_emp_french': 'max', 'share_comp_french': 'max', 'emp_data': 'max'})\n",
    "    .assign(french_eligible = lambda c: c['emp_french'].gt(0),\n",
    "         data_eligible = lambda c: c['emp_data'].gt(0))\n",
    "    .assign(role_eligible = lambda c: c['data_eligible'] & c['french_eligible'])\n",
    "    [['rcid', 'french_eligible', 'data_eligible','role_eligible', 'share_emp_french', 'share_comp_french']].\n",
    "    merge(pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet'))\n",
    "    \n",
    "    ### determine whether the firm likely french or not \n",
    "    .assign(url_ending = lambda c: c['url'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) else None))\n",
    "    .assign(\n",
    "        admin_score=lambda c: 0\n",
    "        ## TOP LEVEL DOMAIN \n",
    "        + c['url_ending'].eq('fr')  \n",
    "        - c['url_ending'].isin(non_french_country_domains)  \n",
    "\n",
    "        ## FACTSET\n",
    "        + c['factset_entity_id'].isin(french_factset_ids)\n",
    "        - (~c['factset_entity_id'].isin(french_factset_ids) & c['factset_entity_id'].notna())\n",
    "\n",
    "        # LEI CHECK \n",
    "        +  c['lei'].isin(french_leis) \n",
    "        -  (~c['lei'].isin(french_leis) & c['lei'].notna()) \n",
    "\n",
    "        # ISIN \n",
    "        + c['isin'].str[:2].eq(\"FR\") # add if french isin\n",
    "        - (~c['isin'].str[:2].eq(\"FR\") & c['isin'].notna())\n",
    "\n",
    "        # CUSIP \n",
    "        + c['cusip'].str[:1].eq(\"F\") # add if french cusip\n",
    "        - (~c['cusip'].str[:1].eq(\"F\") & c['cusip'].notna())\n",
    "\n",
    "        # Firm type \n",
    "        + c['firm_type_french_likelihood'].eq(\"likely french\") \n",
    "        - c['firm_type_french_likelihood'].eq(\"unlikely french\"))\n",
    "     .assign(\n",
    "         likely_french = lambda c: \n",
    "         c['french_eligible'] & (\n",
    "         c['admin_score'].gt(0) | \n",
    "         (c['admin_score'].eq(0) & (c['share_emp_french'].gt(.5) | c['share_comp_french'].gt(.5)))))\n",
    "    \n",
    "    #### mark whether it's a subsidiary \n",
    "    .assign(subsidiary = lambda c: c['rcid'] != c['ultimate_parent_rcid'])\n",
    "    [['rcid','lei','company','company_cleaned', 'company_stripped', 'year_founded', 'ultimate_parent_rcid',\n",
    "      'french_eligible', 'data_eligible', 'role_eligible', 'likely_french', 'subsidiary']]\n",
    ")\n",
    "companies.to_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
