{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cfed99-f98d-4719-8f92-8f1dd2ac1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os','math', 'glob',\n",
    "           're', 'pickle', 'numpy as np',\n",
    "           'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed',\n",
    "          'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata', 'sys', 'numpy as np', 'glob','from sklearn.decomposition import PCA']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "processed_linkedin = '1) data/15_revelio_outputs/1_inputs/b_processed_data/linkedin/'\n",
    "processed_admin = '1) data/15_revelio_outputs/1_inputs/b_processed_data/admin/'\n",
    "import A_helper_functions as hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd16b17-2c28-471a-9894-13745089d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Perform PCA to generate metrics of differentiation \n",
    "########################################################################################\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, weight_var, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=(weight_var, 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'{weight_var}_PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren': 'firmid'})\n",
    "output = pd.merge(long_data, matching_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d75eb9-58a7-4338-9b59-a206dc30c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pca analysis\n",
    "for weight_var in ['weight']:\n",
    "    print('starting pca gen')\n",
    "    pca_model = collapse_year_level(sample_year, True, weight_var)\n",
    "    print('finished pca gen')\n",
    "    [collapse_year_level(year,False,weight_var,pca_model) for year in range(2018, 2024)]\n",
    "    (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "     .to_parquet(processed_linkedin + 'matched_firm_pca_'+ weight_var + '_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a01f23-44ac-4107-b894-ea1f7c09bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_var = 'weight'\n",
    "pca_model = collapse_year_level(sample_year, True, weight_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc672c84-8283-40c1-84a5-deed670652f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "temp = (\n",
    "    output.assign(\n",
    "        valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                        (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "        wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "    )\n",
    "    .loc[lambda x: x['valid']]\n",
    "    .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "    .agg(comp=(weight_var, 'sum'))\n",
    "    .assign(year=year)\n",
    "    .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "    .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "    .replace([np.inf, -np.inf], np.nan)\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f07f5-572e-43b8-aaf0-1356f3e3b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_var = 'weight'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
