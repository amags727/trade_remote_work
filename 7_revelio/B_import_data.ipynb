{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfd671-5d10-48dd-ac45-3a11040ee46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os','math', 'glob',\n",
    "           're', 'pickle', 'numpy as np',\n",
    "           'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed',\n",
    "          'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata', 'sys', 'numpy as np', 'shutil', 'itertools', 'from langdetect import detect, DetectorFactory',\n",
    "          'shutil']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf\n",
    "raw_admin = '1) data/15_revelio_data/1_inputs/a_raw_data/admin/'\n",
    "processed_linkedin = '1) data/15_revelio_data/1_inputs/b_processed_data/linkedin/'\n",
    "processed_admin = '1) data/15_revelio_data/1_inputs/b_processed_data/admin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cbda8-00fa-47e2-a32f-876f4f6de32e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## GENERATE THE LIST OF SIREN / FIRM NAMES \n",
    "## set parameters and define the wrapper \n",
    "chunks =  os.cpu_count() - 10; cut_off = .01\n",
    "def wrapper(index, function):\n",
    "    if function == \"clean\":\n",
    "        return(hf.clean_firm_names(siren_chunks[index], \"admin_name\", False))\n",
    "    else:\n",
    "        return(hf.strip_words(siren_chunks[index], 'admin_name_cleaned', common_words))          \n",
    "\n",
    "################\n",
    "### IMPORT THE SIREN NUMBERS \n",
    "################\n",
    "siren_numbers = (\n",
    "    ##import\n",
    "    pd.read_csv(raw_admin + '1_StockUniteLegaleHistorique_utf8.csv',\n",
    "                usecols=['denominationUniteLegale', 'siren', 'dateDebut', 'dateFin', \n",
    "                         'activitePrincipaleUniteLegale','nomenclatureActivitePrincipaleUniteLegale', 'etatAdministratifUniteLegale'],\n",
    "                dtype = {'siren': 'str'})\n",
    "    \n",
    "    #rename columns\n",
    "    .rename(columns={'denominationUniteLegale': 'admin_name', 'dateDebut': 'start_date', 'dateFin': 'end_date', 'etatAdministratifUniteLegale': 'status',\n",
    "                     'activitePrincipaleUniteLegale' : 'industry', 'nomenclatureActivitePrincipaleUniteLegale' : 'industry_system'}) \n",
    "   \n",
    "    # fix date variables \n",
    "     .assign(start_date=lambda df: pd.to_datetime(df['start_date'], errors='coerce'),\n",
    "             end_date=lambda df: pd.to_datetime(df['end_date'], errors='coerce'))\n",
    "    # filter \n",
    "    .loc[lambda df: df['admin_name'].notna() & ~df['admin_name'].eq('[ND]') & ~df['status'].eq('C')]\n",
    ")\n",
    "################\n",
    "### NOTE THE INDUSTRIES OF EACH SIREN FOR THE PERIOD OF INTEREST \n",
    "################\n",
    "industry_year_dta = []\n",
    "for year in range(2008,2024):\n",
    "    industry_year_dta.append(\n",
    "        siren_numbers.loc[lambda c: c['start_date'].dt.year.le(year) & c['end_date'].dt.year.ge(year)]\n",
    "        .sort_values(by = ['siren','end_date'], ascending = [True,False])\n",
    "        .groupby('siren').head(1)\n",
    "       .assign(year = year)\n",
    "       [['siren','year', 'industry', 'industry_system']]\n",
    "    )\n",
    "pd.concat(industry_year_dta, ignore_index = True).to_parquet(processed_admin +'siren_industry_year.parquet')\n",
    "\n",
    "################\n",
    "### RETRIEVE THE START AND END DATE OF THE SIREN / Name Combo (this method is orders of magnitudes faster than aggregating)\n",
    "################\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "start_dates = (siren_numbers.loc[lambda c: c['start_date'].notna()]\n",
    "               .sort_values(['combo','start_date'])\n",
    "               .groupby(['combo']).head(1)[['siren','admin_name','start_date']])\n",
    "\n",
    "na_end_dates = (siren_numbers.loc[lambda c: c['end_date'].isna()]\n",
    "                .drop_duplicates(subset = 'combo')[['siren','admin_name','combo','end_date']])\n",
    "\n",
    "end_dates = (siren_numbers.loc[lambda c: ~c['combo'].isin(na_end_dates['combo'])]\n",
    "            .sort_values(['combo','end_date'], ascending = [True, False])\n",
    "            .groupby('combo').head(1))\n",
    "siren_numbers = (pd.merge(start_dates, pd.concat([na_end_dates,end_dates])[['siren','admin_name','end_date']], how = 'outer')\n",
    "                .loc[lambda x: x['end_date'].dt.year.gt(2007) | x['end_date'].isna()])\n",
    "\n",
    "################\n",
    "## Generate Cleaned Names \n",
    "################\n",
    "siren_chunks = np.array_split(siren_numbers,chunks); \n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'clean') for index in range(chunks)), ignore_index = True)\n",
    "\n",
    "#establish / remove the list of common words \n",
    "word_counts = siren_numbers['admin_name_cleaned'].str.split(expand = True).stack().value_counts()\n",
    "common_words = set(word_counts[word_counts > np.max(word_counts) * cut_off].index)\n",
    "common_words = {word for word in common_words if not word.isnumeric()}\n",
    "with open(processed_admin + 'common_words.txt', 'w') as file: file.write('\\n'.join(common_words))\n",
    "\n",
    "\n",
    "### use the common words to clean the firms: \n",
    "siren_chunks = np.array_split(siren_numbers,chunks)\n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'strip') for index in range(chunks)), ignore_index = True)\n",
    "siren_numbers.to_parquet(processed_admin +'siren_admin.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d40b78-1634-40a5-81d0-a72bdab2cd72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##Generate a list of all french users\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "french_users = db.raw_sql(\"\"\"\n",
    "SELECT user_id \n",
    "FROM revelio.individual_user \n",
    "WHERE user_country = 'France'\n",
    "\"\"\")\n",
    "french_users.to_parquet( processed_linkedin + 'all_french_users.parquet')\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0210183-c5cd-4a06-bf4a-8ef339bb3a27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Generate OUR ROLE DICTIONARY \n",
    "to_drop = ['role_k1000', 'role_k500', 'role_k300', 'role_k150', 'role_k50', 'job_category', 'onet_code', 'onet_title','role_id']\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "roles = (db.raw_sql(\"select * from revelio.individual_role_lookup\")\n",
    "              .applymap(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "\n",
    "roles['role_id'] = roles.index\n",
    "roles['total'] = True\n",
    "roles['engineer'] = roles['job_category'] == 'engineer'\n",
    "roles['data'] = (\n",
    "    (roles['role_k50'] == 'data analyst') |\n",
    "    (roles['role_k150'].str.contains('data', na=False)) |\n",
    "    (roles['onet_title'].str.contains('database', na=False)) |\n",
    "    (roles['role_k1500'].str.contains('data center', na=False))\n",
    ")\n",
    "roles['data_analyst'] = (roles['data'] & \n",
    "                         roles['role_k50'].str.contains('analyst', na=False) |\n",
    "                         roles['role_k1500'].str.contains('intelligence', na=False))\n",
    "roles['data_engineer'] = (roles['data'] & ~roles['data_analyst'])\n",
    "                         \n",
    "                         \n",
    "rnd = pd.read_excel(raw_admin + \"ONET_RandD_roles.xlsx\").assign(rnd=True)[['Code', 'rnd']]\n",
    "stem = pd.read_excel(raw_admin + \"ONET_stem_roles.xlsx\").assign(stem=True)[['Code', 'stem']]\n",
    "roles = (roles.merge(rnd, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "           .merge(stem, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "            .assign(rnd=lambda x: x['rnd'].fillna(False))\n",
    "           .assign(stem=lambda x: x['stem'].fillna(False))\n",
    "         .applymap(lambda x: int(x) if isinstance(x, bool) else x))\n",
    "roles['non_data_rnd'] = roles['rnd'] & ~roles['data'];\n",
    "roles.drop(to_drop, axis = 1)\n",
    "roles.to_csv(processed_linkedin +'revelio_role_dict.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c85336-f45e-44e8-a013-7497eba76dbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## GEN LIST OF FRENCH FACTSET IDS \n",
    "db.raw_sql(\"\"\"\n",
    "select factset_entity_id \n",
    "from factset.edm_standard_address \n",
    "where iso_country = 'FR'\n",
    "\"\"\").to_parquet(processed_admin +'factset_french_domiciled.parquet')\n",
    "db.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47fccb-310e-42da-91e2-1e5d159211ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## OBSERVE THE NUMBER OF DATA WORKERS IN EACH MARKET \n",
    "# set parameters  \n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "year_range = range(2008,2024)\n",
    "data_roles = pd.read_csv(processed_linkedin +'revelio_role_dict.csv') \\\n",
    "    .loc[lambda x: x['data'].eq(1), 'role_k1500'] \\\n",
    "    .tolist()\n",
    "temp_direct = processed_linkedin +'temp_ctry_output'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "linkedin_to_iso_cross_walk = pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "    \n",
    "\n",
    "# define helper functions \n",
    "def collapse_wrapper(year, data_output):\n",
    "    data_output['startdate'] = pd.to_datetime(data_output['startdate'], errors='coerce')\n",
    "    data_output['enddate'] = pd.to_datetime(data_output['enddate'], errors='coerce')\n",
    "    data_output = (\n",
    "        data_output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            comp =lambda x: x['total_compensation']*x['weight']).\n",
    "        loc[lambda x: x['valid']].\n",
    "        groupby('ctry').agg(\n",
    "            ctry_data_empl=('weight', 'sum'),\n",
    "            ctry_data_comp=('comp', 'sum')\n",
    "        ).reset_index().assign(year = year))\n",
    "    return(data_output)\n",
    "\n",
    "\n",
    "## GENERATE THE NUMBER / COMPENSATION FOR ALL DATA ROLES IN EACH COUNTRY \n",
    "possible_combos = pd.DataFrame(itertools.product(linkedin_to_iso_cross_walk['ctry'].unique(), year_range), columns=['ctry', 'year'])\n",
    "data_roles_output = (\n",
    "    db.raw_sql(\n",
    "        \"\"\"\n",
    "       SELECT country, weight, total_compensation, startdate, enddate \n",
    "       FROM revelio.individual_positions \n",
    "       WHERE role_k1500 IN %(data_roles)s\n",
    "       \"\"\", \n",
    "        params= {\"data_roles\": tuple(data_roles)})\n",
    "    .merge(linkedin_to_iso_cross_walk))\n",
    "\n",
    "data_roles_output = (\n",
    "    pd.concat([collapse_wrapper(year, data_roles_output) for year in year_range])\n",
    "    .merge(possible_combos, how = 'right')\n",
    "    .assign(ctry_data_empl=lambda x: x['ctry_data_empl'].fillna(0),\n",
    "            ctry_data_comp=lambda x: x['ctry_data_comp'].fillna(0))\n",
    ")\n",
    "data_roles_output.to_parquet(processed_linkedin +'data_roles_in_all_countries.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483efa18-6d46-4a95-a0cd-2c2148b475d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Observe the Origin Universities /graduation dates of French Data Scientists\n",
    "year_range = range(2008,2024)\n",
    "cores =  os.cpu_count() - 10; \n",
    "data_roles = pd.read_csv(processed_linkedin +'revelio_role_dict.csv').loc[lambda x: x['data'].eq(1), 'role_k1500'].tolist()\n",
    "params = {\"data_roles\": tuple(data_roles)}\n",
    "role_output = db.raw_sql(\n",
    "    \"\"\"\n",
    "    SELECT user_id, weight, total_compensation, startdate, enddate \n",
    "    FROM revelio.individual_positions \n",
    "    WHERE role_k1500 IN %(data_roles)s AND country = 'France'\n",
    "    \"\"\", \n",
    "    params=params)\n",
    "\n",
    "params = {\"data_ids\": tuple(role_output['user_id'].tolist())}\n",
    "user_output = (\n",
    "    db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM revelio.individual_user_education \n",
    "        where user_id IN %(data_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "        ) \n",
    "    .loc[lambda x: ~x['rsid'].isna() & ~x['enddate'].isna()] \n",
    "    .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "            enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "    .assign(grad_year=lambda x: x['enddate'].dt.year))\n",
    "\n",
    "def university_collapse_wrapper(year):\n",
    "    temp = (\n",
    "        role_output.assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                              enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign(valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "                comp =lambda x: x['total_compensation']*x['weight'])\n",
    "        .loc[lambda x: x['valid']] \n",
    "        .merge(user_output.rename(columns = {'startdate': 'uni_start_date','enddate': 'uni_end_date'})) \n",
    "        .loc[lambda x: x['uni_end_date'].le(x['startdate'])]\n",
    "        .loc[lambda x: x.groupby('user_id')['uni_end_date'].idxmax()]\n",
    "        .loc[lambda x: x['university_country'].eq('France')]\n",
    "        .groupby(['university_name', 'grad_year', 'rsid', 'university_location', 'university_country'])\n",
    "        .agg(data_grads =('weight', 'sum'),\n",
    "             comp_weighted_data_grads=('comp', 'sum'))\n",
    "        .assign(observation_year = year)\n",
    "    ).reset_index()\n",
    "    return(temp)\n",
    "\n",
    "yr_lvl_dta_uni = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')(delayed(university_collapse_wrapper)(year) for year in year_range),ignore_index = True)\n",
    "yr_lvl_dta_uni.to_parquet(processed_linkedin +'data_grads_across_france.parquet')\n",
    "\n",
    "### GENERATE A LIST OF LOCATIONS SO WE CAN START TRYING TO MATCH TO FIRM DATA \n",
    "simple_uni_x_location = (\n",
    "    pd.merge(yr_lvl_dta_uni,\n",
    "             yr_lvl_dta_uni.groupby('university_location', as_index=False)['data_grads'].max())\n",
    "    [['university_name', 'university_location']].drop_duplicates()\n",
    "    .merge(yr_lvl_dta_uni[['university_name','university_location']]\n",
    "           .drop_duplicates()\n",
    "           .assign(num_unis = lambda df: df.groupby('university_location')['university_location'].transform('count'))\n",
    "           [['university_location', 'num_unis']].drop_duplicates())\n",
    ")\n",
    "simple_uni_x_location.to_excel(processed_admin +'uni_x_location_raw.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bad472-62ad-4ffc-b31e-a1fc8fbfee25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Generate A CROSS BETWEEN LEI AND SIREN CODES \n",
    "#When INSEE is the managing Local Operating Unit (LOU) or the firm is french, it identifies firms with SIREN codes.\n",
    "##The initial list of LEI codes is provided by the GLIEF\n",
    "##(https://search.gleif.org/#/search/simpleSearch=France&fulltextFilterId=LEIREC_FULLTEXT&currentPage=1&perPage=15&expertMode=false).\n",
    "\n",
    "# File path\n",
    "file_path = raw_admin + '20241105-0000-gleif-goldencopy-lei2-golden-copy.csv'\n",
    "\n",
    "# Columns of interest and their new names\n",
    "interest_cols = ['LEI','Entity.LegalName','Entity.LegalAddress.Country', 'Entity.RegistrationAuthority.RegistrationAuthorityEntityID','Registration.ManagingLOU']\n",
    "new_names = [\"lei\", \"lei_name\", 'lei_country', 'lei_siren',\"managing_lou\"]\n",
    "crosswalk = pd.read_csv(file_path, usecols=interest_cols, low_memory=False)\n",
    "crosswalk.columns = new_names\n",
    "\n",
    "crosswalk = (crosswalk.assign(insee_registered = lambda c: c['managing_lou'] == '969500Q2MA9VBQ8BG884',\n",
    "             lei_siren = lambda c: c['lei_siren'].fillna('').astype(str).apply(lambda x: re.sub(r'[^a-zA-Z0-9]', '',x)))\n",
    "             .loc[lambda x: (x['lei_country'].eq(\"FR\") | x['insee_registered'])])\n",
    "\n",
    "crosswalk.to_parquet(processed_admin +'LEI_siren_crosswalk.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1f2a5-b171-4811-a67a-b08dda03206f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FIND ALL FIRMS THAT HAVE HIRED A FRENCH USER \n",
    "french_users = pd.read_parquet(processed_linkedin + 'all_french_users.parquet')\n",
    "num_chunks = 500\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(french_users['user_id'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'user_id_list': tuple(chunks[index].tolist())}\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT DISTINCT rcid\n",
    "            FROM revelio.individual_positions \n",
    "            WHERE user_id IN %(user_id_list)s AND rcid IS NOT NULL\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "         .drop_duplicates().loc[lambda x: ~x['rcid'].isna()])\n",
    "output.to_parquet(processed_linkedin + 'all_french_user_rcids.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb7dfc-fa99-41eb-b246-ce35e742dc87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FIND ALL FIRMS THAT HAVE A POSITION LOCATED IN FRANCE \n",
    "french_roles = db.raw_sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT rcid\n",
    "    FROM revelio.individual_positions\n",
    "    WHERE country = 'France' AND rcid IS NOT NULL\n",
    "    \"\"\",\n",
    ")\n",
    "french_roles.to_parquet(processed_linkedin + 'all_french_role_rcids.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23136cd1-8ec5-4365-a794-268d548d9722",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FIND ALL FIRMS THAT ARE HEADQUARTERED IN FRANCE \n",
    "french_hqs = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT DISTINCT rcid\n",
    "            FROM revelio.company_mapping\n",
    "            WHERE hq_country = 'France' AND rcid IS NOT NULL\n",
    "            \"\"\"\n",
    "        ).to_parquet(processed_linkedin + 'all_french_hq_rcids.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd9d34-4633-4ecc-9bd8-86e15585ee79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FIND ALL FIRMS THAT HAVE A FRENCH LEI \n",
    "french_leis = pd.read_parquet(processed_admin +'LEI_siren_crosswalk.parquet')\n",
    "num_chunks = 50\n",
    "temp_direct = os.path.join(processed_linkedin, 'temp')\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(french_leis['lei'].dropna().unique(), num_chunks)\n",
    "\n",
    "for index in reversed(range(num_chunks)):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        lei_list = tuple(chunks[index].tolist())\n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "        temp = db.raw_sql(\n",
    "                    \"\"\"\n",
    "                    SELECT  rcid\n",
    "                    FROM revelio.company_mapping\n",
    "                    WHERE lei IN %(lei_list)s\n",
    "                    \"\"\",  \n",
    "            params= {\"lei_list\": lei_list}).to_parquet(file_path)\n",
    "\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(os.path.join(temp_direct, \"*.parquet\"))],ignore_index=True)\n",
    "output.to_parquet(processed_linkedin + 'all_french_lei_rcids.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda115d-0879-4044-8592-763f3f51a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERATE FIRM LEVEL DATA ON PROSPECTIVE FRENCH COMPANIES \n",
    "\n",
    "## SET PARAMETER VALUES \n",
    "def safe_detect(x):\n",
    "    try:\n",
    "        return detect(x) if pd.notnull(x) else None\n",
    "    except:\n",
    "        return None\n",
    "DetectorFactory.seed = 0 \n",
    "\n",
    "all_french_rcids = (pd.concat([\n",
    "    pd.read_parquet(os.path.join(processed_linkedin, 'all_french_role_rcids.parquet')),\n",
    "    pd.read_parquet(os.path.join(processed_linkedin, 'all_french_user_rcids.parquet')),\n",
    "    pd.read_parquet(processed_linkedin + 'all_french_hq_rcids.parquet'),\n",
    "    pd.read_parquet(processed_linkedin + 'all_french_lei_rcids.parquet')])\n",
    "                    .drop_duplicates())\n",
    "common_words  = open(processed_admin +'common_words.txt', 'r').read().splitlines()\n",
    "french_leis = pd.read_parquet(processed_admin +'LEI_siren_crosswalk.parquet')['lei']\n",
    "french_factset_ids = pd.read_parquet(processed_admin +'factset_french_domiciled.parquet')['factset_entity_id']\n",
    "processed_admin +'factset_french_domiciled.parquet'\n",
    "\n",
    "# Prepare for chunking\n",
    "num_chunks = 500\n",
    "temp_direct = os.path.join(processed_linkedin, 'temp')\n",
    "shutil.rmtree(temp_direct)\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(all_french_rcids['rcid'].dropna().unique(), num_chunks)\n",
    "\n",
    "# SCRAPE / PROCESS DATA FOR EACH CHUNK \n",
    "for index in range(num_chunks):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "\n",
    "        rcid_list = tuple(chunks[index].tolist())\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM revelio.company_mapping\n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\",\n",
    "            params={\"rcid_list\": rcid_list}\n",
    "        )\n",
    "        temp = temp.loc[~temp['company'].isna() & ~temp['company'].eq('')]\n",
    "        temp = hf.clean_firm_names(temp, 'company',True)\n",
    "        temp = hf.strip_words(temp, 'company_cleaned', common_words)\n",
    "        temp = (temp\n",
    "                .assign(description_french = lambda x: (\n",
    "                    x['slogan'].apply(safe_detect).eq('fr') |\n",
    "                    x['description'].apply(safe_detect).eq('fr')))\n",
    "                .assign(admin_french = lambda x: x['hq_country'].eq('France')\n",
    "                        | x['url'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) else None).eq('fr')\n",
    "                        | x['factset_entity_id'].isin(french_factset_ids)\n",
    "                        | x['isin'].str[:2].eq(\"FR\")\n",
    "                        | x['lei'].isin(french_leis) \n",
    "                        | x['cusip'].str[:1].eq(\"F\")\n",
    "                        | x['firm_type_french_likelihood'].eq('likely french')\n",
    "                        | x['description_french']\n",
    "                        | x['hq_country'].eq('France')))\n",
    "        temp = temp[['rcid', 'company', 'company_cleaned','company_stripped', 'extracted_terms', 'lei',\n",
    "                    'child_rcid', 'ultimate_parent_rcid', 'hq_street_address',\n",
    "                     'hq_zip_code', 'hq_city', 'hq_metro_area', 'hq_state', 'hq_country',\n",
    "                     'hq_region', 'admin_french']]\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "# Combine and save full output\n",
    "output = (\n",
    "    pd.concat([pd.read_parquet(file) for file in glob.glob(os.path.join(temp_direct, \"*.parquet\"))],ignore_index=True)\n",
    "    .assign(has_subsid = lambda x: ~x['child_rcid'].isna(), \n",
    "             is_subsid = lambda x: ~x['ultimate_parent_rcid'].isna(),\n",
    "             has_lei = lambda x: ~x['lei'].isna(),\n",
    "             french_hq = lambda x: np.where(x['hq_country'].isna(), pd.NA, x['hq_country'] == 'France')))\n",
    "\n",
    "output.to_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e70a7-536c-416e-9eab-63feb4ed8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Roles for Firms that are not admin_french\n",
    "non_french_admin = (\n",
    "    pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "    .loc[lambda x: ~x['admin_french']])\n",
    "\n",
    "num_chunks = 500\n",
    "temp_direct = os.path.join(processed_linkedin, 'temp')\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(non_french_admin['rcid'].dropna().unique(), num_chunks)\n",
    "french_users = pd.read_parquet( processed_linkedin + 'all_french_users.parquet')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,country, weight, total_compensation\n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        temp.loc[temp['user_id'].isin(french_users['user_id']),'country' ] = 'France'\n",
    "        temp = (\n",
    "            temp.loc[~temp['country'].isna()]\n",
    "            .assign(french = lambda x: x['country'].eq('France'),\n",
    "                    comp = lambda x: x['weight'] * x['total_compensation'])\n",
    "            .groupby('rcid', as_index=False)\n",
    "            .apply(lambda g: pd.Series({\n",
    "                  'share_comp_french': g.loc[g['french'], 'comp'].sum() / g['comp'].sum(),\n",
    "                  'share_emp_french': g.loc[g['french'], 'weight'].sum() / g['weight'].sum()})).reset_index()\n",
    "            .assign(role_french = lambda x: x['share_comp_french'].ge(.5) | x['share_emp_french'].ge(.5))\n",
    "            )[['rcid', 'role_french']].to_parquet(file_path)\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(os.path.join(temp_direct, \"*.parquet\"))],ignore_index=True)\n",
    "full_processed_linkedin = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "                           .merge(output, how = 'left')\n",
    "                          .assign(french_eligible = lambda x: x['role_french'].isna() | x['role_french']))\n",
    "\n",
    "full_processed_linkedin.to_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458af870-3737-4753-ba19-a5a841502946",
   "metadata": {},
   "source": [
    "The matching procedure runs in several steps. \n",
    "\n",
    "First, we have the LEI-siren matches for a subset of firms, we identify those and then remove those sirens / rcids from the pool of firms to match. \n",
    "\n",
    "Second, we remove all sirens / rcids that have the same cleaned name as another siren / rcid. We won't be able to match these uniquely and therefore won't consider them. \n",
    "\n",
    "Third, we match all remaining sirens / rcids based on whether they have a unique match based on their cleaned name. We then remove these matches from the list of candidates.\n",
    "\n",
    "Fourth. We identify which of the rcids remaining have a unique perfect match based on the stripped name. Those that don't are discarded. We then match based on the stripped name using the fuzzy matching command \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5937b-ff36-4512-9d37-66edd7ad9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Matching \n",
    "############  \n",
    "### DEFINE PARAMETERS AND IMPORT DATA / Matching Function\n",
    "############  \n",
    "init_matches = 50; final_matches = 5; cores = max(os.cpu_count() - 2, 1);\n",
    "chunks = cores*10\n",
    "\n",
    "def matching_wrapper(index):\n",
    "    #output progress \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{round(index/chunks*100, 2)}%\")\n",
    "    \n",
    "    temp_firms = firm_chunks[index]\n",
    "    #run first version of matcher on all words \n",
    "    matches = matcher.match_names(to_be_matched=temp_firms, column_matching='company_cleaned')\n",
    "    results = (pd.wide_to_long(matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "               .reset_index()[['original_name', 'match_name', 'score']]\n",
    "               .rename(columns={'original_name': 'company_cleaned', 'match_name': 'admin_name_cleaned', 'score': 'raw_score'})\n",
    "               .merge(temp_firms[['company_cleaned', 'company_stripped']], how = 'left', on = 'company_cleaned')\n",
    "               .merge(sirens_to_match[['admin_name_cleaned', 'admin_name_stripped']], how = 'left', on = 'admin_name_cleaned'))\n",
    "    company_chunks = [group for _, group in results.groupby('company_cleaned')]\n",
    "\n",
    "    ### reun the second version of matcher only on words from initial list \n",
    "    results = []\n",
    "    temp_matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "    temp_matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "    for chunk in company_chunks:\n",
    "        chunk = chunk.reset_index()\n",
    "        try:\n",
    "            temp_matcher.load_and_process_master_data(column='admin_name_stripped', df_matching_data=chunk, transform=True)\n",
    "            temp_matches = temp_matcher.match_names(to_be_matched=chunk.iloc[0], column_matching='company_stripped')\n",
    "            temp_results = (pd.wide_to_long(temp_matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "                            .reset_index()[['original_name', 'match_name', 'score']]\n",
    "                            .rename(columns={'original_name': 'company_stripped', 'match_name': 'admin_name_stripped', 'score': 'stripped_score'})\n",
    "                            .drop_duplicates()\n",
    "                            .merge(chunk, how = 'right')\n",
    "                            .assign(match_index = lambda df: df.groupby(['stripped_score']).ngroup()))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing company: {chunk.loc[0,'company_cleaned']} in index {index}\") \n",
    "            temp_results = chunk.assign(match_index = lambda df: df.groupby(['raw_score']).ngroup())\n",
    "\n",
    "        temp_results = (temp_results\n",
    "                        .assign(match_index = lambda df: df['match_index'].max() - df['match_index'] + 1)\n",
    "                        .sort_values(['match_index','raw_score'], ascending = [True, False])\n",
    "                        .loc[lambda df: df['match_index'].le(final_matches)])\n",
    "        results.append(temp_results)\n",
    "    results = pd.concat(results, ignore_index = True)[['company_cleaned', 'admin_name_cleaned', 'raw_score', 'stripped_score', 'match_index']]\n",
    "    return(results)\n",
    "\n",
    "############  \n",
    "### Check for LEI matches \n",
    "############ \n",
    "french_leis = pd.read_parquet(processed_admin +'LEI_siren_crosswalk.parquet')[['lei', 'lei_siren']].rename(columns={'lei_siren': 'firmid'})\n",
    "firms_to_match = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet'))\n",
    "lei_matched = (pd.merge(french_leis, firms_to_match[['rcid','lei']])\n",
    "              .assign(match_method = 'lei')\n",
    "              [['rcid', 'firmid', 'match_method']])\n",
    "\n",
    "############  \n",
    "### Check for exact matches on cleaned names \n",
    "############ \n",
    "firms_to_match = (firms_to_match.loc[lambda x: x['french_eligible'] & ~x['rcid'].isin(lei_matched['rcid'])]\n",
    "                  .assign(count=lambda x: x.groupby('company_cleaned')['company_cleaned'].transform('count'))\n",
    "                  .loc[lambda x: x['count'] == 1]\n",
    "                 [['rcid','company_cleaned','company_stripped']])     \n",
    "\n",
    "sirens_to_match = (pd.read_parquet(processed_admin + 'siren_admin.parquet')\n",
    "                   .rename(columns = {'siren': 'firmid'})\n",
    "                   .loc[lambda x: ~x['firmid'].isin(lei_matched['firmid'])]\n",
    "                   .assign(count=lambda x: x.groupby('admin_name_cleaned')['admin_name_cleaned'].transform('count'))\n",
    "                   .loc[lambda x: x['count'] == 1]\n",
    "                   [['admin_name_cleaned', 'admin_name_stripped','firmid']])\n",
    "clean_matched = (\n",
    "    pd.merge(firms_to_match, sirens_to_match, how = 'inner',\n",
    "             left_on = 'company_cleaned', right_on = 'admin_name_cleaned')\n",
    "    .assign(match_method = 'cleaned')\n",
    "    [['rcid','firmid', 'match_method']])\n",
    "\n",
    "\n",
    "############  \n",
    "### match remaining firms \n",
    "############ \n",
    "sirens_to_match = (sirens_to_match.loc[lambda x: ~x['firmid'].isin(clean_matched['firmid'])] \n",
    "                   .assign(count=lambda x: x.groupby('admin_name_stripped')['admin_name_stripped'].transform('count')))\n",
    "\n",
    "firms_to_match = (firms_to_match\n",
    "                  .loc[lambda c: ~c['rcid'].isin(clean_matched['rcid']) \n",
    "                  & c['company_stripped'].isin(sirens_to_match.loc[lambda x: x['count'].eq(1), 'admin_name_stripped'])])\n",
    "\n",
    "firm_chunks = np.array_split(firms_to_match[['company_cleaned', 'company_stripped']], chunks)\n",
    "print('starting fuzzy matching')\n",
    "matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "matcher.load_and_process_master_data(column='admin_name_cleaned',\n",
    "                                     df_matching_data =sirens_to_match[['admin_name_cleaned', 'admin_name_stripped']],\n",
    "                                     transform=True)\n",
    "matching_output = Parallel(n_jobs=cores, backend='multiprocessing')(delayed(matching_wrapper)(index) for index in range(chunks))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "strip_matched = (pd.concat(matching_output, ignore_index = True)\n",
    "                 .loc[lambda x: x['match_index'].eq(1) & x['stripped_score'].eq(100)]\n",
    "                 .assign(match_method = 'strip',\n",
    "                         count=lambda x: x.groupby('company_cleaned')['company_cleaned'].transform('count'))\n",
    "                 .loc[lambda x: x['count'] == 1]\n",
    "                 .merge(sirens_to_match[['admin_name_cleaned', 'firmid']])\n",
    "                 .merge(firms_to_match[['company_cleaned', 'rcid']])\n",
    "                 [['rcid','firmid', 'match_method']])\n",
    "\n",
    "all_matches = pd.concat([lei_matched, clean_matched, strip_matched], ignore_index = True)\n",
    "final_output = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "              .merge(all_matches, on = 'rcid'))\n",
    "final_output.to_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
