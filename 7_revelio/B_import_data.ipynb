{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cda8629-b93b-483b-8d7d-6eff86307979",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os','math', 'glob',\n",
    "           're', 'pickle', 'numpy as np',\n",
    "           'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed',\n",
    "          'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata', 'sys', 'numpy as np', 'shutil']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf\n",
    "#db = wrds.Connection(wrds_username='am0195')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a3d30-abca-4d9c-ae46-bef052d1f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hi = pd.read_parquet('/Users/amagnuson/Downloads/02_25_export/1) new dummy data/16_bs_br_linkedin_big_sim.parquet')\n",
    "sample_df = hi.sample(frac=0.01, random_state=42)  # Set random_state for reproducibility\n",
    "sample_df.to_parquet('5) reduced_form_work/1) data/16_bs_br_linkedin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1803c6-be95-4dd3-bfdf-59238a0cbeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = hi.sample(frac=0.1, random_state=42)  # Set random_state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d6421-b8c1-43e8-a110-dc35c776163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f0218-c6d8-4bbb-93d0-22632af2dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate the list of SIREN / Firm Names\n",
    "########################################################################################\n",
    "## set parameters and define the wrapper \n",
    "chunks =  os.cpu_count() - 10; cut_off = .01\n",
    "def wrapper(index, function):\n",
    "    if function == \"clean\":\n",
    "        return(hf.clean_firm_names(siren_chunks[index], \"admin_name\", False))\n",
    "    else:\n",
    "        return(hf.strip_words(siren_chunks[index], 'admin_name_cleaned', common_words))          \n",
    "\n",
    "################\n",
    "### IMPORT THE SIREN NUMBERS \n",
    "################\n",
    "siren_numbers = (\n",
    "    ##import\n",
    "    pd.read_csv('../1_IWH/data/2_patent_tm_scraping/1_raw/1_StockUniteLegaleHistorique_utf8.csv',\n",
    "                usecols=['denominationUniteLegale', 'siren', 'dateDebut', 'dateFin', \n",
    "                         'activitePrincipaleUniteLegale','nomenclatureActivitePrincipaleUniteLegale', 'etatAdministratifUniteLegale'],\n",
    "                dtype = {'siren': 'str'})\n",
    "    \n",
    "    #rename columns\n",
    "    .rename(columns={'denominationUniteLegale': 'admin_name', 'dateDebut': 'start_date', 'dateFin': 'end_date', 'etatAdministratifUniteLegale': 'status',\n",
    "                     'activitePrincipaleUniteLegale' : 'industry', 'nomenclatureActivitePrincipaleUniteLegale' : 'industry_system'}) \n",
    "   \n",
    "    # fix date variables \n",
    "     .assign(start_date=lambda df: pd.to_datetime(df['start_date'], errors='coerce'),\n",
    "             end_date=lambda df: pd.to_datetime(df['end_date'], errors='coerce'))\n",
    "    # filter \n",
    "    .loc[lambda df: df['admin_name'].notna() & ~df['admin_name'].eq('[ND]') & ~df['status'].eq('C')]\n",
    ")\n",
    "################\n",
    "### NOTE THE INDUSTRIES OF EACH SIREN FOR THE PERIOD OF INTEREST \n",
    "################\n",
    "industry_year_dta = []\n",
    "for year in range(2008,2024):\n",
    "    industry_year_dta.append(\n",
    "        siren_numbers.loc[lambda c: c['start_date'].dt.year.le(year) & c['end_date'].dt.year.ge(year)]\n",
    "        .sort_values(by = ['siren','end_date'], ascending = [True,False])\n",
    "        .groupby('siren').head(1)\n",
    "       .assign(year = year)\n",
    "       [['siren','year', 'industry', 'industry_system']]\n",
    "    )\n",
    "pd.concat(industry_year_dta, ignore_index = True).to_parquet('data/2_processed/admin/siren_industry_year.parquet')\n",
    "\n",
    "################\n",
    "### RETRIEVE THE START AND END DATE OF THE SIREN / Name Combo (this method is orders of magnitudes faster than aggregating)\n",
    "################\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "siren_numbers['combo'] = siren_numbers['siren'] + siren_numbers['admin_name']\n",
    "start_dates = (siren_numbers.loc[lambda c: c['start_date'].notna()]\n",
    "               .sort_values(['combo','start_date'])\n",
    "               .groupby(['combo']).head(1)[['siren','admin_name','start_date']])\n",
    "\n",
    "na_end_dates = (siren_numbers.loc[lambda c: c['end_date'].isna()]\n",
    "                .drop_duplicates(subset = 'combo')[['siren','admin_name','combo','end_date']])\n",
    "\n",
    "end_dates = (siren_numbers.loc[lambda c: ~c['combo'].isin(na_end_dates['combo'])]\n",
    "            .sort_values(['combo','end_date'], ascending = [True, False])\n",
    "            .groupby('combo').head(1))\n",
    "siren_numbers = (pd.merge(start_dates, pd.concat([na_end_dates,end_dates])[['siren','admin_name','end_date']], how = 'outer')\n",
    "                .loc[lambda x: x['end_date'].dt.year.gt(2007) | x['end_date'].isna()])\n",
    "\n",
    "################\n",
    "## Generate Cleaned Names \n",
    "################\n",
    "siren_chunks = np.array_split(siren_numbers,chunks); \n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'clean') for index in range(chunks)), ignore_index = True)\n",
    "\n",
    "#establish / remove the list of common words \n",
    "word_counts = siren_numbers['admin_name_cleaned'].str.split(expand = True).stack().value_counts()\n",
    "common_words = set(word_counts[word_counts > np.max(word_counts) * cut_off].index)\n",
    "common_words = {word for word in common_words if not word.isnumeric()}\n",
    "with open('data/2_processed/admin/common_words.txt', 'w') as file: file.write('\\n'.join(common_words))\n",
    "\n",
    "\n",
    "### use the common words to clean the firms: \n",
    "siren_chunks = np.array_split(siren_numbers,chunks)\n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'strip') for index in range(chunks)), ignore_index = True)\n",
    "siren_numbers.to_parquet('data/2_processed/admin/siren_admin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7387a-0179-4189-b544-21faf45b449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATE EXISTING SIREN CODES IF WE MADE AN UPDATE TO CLEANING / STRIPPING PROCEDURE \n",
    "chunks =  os.cpu_count() - 10; cut_off = .01\n",
    "def wrapper(index, function):\n",
    "    if function == \"clean\":\n",
    "        return(hf.clean_firm_names(siren_chunks[index], \"admin_name\", False))\n",
    "    else:\n",
    "        return(hf.strip_words(siren_chunks[index], 'admin_name_cleaned', common_words))          \n",
    "siren_numbers = pd.read_parquet('data/2_processed/admin/siren_admin.parquet')   \n",
    "\n",
    "## Generate Cleaned Names \n",
    "siren_chunks = np.array_split(siren_numbers,chunks); \n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'clean') for index in range(chunks)), ignore_index = True)\n",
    "\n",
    "#establish / remove the list of common words \n",
    "names_vec = ['andre', 'bernard', 'claude', 'jacques', 'jean','louis', 'marie', 'martin', 'michel','paul', 'pierre']\n",
    "word_counts = siren_numbers['admin_name_cleaned'].str.split(expand = True).stack().value_counts()\n",
    "common_words = set(word_counts[word_counts > np.max(word_counts) * cut_off].index)\n",
    "common_words = {word for word in common_words if not word.isnumeric() and word not in names_vec}\n",
    "with open('data/2_processed/admin/common_words.txt', 'w') as file: file.write('\\n'.join(common_words))\n",
    "\n",
    "print('finished generating common words')\n",
    "### use the common words to clean the firms: \n",
    "siren_chunks = np.array_split(siren_numbers,chunks)\n",
    "siren_numbers = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index, 'strip') for index in range(chunks)), ignore_index = True)\n",
    "\n",
    "## remove firms that don't have anything after cleaning \n",
    "siren_numbers = siren_numbers.loc[~siren_numbers['admin_name_cleaned'].eq(\"\")]\n",
    "\n",
    "## export \n",
    "siren_numbers.to_parquet('data/2_processed/admin/siren_admin.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94d14c-46fb-4758-969f-c4b2c1b88ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate our role dictionary \n",
    "########################################################################################\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "roles = (db.raw_sql(\"select * from revelio.individual_role_lookup\")\n",
    "              .applymap(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "\n",
    "\n",
    "roles['role_id'] = roles.index\n",
    "roles['total'] = True\n",
    "roles['engineer'] = roles['job_category'] == 'engineer'\n",
    "roles['data'] = (\n",
    "    (roles['role_k50'] == 'data analyst') |\n",
    "    (roles['role_k150'].str.contains('data', na=False)) |\n",
    "    (roles['onet_title'].str.contains('database', na=False)) |\n",
    "    (roles['role_k1500'].str.contains('data center', na=False))\n",
    ")\n",
    "\n",
    "rnd = pd.read_excel(\"data/1_raw_data/admin/ONET_RandD_roles.xlsx\").assign(rnd=True)[['Code', 'rnd']]\n",
    "stem = pd.read_excel(\"data/1_raw_data/admin/ONET_stem_roles.xlsx\").assign(stem=True)[['Code', 'stem']]\n",
    "roles = (roles.merge(rnd, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "           .merge(stem, left_on =\"onet_code\", right_on=\"Code\", how=\"left\").drop('Code', axis = 1)\n",
    "            .assign(rnd=lambda x: x['rnd'].fillna(False))\n",
    "           .assign(stem=lambda x: x['stem'].fillna(False))\n",
    "         .applymap(lambda x: int(x) if isinstance(x, bool) else x))\n",
    "roles.to_csv('data/2_processed/linkedin/revelio_role_dict.csv', index = False)\n",
    "\n",
    "########################################################################################\n",
    "# Generate our list of french factset ids\n",
    "########################################################################################\n",
    "(db.raw_sql(\"select factset_entity_id \"\n",
    "        \"from factset.edm_standard_address \"\n",
    "         \"where iso_country = 'FR'\" ).\n",
    "      to_parquet('data/2_processed/admin/factset_french_domiciled.parquet'))\n",
    "db.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf348cf4-53ac-486c-b4ca-72b80c222636",
   "metadata": {},
   "source": [
    "Provides a crosswalk between LEI and SIREN codes. When INSEE is the managing Local Operating Unit (LOU) or the firm is french, it identifies firms with SIREN codes. The initial list of LEI codes is provided by the [GLEIF](https://search.gleif.org/#/search/simpleSearch=France&fulltextFilterId=LEIREC_FULLTEXT&currentPage=1&perPage=15&expertMode=false).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c024-00a0-41f4-9959-458e0289b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = 'data/1_raw_data/admin/20241105-0000-gleif-goldencopy-lei2-golden-copy.csv'\n",
    "\n",
    "# Columns of interest and their new names\n",
    "interest_cols = ['LEI','Entity.LegalName','Entity.LegalAddress.Country', 'Entity.RegistrationAuthority.RegistrationAuthorityEntityID','Registration.ManagingLOU']\n",
    "new_names = [\"lei\", \"lei_name\", 'lei_country', 'lei_siren',\"managing_lou\"]\n",
    "crosswalk = pd.read_csv(file_path, usecols=interest_cols, low_memory=False)\n",
    "crosswalk.columns = new_names\n",
    "\n",
    "crosswalk = (crosswalk.assign(insee_registered = lambda c: c['managing_lou'] == '969500Q2MA9VBQ8BG884',\n",
    "             lei_siren = lambda c: c['lei_siren'].fillna('').astype(str).apply(lambda x: re.sub(r'[^a-zA-Z0-9]', '',x)))\n",
    "             .loc[lambda x: (x['lei_country'].eq(\"FR\") | x['insee_registered'])])\n",
    "\n",
    "crosswalk.to_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74b554-cc46-4e85-8f26-26763786f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE COUNTRIES OF ALL USERS \n",
    "########################################################################################\n",
    "\n",
    "countries = pd.read_csv('data/2_processed/linkedin/revelio_country_dict.csv')['country']\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    user_data = db.raw_sql(\n",
    "        \"select user_id, prestige \"\n",
    "        \"from revelio.individual_user \"\n",
    "        \"where user_country = %(country)s \",\n",
    "        params={\"country\": countries.iloc[i]}\n",
    "    )\n",
    "    user_data['country_id'] = i\n",
    "    user_data.to_parquet(f\"data/2_processed/linkedin/user_components_by_country/users_{i}.parquet\")\n",
    "\n",
    "### find the french users \n",
    "france_index = countries[countries == \"France\"].index[0]\n",
    "individual_user = pd.read_parquet(f\"data/2_processed/linkedin/user_components_by_country/users_{france_index}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dfb9b-5d62-47f1-96f6-5c57702812cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES LOCATED IN FRANCE OR ASSIGNED TO A FRENCH PERSON \n",
    "########################################################################################\n",
    "all_roles = []\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 100000\n",
    "user_ids = tuple(individual_user['user_id'].unique())\n",
    "\n",
    "# Split user_ids into smaller batches and execute the query for each batch\n",
    "for i in range(0, len(user_ids), batch_size):\n",
    "    batch_user_ids = user_ids[i:i + batch_size]\n",
    "    params = {\"user_ids\": batch_user_ids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    roles_batch = db.raw_sql(\n",
    "        \"SELECT rcid, ultimate_parent_rcid \"\n",
    "        \"FROM revelio.individual_positions \"\n",
    "        \"WHERE user_id IN %(user_ids)s \"\n",
    "        ,\n",
    "        params=params,\n",
    "    )\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_roles.append(roles_batch)\n",
    "\n",
    "french_roles = db.raw_sql(\n",
    "        \"SELECT rcid, ultimate_parent_rcid \"\n",
    "        \"FROM revelio.individual_positions \"\n",
    "        \"WHERE country = 'France' \",\n",
    "        params=params,\n",
    "    )\n",
    "all_roles.append(french_roles)\n",
    "\n",
    "# Concatenate all batches into a single DataFrame\n",
    "roles = pd.concat(all_roles, ignore_index=True)\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# FIND ALL COMPANIES THAT HAVE HIRED AT LEAST ONE OF THESE ROLES OR HAD A RELATION DO SO\n",
    "########################################################################################\n",
    "\n",
    "rcids = tuple(roles['rcid'].unique());\n",
    "parent_rcids = tuple(map(str, roles['ultimate_parent_rcid'].unique()));\n",
    "\n",
    "all_companies = []\n",
    "## FIND ALL COMPANIES BASED ON THEIR RCID \n",
    "for i in range(0, len(rcids), batch_size):\n",
    "    batch_rcids = rcids[i:i + batch_size]\n",
    "    params = {\"rcids\": batch_rcids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    companies_batch = db.raw_sql(\"SELECT * \"\n",
    "                      \"FROM  revelio.company_mapping \"\n",
    "                       \"WHERE rcid IN %(rcids)s \",\n",
    "                     params = params)\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_companies.append(companies_batch)\n",
    "\n",
    "#### FIND ALL COMPANIES BASED ON THEIR PARENT RCID\n",
    "for i in range(0, len(parent_rcids), batch_size):\n",
    "    batch_rcids = rcids[i:i + batch_size]\n",
    "    params = {\"rcids\": batch_rcids}\n",
    "    \n",
    "    # Execute the query for the current batch\n",
    "    companies_batch = db.raw_sql(\"SELECT * \"\n",
    "                      \"FROM  revelio.company_mapping \"\n",
    "                       \"WHERE ultimate_parent_rcid IN %(rcids)s \",\n",
    "                     params = params)\n",
    "    \n",
    "    # Append the result of this batch to the list\n",
    "    all_companies.append(companies_batch)\n",
    "  \n",
    "companies = pd.concat(all_companies, ignore_index=True).drop_duplicates()\n",
    "companies['year_founded'] = pd.to_numeric(companies['year_founded'], errors='coerce')\n",
    "\n",
    "\n",
    "### CLEAN THE NAMES OF THOSE FIRMS \n",
    "with open('data/2_processed/admin/common_words.txt', 'r') as file:\n",
    "    common_words = set(file.read().splitlines())\n",
    "chunks =  os.cpu_count() - 10\n",
    "company_chunks = np.array_split(companies,chunks); \n",
    "def wrapper(index):\n",
    "    temp = hf.clean_firm_names(company_chunks[index], \"company\", True)\n",
    "    return(hf.strip_words(temp, 'company_cleaned', common_words))\n",
    "       \n",
    "companies = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index) for index in range(chunks)), ignore_index = True)\n",
    "companies = companies.loc[~companies['company_cleaned'].eq(\"\")]\n",
    "companies.to_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112fd8d-b0f1-4120-93f6-072a6d38a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IF WE NEED TO UPDATE THE CLEANING / STRIPPING PROCEDURE \n",
    "companies = pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')\n",
    "\n",
    "with open('data/2_processed/admin/common_words.txt', 'r') as file:\n",
    "    common_words = set(file.read().splitlines())\n",
    "chunks =  os.cpu_count() - 10\n",
    "company_chunks = np.array_split(companies,chunks); \n",
    "def wrapper(index):\n",
    "    temp = hf.clean_firm_names(company_chunks[index], \"company\", True)\n",
    "    return(hf.strip_words(temp, 'company_cleaned', common_words))\n",
    "       \n",
    "companies = pd.concat(Parallel(n_jobs=chunks, backend='multiprocessing')\n",
    "                          (delayed(wrapper)(index) for index in range(chunks)), ignore_index = True)\n",
    "companies.to_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a5ce2-9704-4f5c-a536-7c4de1890c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES TIED TO A COMPANY WITH SOME CONNECTION TO FRANCE\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "year_range = range(2008,2024)\n",
    "int_vars = ['french', 'total', 'engineer', 'data','rnd','stem']\n",
    "role_dict_vars = ['role_id'] + int_vars[1:]\n",
    "chunks = 1000\n",
    "cores =  os.cpu_count() - 10; \n",
    "export_path = 'data/2_processed/linkedin/temp_collapsed_roles'\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "\n",
    "role_to_id = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv').set_index('role_k1500')['role_id'].to_dict()\n",
    "country_dict = pd.read_csv('data/2_processed/linkedin/revelio_country_dict.csv')\n",
    "france_id = country_dict.loc[country_dict['country'] == 'France', 'country_id'].values[0]\n",
    "french_users = pd.read_parquet('data/2_processed/linkedin/user_components_by_country/users_'+ str(france_id)+\".parquet\")['user_id']\n",
    "role_dict = pd.read_csv('data/2_processed/linkedin/revelio_role_dict.csv')\n",
    "\n",
    "companies = pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet')[['rcid']].drop_duplicates()\n",
    "company_chunks = np.array_split(companies, chunks)\n",
    "\n",
    "for index in reversed(range(chunks)):\n",
    "    export_file = export_path + f'/chunk_{index}.parquet'\n",
    "    rcid_list = tuple(map(str, company_chunks[index]['rcid']))\n",
    "    if not os.path.isfile(export_file): \n",
    "        print(f'starting import {index}')\n",
    "        df = db.raw_sql(\n",
    "                \"SELECT user_id, position_id, country AS role_country, startdate, enddate, role_k1500, \"\n",
    "                \"weight, seniority, total_compensation, rcid \"\n",
    "                \"FROM revelio.individual_positions WHERE rcid IN %(rcids)s \", \n",
    "                params={\"rcids\": rcid_list})\n",
    "        \n",
    "        print(f'starting processing {index}')\n",
    "        df_rcid_chunks = np.array_split(df[['rcid']].drop_duplicates(), cores)\n",
    "        df_chunks = []\n",
    "        for chunk in range(cores):\n",
    "            df_chunks.append(df.loc[df['rcid'].isin(df_rcid_chunks[chunk]['rcid'])])\n",
    "\n",
    "        def collapse_and_clean(chunk):\n",
    "            chunk = (chunk\n",
    "                  .assign(role_id = chunk['role_k1500'].map(role_to_id),\n",
    "                          french = chunk['role_country'].eq('France') | chunk['user_id'].isin(french_users),\n",
    "                          startdate = chunk['startdate'].apply(pd.to_datetime),\n",
    "                          enddate = chunk['enddate'].apply(pd.to_datetime))\n",
    "                  .merge(role_dict[role_dict_vars], on='role_id', how='left'))\n",
    "\n",
    "            output_list = []\n",
    "            for year in year_range:\n",
    "                # Create a temporary DataFrame with 'valid' column indicating if 'startdate' <= year <= 'enddate' or 'enddate' is NA\n",
    "                temp = chunk.copy()\n",
    "                temp['valid'] = ((temp['startdate'].dt.year <= year) & \\\n",
    "                                 ((temp['enddate'].dt.year >= year) | temp['enddate'].isna())).astype(int)\n",
    "\n",
    "                # Pre compute interest columns\n",
    "                for col in int_vars:\n",
    "                    temp[f'emp_{col}'] = temp[col] * temp['valid'] * temp['weight']\n",
    "                    temp[f'comp_{col}'] = temp[col] * temp['valid'] * temp['weight']*temp['total_compensation'] / 1000\n",
    "\n",
    "                output = temp.groupby('rcid').agg({\n",
    "                    **{f'emp_{col}': 'sum' for col in int_vars},\n",
    "                    **{f'comp_{col}': 'sum' for col in int_vars}\n",
    "                }).reset_index()\n",
    "\n",
    "                for col in int_vars: \n",
    "                    output[f'emp_{col}'] = round(output[f'emp_{col}'])\n",
    "\n",
    "                output['share_emp_french'] = output['emp_french'] /output['emp_total'] \n",
    "                output['share_comp_french'] = output['comp_french'] / output['comp_total']\n",
    "                output['year'] = year\n",
    "                columns_to_round = ['comp_french', 'comp_total', 'comp_engineer', 'comp_data', 'comp_rnd', 'comp_stem']\n",
    "                output[columns_to_round] = output[columns_to_round].apply(lambda x: x.round(0))\n",
    "\n",
    "                output_list.append(output)\n",
    "            return(pd.concat(output_list).sort_values(by=['rcid']))\n",
    "        roles_yr_level = pd.concat(Parallel(n_jobs=cores, backend='multiprocessing')(delayed(collapse_and_clean)(chunk) for chunk in df_chunks),ignore_index = True)\n",
    "\n",
    "        roles_yr_level.to_parquet(export_file)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "output_list =[]\n",
    "file_list = sorted(glob.glob(export_path + '/*'))\n",
    "for file in file_list:\n",
    "    clear_output(wait=True)\n",
    "    print(file)\n",
    "    output_list.append(pd.read_parquet(file))\n",
    "    \n",
    "output_list = pd.concat(output_list)\n",
    "output_list.to_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "shutil.rmtree(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cabe5-0a68-4bc0-87d9-180adfadab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# USE ROLE DATA TO DETERMINE WHICH FIRMS ARE LIKELY FRENCH \n",
    "########################################################################################\n",
    "\n",
    "french_leis = pd.read_parquet('data/2_processed/admin/LEI_siren_crosswalk.parquet')['lei'].unique()\n",
    "non_french_country_domains = (pd.read_excel('data/2_processed/admin/domain_names_by_country.xlsx')\n",
    "                              .assign(name=lambda x: x['name'].str.replace('.', '', regex=False))\n",
    "                              .query('include != include')['name'])\n",
    "french_factset_ids = pd.read_parquet('data/2_processed/admin/factset_french_domiciled.parquet')['factset_entity_id'].unique()\n",
    "roles_data = pd.read_parquet('data/2_processed/linkedin/french_affiliated_firm_roles_collapsed_raw.parquet')\n",
    "\n",
    "companies = (\n",
    "    ## determine the firm's max total / french / data values and shares \n",
    "    roles_data\n",
    "    .assign(cost_per_worker = lambda df: df['comp_total'] / df['emp_total'])\n",
    "    .groupby('rcid', as_index=False)\n",
    "    .agg({'cost_per_worker':'max','emp_total': 'max','emp_french': 'max','emp_data': 'max',\n",
    "          'comp_total': 'max','comp_data': 'max','share_emp_french': 'max', 'share_comp_french': 'max'})\n",
    "    .assign(french_eligible = lambda c: c['emp_french'].gt(0))\n",
    "    [['rcid', 'french_eligible','cost_per_worker', 'share_emp_french', 'share_comp_french','emp_total', 'emp_data' ,'comp_total', 'comp_data']].\n",
    "    merge(pd.read_parquet('data/1_raw_data/linkedin/revelio/france_affiliated_firms.parquet'))\n",
    "    \n",
    "    ### determine whether the firm likely french or not \n",
    "    .assign(url_ending = lambda c: c['url'].apply(lambda x: x.split('.')[-1] if isinstance(x, str) else None))\n",
    "    .assign(\n",
    "        admin_score=lambda c: 0\n",
    "        ## TOP LEVEL DOMAIN \n",
    "        + c['url_ending'].eq('fr')  \n",
    "        - c['url_ending'].isin(non_french_country_domains)  \n",
    "\n",
    "        ## FACTSET\n",
    "        + c['factset_entity_id'].isin(french_factset_ids)\n",
    "        - (~c['factset_entity_id'].isin(french_factset_ids) & c['factset_entity_id'].notna())\n",
    "\n",
    "        # LEI CHECK \n",
    "        +  c['lei'].isin(french_leis) \n",
    "        -  (~c['lei'].isin(french_leis) & c['lei'].notna()) \n",
    "\n",
    "        # ISIN \n",
    "        + c['isin'].str[:2].eq(\"FR\") # add if french isin\n",
    "        - (~c['isin'].str[:2].eq(\"FR\") & c['isin'].notna())\n",
    "\n",
    "        # CUSIP \n",
    "        + c['cusip'].str[:1].eq(\"F\") # add if french cusip\n",
    "        - (~c['cusip'].str[:1].eq(\"F\") & c['cusip'].notna())\n",
    "\n",
    "        # Firm type \n",
    "        + c['firm_type_french_likelihood'].eq(\"likely french\") \n",
    "        - c['firm_type_french_likelihood'].eq(\"unlikely french\"))\n",
    "     .assign(\n",
    "         likely_french = lambda c: \n",
    "         c['french_eligible'] & (\n",
    "         c['admin_score'].gt(0) | \n",
    "         (c['admin_score'].eq(0) & (c['share_emp_french'].gt(.5) | c['share_comp_french'].gt(.5)))))\n",
    "    \n",
    "    #### mark whether it's a subsidiary \n",
    "    .assign(subsidiary = lambda c: c['rcid'] != c['ultimate_parent_rcid'])\n",
    "    [['rcid','lei','company','company_cleaned', 'company_stripped', 'year_founded', 'ultimate_parent_rcid',\n",
    "      'likely_french', 'subsidiary','share_emp_french', 'share_comp_french',\n",
    "      'emp_total', 'emp_data' ,'comp_total', 'comp_data','cost_per_worker']]\n",
    ")\n",
    "companies.to_parquet('data/2_processed/linkedin/france_affiliated_firms_cleaned.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
