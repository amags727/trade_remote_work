{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210b24cb-dc94-471c-8403-689a3cd11446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output', 'random',\n",
    "          'unicodedata','sys', 'from langdetect import detect, DetectorFactory']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "import A_helper_functions as hf\n",
    "processed_linkedin = '1) data/15_revelio_data/1_inputs/b_processed_data/linkedin/'\n",
    "processed_admin = '1) data/15_revelio_data/1_inputs/b_processed_data/admin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61993c9-7af5-4e2b-a461-282a3eea142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/2mc_0qf15f94x6b5pj5t5fs00000gp/T/ipykernel_68068/2176553411.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "non_french_admin = (\n",
    "    pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "    .loc[lambda x: ~x['admin_french'] &\n",
    "    ~x['rcid'].isin(pd.read_parquet(processed_linkedin +'non_french_admin_pt1.parquet')['rcid'])\n",
    "    ])\n",
    "\n",
    "num_chunks = 500\n",
    "temp_direct = os.path.join(processed_linkedin, 'temp')\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(non_french_admin['rcid'].dropna().unique(), num_chunks)\n",
    "french_users = pd.read_parquet( processed_linkedin + 'all_french_users.parquet')\n",
    "\n",
    "for index in reversed(range(num_chunks)):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,country, weight, total_compensation\n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        temp.loc[temp['user_id'].isin(french_users['user_id']),'country' ] = 'France'\n",
    "        temp = (\n",
    "            temp.loc[~temp['country'].isna()]\n",
    "            .assign(french = lambda x: x['country'].eq('France'),\n",
    "                    comp = lambda x: x['weight'] * x['total_compensation'])\n",
    "            .groupby('rcid', as_index=False)\n",
    "            .apply(lambda g: pd.Series({\n",
    "                  'share_comp_french': g.loc[g['french'], 'comp'].sum() / g['comp'].sum(),\n",
    "                  'share_emp_french': g.loc[g['french'], 'weight'].sum() / g['weight'].sum()})).reset_index()\n",
    "            .assign(role_french = lambda x: x['share_comp_french'].ge(.5) | x['share_emp_french'].ge(.5))\n",
    "            )[['rcid', 'role_french']].to_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601757b1-033c-4b16-8419-346e36f33e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Matching \n",
    "############  \n",
    "### DEFINE PARAMETERS AND IMPORT DATA / Matching Function\n",
    "############  \n",
    "init_matches = 50; final_matches = 5; cores = max(os.cpu_count() - 5, 1);\n",
    "chunks = cores*10\n",
    "\n",
    "def matching_wrapper(index):\n",
    "    #output progress \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{round(index/chunks*100, 2)}%\")\n",
    "    \n",
    "    temp_firms = firm_chunks[index]\n",
    "    #run first version of matcher on all words \n",
    "    matches = matcher.match_names(to_be_matched=temp_firms, column_matching='company_cleaned')\n",
    "    results = (pd.wide_to_long(matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "               .reset_index()[['original_name', 'match_name', 'score']]\n",
    "               .rename(columns={'original_name': 'company_cleaned', 'match_name': 'admin_name_cleaned', 'score': 'raw_score'})\n",
    "               .merge(temp_firms[['company_cleaned', 'company_stripped']], how = 'left', on = 'company_cleaned')\n",
    "               .merge(sirens_to_match[['admin_name_cleaned', 'admin_name_stripped']], how = 'left', on = 'admin_name_cleaned'))\n",
    "    company_chunks = [group for _, group in results.groupby('company_cleaned')]\n",
    "\n",
    "    ### reun the second version of matcher only on words from initial list \n",
    "    results = []\n",
    "    temp_matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "    temp_matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "    for chunk in company_chunks:\n",
    "        chunk = chunk.reset_index()\n",
    "        try:\n",
    "            temp_matcher.load_and_process_master_data(column='admin_name_stripped', df_matching_data=chunk, transform=True)\n",
    "            temp_matches = temp_matcher.match_names(to_be_matched=chunk.iloc[0], column_matching='company_stripped')\n",
    "            temp_results = (pd.wide_to_long(temp_matches,stubnames=[\"match_name\", \"score\", \"match_index\"], i=\"original_name\", j=\"match\",suffix=\"_\\d+\")\n",
    "                            .reset_index()[['original_name', 'match_name', 'score']]\n",
    "                            .rename(columns={'original_name': 'company_stripped', 'match_name': 'admin_name_stripped', 'score': 'stripped_score'})\n",
    "                            .drop_duplicates()\n",
    "                            .merge(chunk, how = 'right')\n",
    "                            .assign(match_index = lambda df: df.groupby(['stripped_score']).ngroup()))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing company: {chunk.loc[0,'company_cleaned']} in index {index}\") \n",
    "            temp_results = chunk.assign(match_index = lambda df: df.groupby(['raw_score']).ngroup())\n",
    "\n",
    "        temp_results = (temp_results\n",
    "                        .assign(match_index = lambda df: df['match_index'].max() - df['match_index'] + 1)\n",
    "                        .sort_values(['match_index','raw_score'], ascending = [True, False])\n",
    "                        .loc[lambda df: df['match_index'].le(final_matches)])\n",
    "        results.append(temp_results)\n",
    "    results = pd.concat(results, ignore_index = True)[['company_cleaned', 'admin_name_cleaned', 'raw_score', 'stripped_score', 'match_index']]\n",
    "    return(results)\n",
    "\n",
    "############  \n",
    "### Check for LEI matches \n",
    "############ \n",
    "french_leis = pd.read_parquet(processed_admin +'LEI_siren_crosswalk.parquet')[['lei', 'lei_siren']].rename(columns={'lei_siren': 'firmid'})\n",
    "firms_to_match = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet'))\n",
    "lei_matched = (pd.merge(french_leis, firms_to_match[['rcid','lei']])\n",
    "              .assign(match_method = 'lei')\n",
    "              [['rcid', 'firmid', 'match_method']])\n",
    "\n",
    "############  \n",
    "### Check for exact matches on cleaned names \n",
    "############ \n",
    "firms_to_match = (firms_to_match.loc[lambda x: x['french_eligible'] & ~x['rcid'].isin(lei_matched['rcid'])]\n",
    "                  .assign(count=lambda x: x.groupby('company_cleaned')['company_cleaned'].transform('count'))\n",
    "                  .loc[lambda x: x['count'] == 1]\n",
    "                 [['rcid','company_cleaned','company_stripped']])     \n",
    "\n",
    "sirens_to_match = (pd.read_parquet(processed_admin + 'siren_admin.parquet')\n",
    "                   .rename(columns = {'siren': 'firmid'})\n",
    "                   .loc[lambda x: ~x['firmid'].isin(lei_matched['firmid'])]\n",
    "                   .assign(count=lambda x: x.groupby('admin_name_cleaned')['admin_name_cleaned'].transform('count'))\n",
    "                   .loc[lambda x: x['count'] == 1]\n",
    "                   [['admin_name_cleaned', 'admin_name_stripped','firmid']])\n",
    "clean_matched = (\n",
    "    pd.merge(firms_to_match, sirens_to_match, how = 'inner',\n",
    "             left_on = 'company_cleaned', right_on = 'admin_name_cleaned')\n",
    "    .assign(match_method = 'cleaned')\n",
    "    [['rcid','firmid', 'match_method']])\n",
    "\n",
    "\n",
    "############  \n",
    "### match remaining firms \n",
    "############ \n",
    "sirens_to_match = (sirens_to_match.loc[lambda x: ~x['firmid'].isin(clean_matched['firmid'])] \n",
    "                   .assign(count=lambda x: x.groupby('admin_name_stripped')['admin_name_stripped'].transform('count')))\n",
    "\n",
    "firms_to_match = (firms_to_match\n",
    "                  .loc[lambda c: ~c['rcid'].isin(clean_matched['rcid']) \n",
    "                  & c['company_stripped'].isin(sirens_to_match.loc[lambda x: x['count'].eq(1), 'admin_name_stripped'])])\n",
    "\n",
    "firm_chunks = np.array_split(firms_to_match[['company_cleaned', 'company_stripped']], chunks)\n",
    "matcher = NameMatcher(number_of_matches=init_matches, legal_suffixes=False, common_words= False, top_n= init_matches, verbose=False)\n",
    "matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "matcher.load_and_process_master_data(column='admin_name_cleaned',\n",
    "                                     df_matching_data =sirens_to_match[['admin_name_cleaned', 'admin_name_stripped']],\n",
    "                                     transform=True)\n",
    "\n",
    "matching_output = Parallel(n_jobs=cores, backend='multiprocessing')(delayed(matching_wrapper)(index) for index in range(chunks))\n",
    "strip_matched = (pd.concat(matching_output, ignore_index = True)\n",
    "                 .loc[lambda x: x['match_index'].eq(1) & x['stripped_score'].eq(100)]\n",
    "                 .assign(match_method = 'strip',\n",
    "                         count=lambda x: x.groupby('company_cleaned')['company_cleaned'].transform('count'))\n",
    "                 .loc[lambda x: x['count'] == 1]\n",
    "                 .merge(sirens_to_match[['admin_name_cleaned', 'firmid']])\n",
    "                 .merge(firms_to_match[['company_cleaned', 'rcid']])\n",
    "                 [['rcid','firmid', 'match_method']])\n",
    "\n",
    "all_matches = pd.concat([lei_matched, clean_matched, strip_matched], ignore_index = True)\n",
    "final_output = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "              .merge(all_matches, how = 'left', on = 'rcid'))\n",
    "final_output.to_parquet(processed_linkedin + 'firm_lvl_info_all_potential_french_firms.parquet')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
