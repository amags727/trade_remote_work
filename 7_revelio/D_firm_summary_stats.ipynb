{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d945b2e3-a4d3-4155-961d-3c3ab276c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata','sys', 'matplotlib.pyplot as plt', 'glob', 'shutil','from sklearn.decomposition import PCA',\n",
    "          'from geopy  import Nominatim', 'time', 'geopandas as gpd', 'requests', 'from io import BytesIO']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "\n",
    "\n",
    "cluster = 'Google' not in os.getcwd()\n",
    "if ~cluster:\n",
    "    raw_admin = '1) data/15_revelio_data/1_inputs/a_raw_data/admin/'\n",
    "    processed_linkedin = '1) data/15_revelio_data/1_inputs/b_processed_data/linkedin/'\n",
    "    processed_admin = '1) data/15_revelio_data/1_inputs/b_processed_data/admin/'\n",
    "if cluster:\n",
    "    raw_admin = 'data/1_raw_data/admin/'\n",
    "    processed_linkedin = 'data/2_processed/linkedin/'\n",
    "    processed_admin = 'data/2_processed/admin/'\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fcf3e-e20e-43d6-822f-4620760b449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD PARENT HQ LOCATION \n",
    "subsids =pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet').loc[lambda x: x['is_subsid']]\n",
    "num_chunks = 50\n",
    "temp_direct = processed_linkedin + 'temp_5'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(subsids['ultimate_parent_rcid'].unique(), num_chunks)\n",
    "\n",
    "# SCRAPE / PROCESS DATA FOR EACH CHUNK \n",
    "for index in range(num_chunks):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "        rcid_list = tuple(chunks[index].tolist())\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid, hq_country\n",
    "            FROM revelio.company_mapping\n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\",\n",
    "            params={\"rcid_list\": rcid_list}\n",
    "        )\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={'rcid': 'ultimate_parent_rcid', 'hq_country' :  'ultimate_parent_hq_country'}))\n",
    "\n",
    "output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet').merge(output, how = 'left')\n",
    "output.to_parquet('firm_lvl_info_all_matched_firms.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935cd0b5-ff91-463f-88c9-67dbee79d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "## FIND ALL ROLES ASSOCIATED WITH OUR MATCHED COMPANIES\n",
    "remaining_to_find = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet'))\n",
    "\n",
    "num_chunks = 500\n",
    "temp_direct = processed_linkedin + 'temp'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(remaining_to_find['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,position_id, weight, total_compensation,position_number, startdate, enddate, role_k1500,country, state, metro_area, city, seniority \n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        for col in ['startdate', 'enddate']:\n",
    "            temp[col] = pd.to_datetime(temp[col], errors='coerce')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "output.to_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "893720f0-9366-4387-8469-61ab8801a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Match Cities from our data to NUTS 3 regions \n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "## import and clean admin data \n",
    "french_cities = (pd.read_csv(raw_admin + 'insee_french_cities.csv',\n",
    "                          usecols = ['city_code', 'latitude', 'longitude'])\n",
    "                 .rename(columns={'city_code':'city'})\n",
    "                 .loc[lambda x: ~x['latitude'].isna()])\n",
    "french_cities = hf.clean_firm_names(french_cities, \"city\", False).drop_duplicates(subset = 'city_cleaned')\n",
    "\n",
    "## import and clean linkedin data \n",
    "role_cities = (pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet',\n",
    "                               columns=[\"city\", \"country\"])\n",
    "               .loc[lambda x: x['country'].eq('France') & ~x['city'].isna()].drop_duplicates())\n",
    "\n",
    "firm_cities = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet',\n",
    "                               columns=['hq_city', 'hq_country'])\n",
    "               .loc[lambda x: x['hq_country'].eq('France') & ~x['hq_city'].isna()]\n",
    "               .drop_duplicates())\n",
    "uni_cities = (pd.read_parquet(processed_linkedin +'data_grads_across_france.parquet')\n",
    "              [['university_city']].drop_duplicates)\n",
    "linkedin_cities = pd.Series(pd.concat([firm_cities['hq_city'],role_cities['city'],\n",
    "                                      uni_cities['university_city']]).unique())\n",
    "linkedin_cities = hf.clean_firm_names(pd.DataFrame({'city': linkedin_cities}), 'city', False)\n",
    "\n",
    "## match based on cleaned names \n",
    "linkedin_cities = pd.merge(linkedin_cities, french_cities.drop('city', axis = 1), how = 'left')\n",
    "\n",
    "\n",
    "## use openstreetmap to match \n",
    "geolocator = Nominatim(user_agent=\"your_app_name_here\")\n",
    "for i in range(len(linkedin_cities)):\n",
    "    if pd.isna(linkedin_cities.loc[i, 'latitude']):\n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(i+1)/len(linkedin_cities),2))+ '%')\n",
    "        print(i)\n",
    "        city_string = re.sub(r\"Arrondissement d(?:e|u|es|')\\s*\", \"\", linkedin_cities['city'][i])\n",
    "        location = geolocator.geocode(city_string + \", France\", timeout = 10)\n",
    "        if location:\n",
    "            linkedin_cities.loc[i, ['latitude', 'longitude']] = [\n",
    "                location.latitude, location.longitude]\n",
    "        time.sleep(1)\n",
    "\n",
    "## finally use the matches generated by chatgpt for the final set of matches \n",
    "chat_matched_cities =  (pd.merge(linkedin_cities.loc[linkedin_cities['latitude'].isna()][['city', 'city_cleaned']],\n",
    "                    pd.read_csv(processed_admin + 'chat_gpt_matches_for_final_cities.csv',  encoding='latin1')\n",
    "                    , on = 'city_cleaned', how = 'outer'))\n",
    "linkedin_cities = pd.concat([linkedin_cities.loc[~linkedin_cities['latitude'].isna()],chat_matched_cities],\n",
    "                            ignore_index = True)\n",
    "\n",
    "## match to NUTS 3 regions \n",
    "gisco_nuts = (gpd.read_file(BytesIO(\n",
    "    requests.get(\"https://gisco-services.ec.europa.eu/distribution/v2/nuts/geojson/NUTS_RG_60M_2021_4326_LEVL_3.geojson\")\n",
    "    .content)).loc[lambda x: x['CNTR_CODE'] == 'FR'].to_crs(\"EPSG:4326\")[['NUTS_ID', 'NUTS_NAME', 'geometry']])\n",
    "\n",
    "linkedin_cities = (linkedin_cities\n",
    "    .drop(['department_name', 'department_number'],axis = 1)\n",
    "    .dropna(subset=[\"latitude\", \"longitude\"]))\n",
    "\n",
    "linkedin_cities = (\n",
    "    gpd.GeoDataFrame(linkedin_cities,geometry=gpd.points_from_xy(\n",
    "        linkedin_cities[\"longitude\"], linkedin_cities[\"latitude\"]),crs=\"EPSG:4326\")\n",
    "    .sjoin(gisco_nuts.to_crs(\"EPSG:4326\"), \n",
    "          predicate=\"intersects\")\n",
    "    .loc[lambda x: ~x[\"NUTS_ID\"].str.startswith(\"FRY\")]\n",
    "    .reset_index()[['city', 'NUTS_ID', 'NUTS_NAME']])\n",
    "\n",
    "## export the file \n",
    "linkedin_cities.to_parquet(processed_linkedin + 'linkedin_city_coords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0180f2-e66d-4dfc-9066-4323b0e5aec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/2_processed/linkedin/temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a6e8f94e3642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtemp_direct\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mprocessed_linkedin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'temp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_direct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_direct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrole_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_linkedin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'matched_firm_role_output.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/helmod/apps/centos7/Core/Anaconda3/2021.05-jupyterood-fasrc01/x/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/helmod/apps/centos7/Core/Anaconda3/2021.05-jupyterood-fasrc01/x/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/2_processed/linkedin/temp'"
     ]
    }
   ],
   "source": [
    "## FIND THE PRESTIGE / EDUCATION OF ALL EMPLOYEES ASSOCIATED WITH OUR MATCHED COMPANIES \n",
    "num_chunks = 500\n",
    "temp_direct =  processed_linkedin + 'temp'                              \n",
    "shutil.rmtree(temp_direct)\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "\n",
    "existing_prestige = pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet') ## REMOVE\n",
    "role_output = role_output.loc[~role_output['user_id'].isin(existing_prestige['user_id'])] ## REMOVE \n",
    "                              \n",
    "chunks = np.array_split(role_output['user_id'].unique(), num_chunks)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path): \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            select user_id, prestige, highest_degree \n",
    "            from revelio.individual_user \n",
    "            where user_id IN %(user_ids)s\n",
    "            \"\"\",\n",
    "            params= {'user_ids': tuple(chunks[index].tolist())})\n",
    "        temp.to_parquet(file_path)\n",
    "    \n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True) \n",
    "output = pd.concat([output, existing_prestige], ignore_index = True) # REMOVE \n",
    "output.to_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet')                              \n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa01ffc-517c-48bb-b3c9-df94fc2b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform PCA to generate metrics of differentiation \n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, weight_var, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=(weight_var, 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'{weight_var}_PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')[['rcid', 'firmid']]\n",
    "output = pd.merge(long_data, matching_output)\n",
    "\n",
    "# run pca analysis\n",
    "for weight_var in ['wgted_comp','weight']:\n",
    "    print('starting pca gen')\n",
    "    pca_model = collapse_year_level(sample_year, True, weight_var)\n",
    "    print('finished pca gen')\n",
    "    [collapse_year_level(year,False,weight_var,pca_model) for year in range(2008, 2024)]\n",
    "    (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "     .to_parquet(processed_linkedin + 'matched_firm_pca_'+ weight_var + '_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a87a38-9560-47bf-bd94-7368ae9abac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE AMOUNT OF WORKERS CURRENTLY WORKING OR WITH EXPERIENCE ABROAD PER COMPANY \n",
    "########################################################################################\n",
    "\n",
    "### SET PARAMETERS\n",
    "num_chunks = 10000\n",
    "temp_direct = processed_linkedin + 'temp_4'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "matching_output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')[['rcid', 'firmid']]\n",
    "\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet').merge(matching_output)\n",
    "chunks = np.array_split(role_output['firmid'].unique(), num_chunks)\n",
    "linkedin_to_iso_cross_walk = (pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "\n",
    "### DEFINE FUNCTIONS \n",
    "def run_subsection(index):\n",
    "    clear_output(wait=True)\n",
    "    print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "    \n",
    "    role_subset = role_output.loc[lambda x: x['firmid'].isin(chunks[index])]\n",
    "    params = {\"user_ids\":  tuple(role_subset['user_id'].unique())}\n",
    "    ever_role_subset = db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id, country, startdate, enddate\n",
    "        FROM revelio.individual_positions \n",
    "        where user_id IN %(user_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "    )\n",
    "    print('done scraping')\n",
    "    subset_output = pd.concat([collapse_year_level(year, role_subset, ever_role_subset) for year in range(2009, 2021)],\n",
    "                            ignore_index=True)\n",
    "    subset_output.to_parquet(temp_direct + \"/temp\"+str(index)+\".parquet\")\n",
    "\n",
    "    \n",
    "def collapse_year_level(year, role_subset, ever_role_subset):\n",
    "    print(year)\n",
    "    cutoff_date = pd.Timestamp(f'{year}-01-01')\n",
    "    temp = (\n",
    "        ## determine which users are active in a given year for a given firm\n",
    "        role_subset\n",
    "        .rename(columns={'country':'current_ctry'})\n",
    "        .assign(\n",
    "                startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign( valid = lambda x: (x['startdate'] <= cutoff_date) & ((x['enddate'].isna()) | (x['enddate'] >= cutoff_date)))\n",
    "        .loc[lambda x: x['valid'], ['firmid','user_id', 'current_ctry', 'weight', 'total_compensation']]\n",
    "        .assign(priority = lambda x: (x['current_ctry'] == 'France').astype(int))\n",
    "        .sort_values(by = 'priority', ascending = False)\n",
    "        .drop_duplicates(subset = ['firmid', 'user_id'], keep = 'first')\n",
    "    \n",
    "        ## add all the roles those users have ever held \n",
    "         .merge(ever_role_subset).drop_duplicates()\n",
    "    \n",
    "        # drop all roles that start after the year of interest\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce'),\n",
    "               years_since_active = lambda x: (year - x['enddate'].dt.year.fillna(year)).clip(lower=0))\n",
    "         .loc[lambda x: (x['startdate'] <= cutoff_date) & ~x['country'].eq('France')]\n",
    "    \n",
    "        # compute the employee's effective tenure in the foreign market \n",
    "        .assign(effective_end_date = lambda x: x['enddate'].where(x['enddate'].notna() \n",
    "                                                                  & x['enddate'].lt(cutoff_date), cutoff_date))\n",
    "        .assign(duration = lambda x: (x['effective_end_date'] - x['startdate']).dt.days/ 365.25)\n",
    "    \n",
    "        # collapse down (note this will double count tenure if they held multiple roles \n",
    "        .groupby(['firmid', 'user_id', 'current_ctry', 'country'], as_index=False)\n",
    "        .agg({ 'duration': 'sum','years_since_active': 'min', 'total_compensation' : 'min', 'weight' : 'min'})\n",
    "        .rename(columns={'country':'ctry'})\n",
    "        .assign(year = year) \n",
    "    )\n",
    "    return(temp)\n",
    "\n",
    "### EXECUTE SCRAPING AND INITIAL COLLAPSE TO YEAR-firmid-ctry LEVEL \n",
    "[run_subsection(index) for index in range(num_chunks)]\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "\n",
    "### Match to ISO-2 CODES AND EXPORT \n",
    "output.merge(linkedin_to_iso_cross_walk, how = 'left').to_parquet(processed_linkedin + 'matched_firm_foreign_emp_history.parquet')\n",
    "\n",
    "shutil.rmtree(temp_direct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
