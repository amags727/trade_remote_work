{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692d19b4-0240-4151-a6ae-ea78a2b4dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata','sys', 'matplotlib.pyplot as plt', 'glob', 'shutil','from sklearn.decomposition import PCA']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "\n",
    "\n",
    "cluster = 'Google' not in os.getcwd()\n",
    "if ~cluster:\n",
    "    raw_admin = '1) data/15_revelio_data/1_inputs/a_raw_data/admin/'\n",
    "    processed_linkedin = '1) data/15_revelio_data/1_inputs/b_processed_data/linkedin/'\n",
    "    processed_admin = '1) data/15_revelio_data/1_inputs/b_processed_data/admin/'\n",
    "if cluster:\n",
    "    raw_admin = 'data/1_raw_data/admin/'\n",
    "    processed_linkedin = 'data/2_processed/linkedin/'\n",
    "    processed_admin = 'data/2_processed/admin/'\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4fcf3e-e20e-43d6-822f-4620760b449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD PARENT HQ LOCATION \n",
    "subsids =pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet').loc[lambda x: x['is_subsid']]\n",
    "num_chunks = 50\n",
    "temp_direct = processed_linkedin + 'temp_5'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(subsids['ultimate_parent_rcid'].unique(), num_chunks)\n",
    "\n",
    "# SCRAPE / PROCESS DATA FOR EACH CHUNK \n",
    "for index in range(num_chunks):\n",
    "    file_path = os.path.join(temp_direct, f\"temp{index}.parquet\")\n",
    "    if not os.path.exists(file_path):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{round(100 * (index + 1) / num_chunks, 2)}%\")\n",
    "        rcid_list = tuple(chunks[index].tolist())\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid, hq_country\n",
    "            FROM revelio.company_mapping\n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\",\n",
    "            params={\"rcid_list\": rcid_list}\n",
    "        )\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={'rcid': 'ultimate_parent_rcid', 'hq_country' :  'ultimate_parent_hq_country'}))\n",
    "\n",
    "output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet').merge(output, how = 'left')\n",
    "output.to_parquet('firm_lvl_info_all_matched_firms.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810d7bc5-ad09-4ea5-ba61-2a00ccbeb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIND ALL ROLES ASSOCIATED WITH OUR MATCHED COMPANIES\n",
    "remaining_to_find = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet'))\n",
    "\n",
    "num_chunks = 500\n",
    "temp_direct = processed_linkedin + 'temp'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(remaining_to_find['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id, weight, total_compensation, startdate, enddate, role_k1500,country, state, metro_area, seniority \n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        for col in ['startdate', 'enddate']:\n",
    "            temp[col] = pd.to_datetime(temp[col], errors='coerce')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "output.to_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c24747-6ba4-487a-bd52-6e90afe6eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.6%\n"
     ]
    }
   ],
   "source": [
    "## FIND THE PRESTIGE / EDUCATION OF ALL EMPLOYEES ASSOCIATED WITH OUR MATCHED COMPANIES \n",
    "num_chunks = 500\n",
    "temp_direct =  processed_linkedin + 'temp'                              \n",
    "shutil.rmtree(temp_direct)\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "\n",
    "existing_prestige = pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet') ## REMOVE\n",
    "role_output = role_output.loc[~role_output['user_id'].isin(existing_prestige['user_id'])] ## REMOVE \n",
    "                              \n",
    "chunks = np.array_split(role_output['user_id'].unique(), num_chunks)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path): \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            select user_id, prestige, highest_degree \n",
    "            from revelio.individual_user \n",
    "            where user_id IN %(user_ids)s\n",
    "            \"\"\",\n",
    "            params= {'user_ids': tuple(chunks[index].tolist())})\n",
    "        temp.to_parquet(file_path)\n",
    "    \n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True) \n",
    "output = pd.concat([output, existing_prestige], ignore_index = True) # REMOVE \n",
    "output.to_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet')                              \n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa01ffc-517c-48bb-b3c9-df94fc2b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform PCA to generate metrics of differentiation \n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, weight_var, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=(weight_var, 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'{weight_var}_PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')[['rcid', 'firmid']]\n",
    "output = pd.merge(long_data, matching_output)\n",
    "\n",
    "# run pca analysis\n",
    "for weight_var in ['wgted_comp','weight']:\n",
    "    print('starting pca gen')\n",
    "    pca_model = collapse_year_level(sample_year, True, weight_var)\n",
    "    print('finished pca gen')\n",
    "    [collapse_year_level(year,False,weight_var,pca_model) for year in range(2008, 2024)]\n",
    "    (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "     .to_parquet(processed_linkedin + 'matched_firm_pca_'+ weight_var + '_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09a5d5-bd0c-48b9-84b9-e4f54c8411e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE AMOUNT OF WORKERS CURRENTLY WORKING OR WITH EXPERIENCE ABROAD PER COMPANY \n",
    "########################################################################################\n",
    "\n",
    "### SET PARAMETERS\n",
    "num_chunks = 10000\n",
    "temp_direct = processed_linkedin + 'temp_4'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "matching_output = pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')[['rcid', 'firmid']]\n",
    "\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet').merge(matching_output)\n",
    "chunks = np.array_split(role_output['firmid'].unique(), num_chunks)\n",
    "linkedin_to_iso_cross_walk = (pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "\n",
    "### DEFINE FUNCTIONS \n",
    "def run_subsection(index):\n",
    "    clear_output(wait=True)\n",
    "    print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "    \n",
    "    role_subset = role_output.loc[lambda x: x['firmid'].isin(chunks[index])]\n",
    "    params = {\"user_ids\":  tuple(role_subset['user_id'].unique())}\n",
    "    ever_role_subset = db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id, country, startdate, enddate\n",
    "        FROM revelio.individual_positions \n",
    "        where user_id IN %(user_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "    )\n",
    "    print('done scraping')\n",
    "    subset_output = pd.concat([collapse_year_level(year, role_subset, ever_role_subset) for year in range(2009, 2021)],\n",
    "                            ignore_index=True)\n",
    "    subset_output.to_parquet(temp_direct + \"/temp\"+str(index)+\".parquet\")\n",
    "\n",
    "    \n",
    "def collapse_year_level(year, role_subset, ever_role_subset):\n",
    "    print(year)\n",
    "    cutoff_date = pd.Timestamp(f'{year}-01-01')\n",
    "    temp = (\n",
    "        ## determine which users are active in a given year for a given firm\n",
    "        role_subset\n",
    "        .rename(columns={'country':'current_ctry'})\n",
    "        .assign(\n",
    "                startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign( valid = lambda x: (x['startdate'] <= cutoff_date) & ((x['enddate'].isna()) | (x['enddate'] >= cutoff_date)))\n",
    "        .loc[lambda x: x['valid'], ['firmid','user_id', 'current_ctry', 'weight', 'total_compensation']]\n",
    "        .assign(priority = lambda x: (x['current_ctry'] == 'France').astype(int))\n",
    "        .sort_values(by = 'priority', ascending = False)\n",
    "        .drop_duplicates(subset = ['firmid', 'user_id'], keep = 'first')\n",
    "    \n",
    "        ## add all the roles those users have ever held \n",
    "         .merge(ever_role_subset).drop_duplicates()\n",
    "    \n",
    "        # drop all roles that start after the year of interest\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce'),\n",
    "               years_since_active = lambda x: (year - x['enddate'].dt.year.fillna(year)).clip(lower=0))\n",
    "         .loc[lambda x: (x['startdate'] <= cutoff_date) & ~x['country'].eq('France')]\n",
    "    \n",
    "        # compute the employee's effective tenure in the foreign market \n",
    "        .assign(effective_end_date = lambda x: x['enddate'].where(x['enddate'].notna() \n",
    "                                                                  & x['enddate'].lt(cutoff_date), cutoff_date))\n",
    "        .assign(duration = lambda x: (x['effective_end_date'] - x['startdate']).dt.days/ 365.25)\n",
    "    \n",
    "        # collapse down (note this will double count tenure if they held multiple roles \n",
    "        .groupby(['firmid', 'user_id', 'current_ctry', 'country'], as_index=False)\n",
    "        .agg({ 'duration': 'sum','years_since_active': 'min', 'total_compensation' : 'min', 'weight' : 'min'})\n",
    "        .rename(columns={'country':'ctry'})\n",
    "        .assign(year = year) \n",
    "    )\n",
    "    return(temp)\n",
    "\n",
    "### EXECUTE SCRAPING AND INITIAL COLLAPSE TO YEAR-firmid-ctry LEVEL \n",
    "[run_subsection(index) for index in range(num_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f4aba8d-38fb-46c8-93d7-f44a001533d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE \n",
    "linkedin_to_iso_cross_walk = (pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "temp_direct = processed_linkedin + 'temp_4'\n",
    "\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "\n",
    "## REMOVE \n",
    "output = pd.concat([output, pd.read_parquet(processed_linkedin + 'matched_firm_foreign_emp_history_pt1.parquet')], ignore_index = True)\n",
    "### Match to ISO-2 CODES AND EXPORT \n",
    "output.merge(linkedin_to_iso_cross_walk, how = 'left').to_parquet(processed_linkedin + 'matched_firm_foreign_emp_history.parquet')\n",
    "\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e93280-f01f-4ba5-930d-ea7b1f699a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firmid</th>\n",
       "      <th>user_id</th>\n",
       "      <th>current_ctry</th>\n",
       "      <th>ctry</th>\n",
       "      <th>duration</th>\n",
       "      <th>years_since_active</th>\n",
       "      <th>total_compensation</th>\n",
       "      <th>weight</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130002736</td>\n",
       "      <td>24268174.0</td>\n",
       "      <td>France</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2.001369</td>\n",
       "      <td>9.0</td>\n",
       "      <td>69702.68</td>\n",
       "      <td>1.293660</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130002736</td>\n",
       "      <td>112873685.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Ivory Coast</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>10.0</td>\n",
       "      <td>46917.96</td>\n",
       "      <td>1.371606</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130002736</td>\n",
       "      <td>112873685.0</td>\n",
       "      <td>France</td>\n",
       "      <td>New Caledonia</td>\n",
       "      <td>3.000684</td>\n",
       "      <td>6.0</td>\n",
       "      <td>46917.96</td>\n",
       "      <td>1.371606</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130002736</td>\n",
       "      <td>247833618.0</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>0.503765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29273.53</td>\n",
       "      <td>1.465224</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130002736</td>\n",
       "      <td>300268286.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Australia</td>\n",
       "      <td>5.503080</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64046.10</td>\n",
       "      <td>1.325858</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28960917</th>\n",
       "      <td>403822919</td>\n",
       "      <td>707018941.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.169747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46835.59</td>\n",
       "      <td>1.353552</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28960918</th>\n",
       "      <td>403822919</td>\n",
       "      <td>707018941.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>0.167009</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46835.59</td>\n",
       "      <td>1.353552</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28960919</th>\n",
       "      <td>398282939</td>\n",
       "      <td>199282649.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>0.082136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38729.63</td>\n",
       "      <td>1.543775</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28960920</th>\n",
       "      <td>403822919</td>\n",
       "      <td>707018941.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.169747</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46835.59</td>\n",
       "      <td>1.353552</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28960921</th>\n",
       "      <td>403822919</td>\n",
       "      <td>707018941.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>0.167009</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46835.59</td>\n",
       "      <td>1.353552</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28960922 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             firmid      user_id current_ctry            ctry  duration  \\\n",
       "0         130002736   24268174.0       France  United Kingdom  2.001369   \n",
       "1         130002736  112873685.0       France     Ivory Coast  0.999316   \n",
       "2         130002736  112873685.0       France   New Caledonia  3.000684   \n",
       "3         130002736  247833618.0      Belarus         Belarus  0.503765   \n",
       "4         130002736  300268286.0       France       Australia  5.503080   \n",
       "...             ...          ...          ...             ...       ...   \n",
       "28960917  403822919  707018941.0       France          Canada  0.169747   \n",
       "28960918  403822919  707018941.0       France         Ireland  0.167009   \n",
       "28960919  398282939  199282649.0       France         Denmark  0.082136   \n",
       "28960920  403822919  707018941.0       France          Canada  0.169747   \n",
       "28960921  403822919  707018941.0       France         Ireland  0.167009   \n",
       "\n",
       "          years_since_active  total_compensation    weight  year  \n",
       "0                        9.0            69702.68  1.293660  2009  \n",
       "1                       10.0            46917.96  1.371606  2009  \n",
       "2                        6.0            46917.96  1.371606  2009  \n",
       "3                        0.0            29273.53  1.465224  2009  \n",
       "4                        8.0            64046.10  1.325858  2009  \n",
       "...                      ...                 ...       ...   ...  \n",
       "28960917                 1.0            46835.59  1.353552  2019  \n",
       "28960918                 2.0            46835.59  1.353552  2019  \n",
       "28960919                 1.0            38729.63  1.543775  2020  \n",
       "28960920                 2.0            46835.59  1.353552  2020  \n",
       "28960921                 3.0            46835.59  1.353552  2020  \n",
       "\n",
       "[28960922 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5002775-d484-41c6-818f-517b8c2294e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_sum = [col for col in combined_output.columns if 'comp' in col or 'empl' in col]\n",
    "columns_to_keep = ['firmid', 'year','ctry'] + columns_to_sum\n",
    "\n",
    "combined_output = pd.concat(\n",
    "    [combined_output.loc[lambda x: ~x['needs_collapse'], columns_to_keep],\n",
    "    combined_output.loc[lambda x: x['needs_collapse']].groupby(['firmid', 'year', 'ctry'])[columns_to_sum].sum().reset_index()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "combined_output.to_parquet(processed_linkedin + 'matched_firm_foreign_employment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b3fe3-a220-43cd-a1d4-43ecc272433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate our compensation / employment datasets at the year level \n",
    "########################################################################################\n",
    "\n",
    "role_dict = pd.read_csv(processed_linkedin + 'revelio_role_dict.csv')\n",
    "prestige = (pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet').assign(\n",
    "    advanced_degree=lambda x: x['highest_degree'].isin(['Master', 'MBA', 'Doctor']).astype('boolean'),\n",
    "    college=lambda x: (x['advanced_degree'] | x['highest_degree'].isin(['Bachelor'])).astype('boolean'),\n",
    "    ed_data_avail = lambda x: ~x['highest_degree'].isna()))\n",
    "\n",
    "\n",
    "int_cols = list((set(role_dict.columns) - {'role_k1500', 'Unnamed: 0'}) | {'french_data','french','abroad'})\n",
    "matching_output = pd.read_parquet(pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet')[['rcid', 'firmid']])\n",
    "\n",
    "### Get the firm info at the siren level \n",
    "base_vars = ['has_subsid', 'is_subsid', 'is_public', 'has_lei', 'french_hq', 'parent_non_french_hq']\n",
    "matched_firm_base_info = (\n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_base_info.parquet').merge(matching_output)\n",
    "    .assign(needs_collapse = lambda df: df.groupby('firmid')['firmid'].transform('count') > 1)\n",
    ")\n",
    "matched_firm_base_info = pd.concat(\n",
    "    [matched_firm_base_info.loc[lambda x: ~x['needs_collapse'], ['firmid'] + base_vars],\n",
    "     matched_firm_base_info.loc[lambda x: x['needs_collapse']].groupby('firmid', as_index=False)[base_vars].max()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "### generate the base for the role data collapse \n",
    "role_data = (\n",
    "    ## merge together all the component datasets \n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "    .merge(prestige, how = 'left')\n",
    "    .merge(matching_output)\n",
    "    .merge(role_dict, how = 'left')\n",
    "    \n",
    "    ## generate necessary variables  \n",
    "     .assign(french= lambda x: x['country'].eq(\"France\"),\n",
    "         comp =  lambda x: x['total_compensation']*x['weight'])\n",
    "    .assign(french_data = lambda x: x['data'] & x['french'],\n",
    "            abroad = lambda x: ~x['french'] & ~x['country'].isna(),\n",
    "            comp_ed_avail = lambda x: x['ed_data_avail']*x['comp'],\n",
    "            weight_ed_avail = lambda x: x['ed_data_avail']*x['weight'])\n",
    ")\n",
    "\n",
    "### Carry out the collapse \n",
    "def collapse_wrapper(year):\n",
    "    clear_output(wait=True)\n",
    "    print(year)\n",
    "    temp = (\n",
    "        role_data\n",
    "        .assign(valid = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .assign(valid = lambda x: x['valid'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid'])['comp'].transform('max')))\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .assign(ed_comp_denom = lambda x: (x.groupby('firmid')['comp_ed_avail'].transform('sum').where(lambda s: s != 0)),\n",
    "                ed_weight_denom = lambda x: (x.groupby('firmid')['weight_ed_avail'].transform('sum').where(lambda s: s != 0)))\n",
    "        .assign(\n",
    "            comp_weighted_college = lambda x:         x['college']*x['comp']/ x['ed_comp_denom'] ,          \n",
    "            comp_weighted_advanced_degree = lambda x: x['advanced_degree']*x['comp']/ x['ed_comp_denom'],  \n",
    "            weighted_college = lambda x:              x['college']*x['weight'] / x['ed_weight_denom'],       \n",
    "            weighted_advanced_degree = lambda x:      x['advanced_degree']*x['weight'] / x['ed_weight_denom'],\n",
    "            comp_weighted_prestige = lambda x:        x['prestige'] * x['comp'] / x.groupby('firmid')['comp'].transform('sum'),\n",
    "            weighted_prestige      = lambda x:        x['prestige'] * x['weight'] / x.groupby('firmid')['weight'].transform('sum')     \n",
    "       ))\n",
    "    prestige_vars = ['prestige','college', 'advanced_degree']\n",
    "    prestige_vars = [f'comp_weighted_{var}' for var in prestige_vars] + [f'weighted_{var}' for var in prestige_vars] \n",
    "    temp_prestige = temp.groupby('firmid', as_index=False)[prestige_vars].sum()\n",
    "\n",
    "    temp = (temp.melt(id_vars=['firmid', 'comp', 'weight'], value_vars=int_cols,var_name='type',\n",
    "            value_name='valid_flag')\n",
    "     .loc[lambda x: x['valid_flag'].eq(1)]\n",
    "     .groupby(['firmid', 'type'])\n",
    "     .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "     .reset_index()\n",
    "     .pivot(index=['firmid'], columns='type', values=['empl', 'comp']))\n",
    "\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp = pd.merge(temp, temp_prestige,  how='outer')\n",
    "    temp = temp.assign(year = year)\n",
    "    return(temp)\n",
    "role_annual_collapsed = pd.concat([collapse_wrapper(year) for year in range(2008, 2024)], ignore_index=True)\n",
    "\n",
    "cols_to_fill = [col for col in role_annual_collapsed.columns\n",
    "                if (col.startswith('empl') or col.startswith('comp'))\n",
    "                and not any(exclude in col for exclude in ['prestige', 'college', 'advanced_degree'])\n",
    "               ]\n",
    "role_annual_collapsed[cols_to_fill] = role_annual_collapsed[cols_to_fill].fillna(0)\n",
    "\n",
    "final_output = (pd.merge(role_annual_collapsed, matched_firm_base_info, how = 'left')\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_wgted_comp_output.parquet'),how = 'left')\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_weight_output.parquet'),how = 'left'))\n",
    "for var in int_cols[1:]: final_output[f'share_comp_{var}'] = final_output[f'comp_{var}'] / final_output['comp_total']\n",
    "final_output.to_parquet(processed_linkedin + 'matched_firm_empl_and_linkedin_characteristics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b191c7d-2559-4548-8316-067a5b518a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Move all files to their final location\n",
    "########################################################################################\n",
    "for_emp = 'matched_firm_foreign_employment.parquet'\n",
    "all_char = 'matched_firm_empl_and_linkedin_characteristics.parquet'\n",
    "roles_in_ctry = 'data_roles_in_all_countries.parquet'\n",
    "all_matched_ids = 'all_linkedin_matched_firmids_final.parquet'\n",
    "output_dir = '1) data/15_revelio_data/2_outputs/'\n",
    "\n",
    "pd.read_parquet(processed_linkedin + for_emp).to_parquet(output_dir + '15a_'+ for_emp) \n",
    "pd.read_parquet(processed_linkedin + all_char).to_parquet(output_dir + '15b_'+ all_char) \n",
    "pd.read_parquet(processed_linkedin + roles_in_ctry).to_parquet(output_dir + '15c_'+ roles_in_ctry) \n",
    "pd.read_parquet(processed_admin + all_matched_ids).to_parquet(output_dir + '15d_'+ all_matched_ids) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
