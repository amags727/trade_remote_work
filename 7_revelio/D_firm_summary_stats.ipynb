{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f599a061-8995-4821-81c7-cb6ee6bb0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata','sys', 'matplotlib.pyplot as plt', 'glob', 'shutil','from sklearn.decomposition import PCA']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "processed_linkedin = '1) data/15_revelio_data/1_inputs/b_processed_data/linkedin/'\n",
    "processed_admin = '1) data/15_revelio_data/1_inputs/b_processed_data/admin/'\n",
    "output_dir = '1) data/15_revelio_outputs/2_outputs/'\n",
    "import A_helper_functions as hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151d12b-649e-408c-bacb-4ba1e78dbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES ASSOCIATED WITH OUR MATCHED COMPANIES\n",
    "########################################################################################\n",
    "remaining_to_find = (pd.read_parquet(processed_linkedin + 'firm_lvl_info_all_matched_firms.parquet'))\n",
    "remaining_to_find = (remaining_to_find.  ## REMOVE\n",
    "    .loc[lambda x:  ~x['rcid'].isin(pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')['rcid'])\n",
    "    ])\n",
    "\n",
    "num_chunks = 50\n",
    "temp_direct = processed_linkedin + 'temp'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(remaining_to_find['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = role_output = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,country, weight, total_compensation, startdate, enddate, role_k1500 \n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        for col in ['startdate', 'enddate']:\n",
    "            temp[col] = pd.to_datetime(temp[col], errors='coerce')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "output = pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "output = pd.concat([output, pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')]) ## REMOVE \n",
    "output.to_parquet(processed_linkedin + 'matched_firm_role_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa01ffc-517c-48bb-b3c9-df94fc2b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Perform PCA to generate metrics of differentiation \n",
    "########################################################################################\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, weight_var, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=(weight_var, 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'{weight_var}_PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren': 'firmid'})\n",
    "output = pd.merge(long_data, matching_output)\n",
    "\n",
    "# run pca analysis\n",
    "for weight_var in ['wgted_comp','weight']:\n",
    "    print('starting pca gen')\n",
    "    pca_model = collapse_year_level(sample_year, True, weight_var)\n",
    "    print('finished pca gen')\n",
    "    [collapse_year_level(year,False,weight_var,pca_model) for year in range(2008, 2024)]\n",
    "    (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "     .to_parquet(processed_linkedin + 'matched_firm_pca_'+ weight_var + '_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306574f-5521-454c-bce7-0b004cf1c186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE PRESTIGE / education OF ALL EMPLOYEES ASSOCIATED WITH OUR MATCHED COMPANIES \n",
    "########################################################################################\n",
    "num_chunks = 50\n",
    "temp_direct =  processed_linkedin + 'temp' \n",
    "\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "chunks = np.array_split(role_output['user_id'].unique(), num_chunks)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path): \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            select user_id, prestige, highest_degree \n",
    "            from revelio.individual_user \n",
    "            where user_id IN %(user_ids)s\n",
    "            \"\"\",\n",
    "            params= {'user_ids': tuple(chunks[index].tolist())})\n",
    "        temp.to_parquet(file_path)\n",
    "    \n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],\n",
    "           ignore_index = True)\n",
    " .to_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147253e-3724-4d80-b73f-7b0221f8ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE AMOUNT OF WORKERS CURRENTLY WORKING OR WITH EXPERIENCE ABROAD PER COMPANY \n",
    "########################################################################################\n",
    "\n",
    "### SET PARAMETERS\n",
    "num_chunks = 500\n",
    "temp_direct = processed_linkedin + 'temp_user_output'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet').merge(matching_output)\n",
    "chunks = np.array_split(role_output['firmid'].unique(), num_chunks)\n",
    "linkedin_to_iso_cross_walk = (pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "worker_prestige_dta = pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet')\n",
    "\n",
    "### DEFINE FUNCTIONS \n",
    "def run_subsection(index):\n",
    "    clear_output(wait=True)\n",
    "    print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "    \n",
    "    role_subset = role_output.loc[lambda x: x['firmid'].isin(chunks[index])]\n",
    "    params = {\"user_ids\":  tuple(role_subset['user_id'].unique())}\n",
    "    ever_role_subset = db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id, country, startdate, enddate\n",
    "        FROM revelio.individual_positions \n",
    "        where user_id IN %(user_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "    )\n",
    "    print('done scraping')\n",
    "    subset_output = pd.concat([collapse_year_level(year, role_subset, ever_role_subset) for year in range(2009, 2021)],\n",
    "                            ignore_index=True)\n",
    "    subset_output.to_parquet(temp_direct + \"/temp\"+str(index)+\".parquet\")\n",
    "\n",
    "    \n",
    "def collapse_year_level(year, role_subset, ever_role_subset):\n",
    "    print(year)\n",
    "    cutoff_date = pd.Timestamp(f'{year}-01-01')\n",
    "    temp = (\n",
    "        ## determine which users are active in a given year for a given firm\n",
    "        role_subset\n",
    "        .rename(columns={'country':'current_ctry'})\n",
    "        .assign(\n",
    "                startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign( valid = lambda x: (x['startdate'] <= cutoff_date) & ((x['enddate'].isna()) | (x['enddate'] >= cutoff_date)))\n",
    "        .loc[lambda x: x['valid'], ['firmid','user_id', 'current_ctry', 'weight', 'total_compensation']]\n",
    "        .assign(priority = lambda x: (x['current_ctry'] == 'France').astype(int))\n",
    "        .sort_values(by = 'priority', ascending = False)\n",
    "        .drop_duplicates(subset = ['firmid', 'user_id'], keep = 'first')\n",
    "    \n",
    "        ## add all the roles those users have ever held \n",
    "         .merge(ever_role_subset).drop_duplicates()\n",
    "    \n",
    "        # drop all roles that start after the year of interest\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce'),\n",
    "               years_since_active = lambda x: (year - x['enddate'].dt.year.fillna(year)).clip(lower=0))\n",
    "         .loc[lambda x: (x['startdate'] <= cutoff_date) & ~x['country'].eq('France')]\n",
    "    \n",
    "        # compute the employee's effective tenure in the foreign market \n",
    "        .assign(effective_end_date = lambda x: x['enddate'].where(x['enddate'].notna() \n",
    "                                                                  & x['enddate'].lt(cutoff_date), cutoff_date))\n",
    "        .assign(duration = lambda x: (x['effective_end_date'] - x['startdate']).dt.days/ 365.25)\n",
    "    \n",
    "        # collapse down (note this will double count tenure if they held multiple roles \n",
    "        .groupby(['firmid', 'user_id', 'current_ctry', 'country'], as_index=False)\n",
    "        .agg({ 'duration': 'sum','years_since_active': 'min', 'total_compensation' : 'min', 'weight' : 'min'})\n",
    "        .merge(worker_prestige_dta, how = 'left')\n",
    "        .rename(columns={'country':'ctry'})\n",
    "        .assign(year = year) \n",
    "    )\n",
    "    return(temp)\n",
    "\n",
    "### EXECUTE SCRAPING AND INITIAL COLLAPSE TO YEAR-firmid-ctry LEVEL \n",
    "[run_subsection(index) for index in range(num_chunks)]\n",
    "\n",
    "### Match to ISO-2 CODES AND EXPORT \n",
    "combined_output = (\n",
    "    pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "    #.merge(linkedin_to_iso_cross_walk, how = 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5002775-d484-41c6-818f-517b8c2294e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_sum = [col for col in combined_output.columns if 'comp' in col or 'empl' in col]\n",
    "columns_to_keep = ['firmid', 'year','ctry'] + columns_to_sum\n",
    "\n",
    "combined_output = pd.concat(\n",
    "    [combined_output.loc[lambda x: ~x['needs_collapse'], columns_to_keep],\n",
    "    combined_output.loc[lambda x: x['needs_collapse']].groupby(['firmid', 'year', 'ctry'])[columns_to_sum].sum().reset_index()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "combined_output.to_parquet(processed_linkedin + 'matched_firm_foreign_employment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b3fe3-a220-43cd-a1d4-43ecc272433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate our compensation / employment datasets at the year level \n",
    "########################################################################################\n",
    "\n",
    "role_dict = pd.read_csv(processed_linkedin + 'revelio_role_dict.csv')\n",
    "prestige = (pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet').assign(\n",
    "    advanced_degree=lambda x: x['highest_degree'].isin(['Master', 'MBA', 'Doctor']).astype('boolean'),\n",
    "    college=lambda x: (x['advanced_degree'] | x['highest_degree'].isin(['Bachelor'])).astype('boolean'),\n",
    "    ed_data_avail = lambda x: ~x['highest_degree'].isna()))\n",
    "\n",
    "\n",
    "int_cols = list((set(role_dict.columns) - {'role_k1500', 'Unnamed: 0'}) | {'french_data','french','abroad'})\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "\n",
    "### Get the firm info at the siren level \n",
    "base_vars = ['has_subsid', 'is_subsid', 'is_public', 'has_lei', 'french_hq', 'parent_non_french_hq']\n",
    "matched_firm_base_info = (\n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_base_info.parquet').merge(matching_output)\n",
    "    .assign(needs_collapse = lambda df: df.groupby('firmid')['firmid'].transform('count') > 1)\n",
    ")\n",
    "matched_firm_base_info = pd.concat(\n",
    "    [matched_firm_base_info.loc[lambda x: ~x['needs_collapse'], ['firmid'] + base_vars],\n",
    "     matched_firm_base_info.loc[lambda x: x['needs_collapse']].groupby('firmid', as_index=False)[base_vars].max()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "### generate the base for the role data collapse \n",
    "role_data = (\n",
    "    ## merge together all the component datasets \n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "    .merge(prestige, how = 'left')\n",
    "    .merge(matching_output)\n",
    "    .merge(role_dict, how = 'left')\n",
    "    \n",
    "    ## generate necessary variables  \n",
    "     .assign(french= lambda x: x['country'].eq(\"France\"),\n",
    "         comp =  lambda x: x['total_compensation']*x['weight'])\n",
    "    .assign(french_data = lambda x: x['data'] & x['french'],\n",
    "            abroad = lambda x: ~x['french'] & ~x['country'].isna(),\n",
    "            comp_ed_avail = lambda x: x['ed_data_avail']*x['comp'],\n",
    "            weight_ed_avail = lambda x: x['ed_data_avail']*x['weight'])\n",
    ")\n",
    "\n",
    "### Carry out the collapse \n",
    "def collapse_wrapper(year):\n",
    "    clear_output(wait=True)\n",
    "    print(year)\n",
    "    temp = (\n",
    "        role_data\n",
    "        .assign(valid = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .assign(valid = lambda x: x['valid'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid'])['comp'].transform('max')))\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .assign(ed_comp_denom = lambda x: (x.groupby('firmid')['comp_ed_avail'].transform('sum').where(lambda s: s != 0)),\n",
    "                ed_weight_denom = lambda x: (x.groupby('firmid')['weight_ed_avail'].transform('sum').where(lambda s: s != 0)))\n",
    "        .assign(\n",
    "            comp_weighted_college = lambda x:         x['college']*x['comp']/ x['ed_comp_denom'] ,          \n",
    "            comp_weighted_advanced_degree = lambda x: x['advanced_degree']*x['comp']/ x['ed_comp_denom'],  \n",
    "            weighted_college = lambda x:              x['college']*x['weight'] / x['ed_weight_denom'],       \n",
    "            weighted_advanced_degree = lambda x:      x['advanced_degree']*x['weight'] / x['ed_weight_denom'],\n",
    "            comp_weighted_prestige = lambda x:        x['prestige'] * x['comp'] / x.groupby('firmid')['comp'].transform('sum'),\n",
    "            weighted_prestige      = lambda x:        x['prestige'] * x['weight'] / x.groupby('firmid')['weight'].transform('sum')     \n",
    "       ))\n",
    "    prestige_vars = ['prestige','college', 'advanced_degree']\n",
    "    prestige_vars = [f'comp_weighted_{var}' for var in prestige_vars] + [f'weighted_{var}' for var in prestige_vars] \n",
    "    temp_prestige = temp.groupby('firmid', as_index=False)[prestige_vars].sum()\n",
    "\n",
    "    temp = (temp.melt(id_vars=['firmid', 'comp', 'weight'], value_vars=int_cols,var_name='type',\n",
    "            value_name='valid_flag')\n",
    "     .loc[lambda x: x['valid_flag'].eq(1)]\n",
    "     .groupby(['firmid', 'type'])\n",
    "     .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "     .reset_index()\n",
    "     .pivot(index=['firmid'], columns='type', values=['empl', 'comp']))\n",
    "\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp = pd.merge(temp, temp_prestige,  how='outer')\n",
    "    temp = temp.assign(year = year)\n",
    "    return(temp)\n",
    "role_annual_collapsed = pd.concat([collapse_wrapper(year) for year in range(2008, 2024)], ignore_index=True)\n",
    "\n",
    "cols_to_fill = [col for col in role_annual_collapsed.columns\n",
    "                if (col.startswith('empl') or col.startswith('comp'))\n",
    "                and not any(exclude in col for exclude in ['prestige', 'college', 'advanced_degree'])\n",
    "               ]\n",
    "role_annual_collapsed[cols_to_fill] = role_annual_collapsed[cols_to_fill].fillna(0)\n",
    "\n",
    "final_output = (pd.merge(role_annual_collapsed, matched_firm_base_info, how = 'left')\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_wgted_comp_output.parquet'),how = 'left')\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_weight_output.parquet'),how = 'left'))\n",
    "for var in int_cols[1:]: final_output[f'share_comp_{var}'] = final_output[f'comp_{var}'] / final_output['comp_total']\n",
    "final_output.to_parquet(processed_linkedin + 'matched_firm_empl_and_linkedin_characteristics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b191c7d-2559-4548-8316-067a5b518a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Move all files to their final location\n",
    "########################################################################################\n",
    "for_emp = 'matched_firm_foreign_employment.parquet'\n",
    "all_char = 'matched_firm_empl_and_linkedin_characteristics.parquet'\n",
    "roles_in_ctry = 'data_roles_in_all_countries.parquet'\n",
    "all_matched_ids = 'all_linkedin_matched_firmids_final.parquet'\n",
    "output_dir = '1) data/15_revelio_data/2_outputs/'\n",
    "\n",
    "pd.read_parquet(processed_linkedin + for_emp).to_parquet(output_dir + '15a_'+ for_emp) \n",
    "pd.read_parquet(processed_linkedin + all_char).to_parquet(output_dir + '15b_'+ all_char) \n",
    "pd.read_parquet(processed_linkedin + roles_in_ctry).to_parquet(output_dir + '15c_'+ roles_in_ctry) \n",
    "pd.read_parquet(processed_admin + all_matched_ids).to_parquet(output_dir + '15d_'+ all_matched_ids) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
