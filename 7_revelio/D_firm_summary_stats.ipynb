{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45631567-c818-4895-97d8-2978bb7022bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP \n",
    "imports = ['wrds', 'pandas as pd', 'os', 're', 'pickle', 'numpy as np', 'from name_matching.name_matcher import NameMatcher',\n",
    "          'from joblib import Parallel, delayed', 'from IPython.display import display, HTML, clear_output',\n",
    "          'unicodedata','sys', 'matplotlib.pyplot as plt', 'glob', 'shutil','from sklearn.decomposition import PCA']\n",
    "for command in imports:\n",
    "    if command.startswith('from'): exec(command)\n",
    "    else: exec('import ' + command)\n",
    "\n",
    "if not os.getcwd().endswith('Big Data'):\n",
    "    os.chdir('../..')\n",
    "\n",
    "sys.path.append('trade_data_code/2_python')\n",
    "processed_linkedin = '1) data/15_revelio_outputs/1_inputs/b_processed_data/linkedin/'\n",
    "processed_admin = '1) data/15_revelio_outputs/1_inputs/b_processed_data/admin/'\n",
    "output_dir = '1) data/15_revelio_outputs/2_outputs/'\n",
    "import A_helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc2122-b58c-41d4-ac37-e31d83860c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Find Ancilliary Information Associated with our matched companies \n",
    "########################################################################################\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')\n",
    "num_chunks = 50\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(matching_output['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):  \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = (\n",
    "            db.raw_sql(\n",
    "                \"\"\"\n",
    "                SELECT rcid, child_rcid, ultimate_parent_rcid, ticker, lei,  hq_zip_code, hq_metro_area, hq_state, hq_country\n",
    "                FROM  revelio.company_mapping \n",
    "                WHERE rcid IN %(rcid_list)s\n",
    "                \"\"\",\n",
    "                params = {'rcid_list': tuple(chunks[index].tolist())})\n",
    "            .assign(has_subsid = lambda x: ~x['rcid'].eq(x['child_rcid']),\n",
    "                    is_subsid = lambda x: ~x['rcid'].eq(x['ultimate_parent_rcid']),\n",
    "                    is_public = lambda x: ~x['ticker'].isna(),\n",
    "                    has_lei = lambda x: ~x['lei'].isna(),\n",
    "                    french_hq = lambda x: ~x['hq_country'].isna(),\n",
    "                    hq_metro_area =lambda x: x['hq_metro_area']\n",
    "                    .str.replace('france nonmetropolitan area', 'non_metro', regex=False)\n",
    "                    .str.replace('metropolitan area', '', regex=False)\n",
    "                    .str.strip()))\n",
    "\n",
    "        parent_temp = (\n",
    "            db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT ultimate_parent_rcid, hq_country\n",
    "            FROM  revelio.company_mapping \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\",\n",
    "            params = {'rcid_list':  tuple(temp.loc[temp['is_subsid']]['ultimate_parent_rcid'].unique().tolist())})\n",
    "            .assign(parent_non_french_hq = lambda x: (~x['hq_country'].eq('France')).where(x['hq_country'].notna()))\n",
    "            .rename(columns={\"hq_country\": \"parent_hq_country\"}))\n",
    "        temp = pd.merge(temp, parent_temp, how = 'left')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    " .to_parquet(processed_linkedin + 'matched_firm_base_info.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa829d8-1239-4afa-8133-dbdd8e3745ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND ALL ROLES ASSOCIATED WITH OUR MATCHED COMPANIES\n",
    "########################################################################################\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')\n",
    "num_chunks = 50\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "chunks = np.array_split(matching_output['rcid'].unique(), num_chunks)\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path):    \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        params = {'rcid_list': tuple(chunks[index].tolist())}\n",
    "        temp = role_output = db.raw_sql(\n",
    "            \"\"\"\n",
    "            SELECT rcid,user_id,country, weight, total_compensation, startdate, enddate, role_k1500 \n",
    "            FROM revelio.individual_positions \n",
    "            WHERE rcid IN %(rcid_list)s\n",
    "            \"\"\", \n",
    "            params= params)\n",
    "        for col in ['startdate', 'enddate']:\n",
    "            temp[col] = pd.to_datetime(temp[col], errors='coerce')\n",
    "        temp.to_parquet(file_path)\n",
    "\n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],\n",
    "           ignore_index = True)\n",
    " .to_parquet(processed_linkedin + 'matched_firm_role_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa01ffc-517c-48bb-b3c9-df94fc2b1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Perform PCA to generate metrics of differentiation \n",
    "########################################################################################\n",
    "temp_direct = processed_linkedin + 'temp_role'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "\n",
    "def collapse_year_level(year, making_pca, weight_var, pca_model=None):\n",
    "    temp = (\n",
    "        output.assign(\n",
    "            valid=lambda x: x['startdate'].dt.year.le(year) & \n",
    "                            (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "            wgted_comp=lambda x: x['total_compensation'] * x['weight']\n",
    "        )\n",
    "        .loc[lambda x: x['valid']]\n",
    "        .groupby(['firmid', 'role_k1500'], as_index=False)\n",
    "        .agg(comp=(weight_var, 'sum'))\n",
    "        .assign(year=year)\n",
    "        .pivot_table(index=['firmid', 'year'], columns='role_k1500', values='comp', aggfunc='sum', fill_value=0)\n",
    "        .pipe(lambda df: df.div(df.sum(axis=1), axis=0))\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "    )\n",
    "    if making_pca:\n",
    "        pca_model = PCA(n_components=10)\n",
    "        pca_model.fit(temp)\n",
    "        return pca_model\n",
    "    else:\n",
    "        file_path = temp_direct + \"/temp\" + str(year) + \".parquet\"\n",
    "        pd.concat([\n",
    "            temp.reset_index()[['firmid', 'year']],\n",
    "            pd.DataFrame(pca_model.transform(temp), columns=[f'{weight_var}_PC{i+1}' for i in range(10)])\n",
    "        ], axis=1).to_parquet(file_path)\n",
    "        print(year)\n",
    "        \n",
    "#set param values \n",
    "years = range(2008, 2024)\n",
    "sample_year = 2015\n",
    "   \n",
    "# Load and merge data\n",
    "long_data = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren': 'firmid'})\n",
    "output = pd.merge(long_data, matching_output)\n",
    "\n",
    "# run pca analysis\n",
    "for weight_var in ['wgted_comp','weight']:\n",
    "    print('starting pca gen')\n",
    "    pca_model = collapse_year_level(sample_year, True, weight_var)\n",
    "    print('finished pca gen')\n",
    "    [collapse_year_level(year,False,weight_var,pca_model) for year in range(2008, 2024)]\n",
    "    (pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "     .to_parquet(processed_linkedin + 'matched_firm_pca_'+ weight_var + '_output.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306574f-5521-454c-bce7-0b004cf1c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE PRESTIGE / education OF ALL EMPLOYEES ASSOCIATED WITH OUR MATCHED COMPANIES \n",
    "########################################################################################\n",
    "num_chunks = 50\n",
    "temp_direct =  processed_linkedin + 'temp' \n",
    "\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "chunks = np.array_split(role_output['user_id'].unique(), num_chunks)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "\n",
    "for index in range(num_chunks):\n",
    "    file_path = temp_direct + \"/temp\" + str(index) + \".parquet\"\n",
    "    if not os.path.exists(file_path): \n",
    "        clear_output(wait=True)\n",
    "        print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "        temp = db.raw_sql(\n",
    "            \"\"\"\n",
    "            select user_id, prestige, highest_degree \n",
    "            from revelio.individual_user \n",
    "            where user_id IN %(user_ids)s\n",
    "            \"\"\",\n",
    "            params= {'user_ids': tuple(chunks[index].tolist())})\n",
    "        temp.to_parquet(file_path)\n",
    "    \n",
    "(pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],\n",
    "           ignore_index = True)\n",
    " .to_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet'))\n",
    "shutil.rmtree(temp_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437db715-baa5-4688-83c2-9541a919da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# FIND THE AMOUNT OF WORKERS CURRENTLY WORKING OR WITH EXPERIENCE ABROAD PER COMPANY \n",
    "########################################################################################\n",
    "\n",
    "### SET PARAMETERS\n",
    "num_chunks = 500\n",
    "temp_direct = processed_linkedin + 'temp_user_output'\n",
    "os.makedirs(temp_direct, exist_ok=True)\n",
    "db = wrds.Connection(wrds_username='am0195')\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "role_output = pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet').merge(matching_output)\n",
    "chunks = np.array_split(role_output['firmid'].unique(), num_chunks)\n",
    "linkedin_to_iso_cross_walk = (pd.read_csv(processed_admin +'linkedin_to_iso_crosswalk.csv')\n",
    "                              .assign(needs_collapse = lambda df: df.groupby('ctry')['ctry'].transform('count') >1))\n",
    "\n",
    "### DEFINE FUNCTIONS \n",
    "def run_subsection(index):\n",
    "    clear_output(wait=True)\n",
    "    print(str(round(100*(index+1)/num_chunks,2))+ '%')\n",
    "\n",
    "    role_subset = role_output.loc[lambda x: x['firmid'].isin(chunks[index])]\n",
    "    params = {\"user_ids\":  tuple(role_subset['user_id'].unique())}\n",
    "    ever_role_subset = db.raw_sql(\n",
    "        \"\"\"\n",
    "        SELECT user_id, rcid, country, startdate, enddate, weight, total_compensation, seniority\n",
    "        FROM revelio.individual_positions \n",
    "        where user_id IN %(user_ids)s\n",
    "        \"\"\",\n",
    "        params= params \n",
    "        )\n",
    "    print('finished scraping')\n",
    "    subset_output = pd.concat([collapse_year_level(year, role_subset, ever_role_subset) for year in range(2008, 2024)],\n",
    "                            ignore_index=True)\n",
    "    subset_output.to_parquet(temp_direct + \"/temp\"+str(index)+\".parquet\")\n",
    "\n",
    "    \n",
    "def collapse_year_level(year, role_subset, ever_role_subset):\n",
    "    valid_cols = ['valid_now', 'valid_ever', 'valid_l5'] \n",
    "    temp = (\n",
    "         ## Determine which users are active in a given year for a given firm \n",
    "         role_subset\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce')) \n",
    "        .assign(valid=lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .loc[lambda x: x['valid'], ['firmid','user_id']].drop_duplicates()\n",
    "\n",
    "        ## add all the roles those users have ever held \n",
    "        .merge(ever_role_subset)\n",
    "\n",
    "        # drop all roles that start after the year of interest\n",
    "        .drop_duplicates()\n",
    "        .assign(startdate =lambda x: pd.to_datetime(x['startdate'], errors='coerce'),\n",
    "                enddate =lambda x: pd.to_datetime(x['enddate'], errors='coerce'),\n",
    "                comp =lambda x: x['total_compensation']*x['weight'],\n",
    "               )\n",
    "         .loc[lambda x: x['startdate'].dt.year.le(year)]\n",
    "\n",
    "        ## Mark if the the position occured in the year of interest, within 5 years of the year of interest or ever.\n",
    "        ## Second assign step restricts to only the highest compensation value over the validity period. Stops us from double counting promotions etc. \n",
    "        .assign(valid_ever = lambda x: x['comp'] == x.groupby(['user_id', 'firmid', 'country'])['comp'].transform('max'),\n",
    "                valid_now = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)),\n",
    "                valid_l5 = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year-5)))\n",
    "        .assign(valid_now = lambda x: x['valid_now'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid_now'])['comp'].transform('max')),\n",
    "                valid_l5 = lambda x: x['valid_l5'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid_l5'])['comp'].transform('max')))\n",
    "\n",
    "        ## now melt the data frame \n",
    "        .melt(\n",
    "        id_vars=['country', 'firmid', 'comp', 'weight'],\n",
    "        value_vars=valid_cols,\n",
    "        var_name='valid_type',\n",
    "        value_name='valid_flag')\n",
    "        .loc[lambda x: x['valid_flag']]\n",
    "\n",
    "        ## collapse by country firmid valid_type \n",
    "        .groupby(['country', 'firmid', 'valid_type'])\n",
    "        .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "        .reset_index()\n",
    "\n",
    "        ## reshape back to wide \n",
    "        .pivot(index=['firmid', 'country'], columns='valid_type', values=['empl', 'comp'])\n",
    "    )\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns = temp.columns.str.replace('valid_', '', regex=True)\n",
    "    temp['year'] = year\n",
    "    return(temp)\n",
    "\n",
    "### EXECUTE SCRAPING AND INITIAL COLLAPSE TO YEAR-firmid-ctry LEVEL \n",
    "[run_subsection(index) for index in range(num_chunks)]\n",
    "\n",
    "### Match to ISO-2 CODES AND EXPORT \n",
    "combined_output = (\n",
    "    pd.concat([pd.read_parquet(file) for file in glob.glob(temp_direct + \"/*.parquet\")],ignore_index = True)\n",
    "    .merge(linkedin_to_iso_cross_walk, how = 'left'))\n",
    "\n",
    "columns_to_sum = [col for col in combined_output.columns if 'comp' in col or 'empl' in col]\n",
    "columns_to_keep = ['firmid', 'year','ctry'] + columns_to_sum\n",
    "\n",
    "combined_output = pd.concat(\n",
    "    [combined_output.loc[lambda x: ~x['needs_collapse'], columns_to_keep],\n",
    "    combined_output.loc[lambda x: x['needs_collapse']].groupby(['firmid', 'year', 'ctry'])[columns_to_sum].sum().reset_index()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "combined_output.to_parquet(processed_linkedin + 'matched_firm_foreign_employment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a9dca-1a91-4df7-bfab-71f8dde7081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Generate our compensation / employment datasets at the year level \n",
    "########################################################################################\n",
    "role_dict = pd.read_csv(processed_linkedin + 'revelio_role_dict.csv')\n",
    "prestige = (pd.read_parquet(processed_linkedin + 'matched_firm_user_prestige.parquet').assign(\n",
    "    advanced_degree=lambda x: x['highest_degree'].isin(['Master', 'MBA', 'Doctor']).astype('boolean'),\n",
    "    college=lambda x: (x['advanced_degree'] | x['highest_degree'].isin(['Bachelor'])).astype('boolean')))\n",
    "prestige.loc[prestige['highest_degree'].isna(), ['advanced_degree', 'college']] = pd.NA\n",
    "\n",
    "int_cols = list((set(role_dict.columns) - {'role_k1500', 'Unnamed: 0'}) | {'french_data'})\n",
    "matching_output = pd.read_parquet(processed_admin +'fuzzy_matching_output_final.parquet')[['rcid', 'siren']].rename(columns={'siren':'firmid'})\n",
    "\n",
    "### Get the firm info at the siren level \n",
    "base_vars = ['has_subsid', 'is_subsid', 'is_public', 'has_lei', 'french_hq', 'parent_non_french_hq']\n",
    "matched_firm_base_info = (\n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_base_info.parquet').merge(matching_output)\n",
    "    .assign(needs_collapse = lambda df: df.groupby('firmid')['firmid'].transform('count') > 1)\n",
    ")\n",
    "matched_firm_base_info = pd.concat(\n",
    "    [matched_firm_base_info.loc[lambda x: ~x['needs_collapse'], ['firmid'] + base_vars],\n",
    "     matched_firm_base_info.loc[lambda x: x['needs_collapse']].groupby('firmid', as_index=False)[base_vars].max()]\n",
    "    , axis=0, ignore_index=True)\n",
    "\n",
    "### generate the base for the role data collapse \n",
    "role_data = (\n",
    "    ## merge together all the component datasets \n",
    "    pd.read_parquet(processed_linkedin + 'matched_firm_role_output.parquet')\n",
    "    .merge(prestige, how = 'left')\n",
    "    .merge(matching_output)\n",
    "    .merge(role_dict, how = 'left')\n",
    "    \n",
    "    ## generate necessary variables  \n",
    "     .assign(french= lambda x: x['country'].eq(\"France\"),\n",
    "         comp =  lambda x: x['total_compensation']*x['weight'])\n",
    "    .assign(french_data = lambda x: x['data'] & x['french'])\n",
    ")\n",
    "\n",
    "### Carry out the collapse \n",
    "def collapse_wrapper(year):\n",
    "    clear_output(wait=True)\n",
    "    print(year)\n",
    "    temp = (\n",
    "        role_data\n",
    "        .assign(valid = lambda x: x['startdate'].dt.year.le(year) & (x['enddate'].isna() | x['enddate'].dt.year.ge(year)))\n",
    "        .assign(valid = lambda x: x['valid'] & x['comp'].eq(x.groupby(['user_id', 'firmid', 'country','valid'])['comp'].transform('max')))\n",
    "        .loc[lambda x: x['valid']]\n",
    "         #generate the denominator for college vars since there can be NAs\n",
    "        .assign(\n",
    "            comp_weighted_college = lambda x: (~x['college'].isna())*x['comp'],\n",
    "            comp_weighted_advanced_degree = lambda x: (~x['advanced_degree'].isna())*x['comp'],\n",
    "            weighted_college = lambda x: (~x['college'].isna())*x['weight'],\n",
    "            weighted_advanced_degree = lambda x: (~x['advanced_degree'].isna())*x['weight'])\n",
    "         #generate firm averages\n",
    "           .assign(\n",
    "            comp_weighted_college = lambda x: x['college']*x['comp']                 /x.groupby('firmid')['comp_weighted_college'].transform('sum'),\n",
    "            comp_weighted_advanced_degree = lambda x: x['advanced_degree']*x['comp'] /x.groupby('firmid')['comp_weighted_advanced_degree'].transform('sum'),\n",
    "            weighted_college = lambda x: x['college']*x['weight']                    /x.groupby('firmid')['weighted_college'].transform('sum'),\n",
    "            weighted_advanced_degree = lambda x: x['advanced_degree']*x['weight']     /x.groupby('firmid')['weighted_advanced_degree'].transform('sum'),\n",
    "    \n",
    "            comp_weighted_prestige = lambda x: x['prestige'] * x['comp'] / x.groupby('firmid')['comp'].transform('sum'),\n",
    "            weighted_prestige      = lambda x: x['prestige'] * x['weight'] / x.groupby('firmid')['weight'].transform('sum')     \n",
    "           ))\n",
    "    prestige_vars = ['prestige','college', 'advanced_degree']\n",
    "    prestige_vars = [f'comp_weighted_{var}' for var in prestige_vars] + [f'weighted_{var}' for var in prestige_vars]\n",
    "    temp_prestige = temp.groupby('firmid', as_index=False)[prestige_vars].sum()\n",
    "\n",
    "    temp = (temp.melt(id_vars=['firmid', 'comp', 'weight'], value_vars=int_cols,var_name='type',\n",
    "            value_name='valid_flag')\n",
    "     .loc[lambda x: x['valid_flag'].eq(1)]\n",
    "     .groupby(['firmid', 'type'])\n",
    "     .agg(empl=('weight', 'sum'), comp=('comp', 'sum'))\n",
    "     .reset_index()\n",
    "     .pivot(index=['firmid'], columns='type', values=['empl', 'comp']))\n",
    "\n",
    "    temp.columns = [f'{stat}_{vtype}' for stat, vtype in temp.columns]\n",
    "    temp = temp.reset_index()\n",
    "    temp = pd.merge(temp, temp_prestige,  how='outer')\n",
    "    temp = temp.assign(year = year)\n",
    "    return(temp)\n",
    "role_annual_collapsed = pd.concat([collapse_wrapper(year) for year in range(2008, 2024)], ignore_index=True)\n",
    "\n",
    "cols_to_fill = [col for col in role_annual_collapsed.columns\n",
    "                if (col.startswith('empl') or col.startswith('comp'))\n",
    "                and not any(exclude in col for exclude in ['prestige', 'college', 'advanced_degree'])\n",
    "               ]\n",
    "role_annual_collapsed[cols_to_fill] = role_annual_collapsed[cols_to_fill].fillna(0)\n",
    "\n",
    "final_output = (pd.merge(role_annual_collapsed, matched_firm_base_info, how = 'left').\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_wgted_comp_output.parquet'),how = 'left')\n",
    "                .merge(pd.read_parquet(processed_linkedin + 'matched_firm_pca_weight_output.parquet'),how = 'left'))\n",
    "for var in int_cols[1:]: final_output[f'share_comp_{var}'] = final_output[f'comp_{var}'] / final_output['comp_total']\n",
    "final_output.to_parquet(processed_linkedin + 'matched_firm_empl_and_linkedin_characteristics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b191c7d-2559-4548-8316-067a5b518a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1) data/15_revelio_outputs/1_inputs/b_processed_data/linkedin/all_linkedin_matched_firmids_final.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_parquet(processed_linkedin \u001b[38;5;241m+\u001b[39m all_char)\u001b[38;5;241m.\u001b[39mto_parquet(output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15b_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m all_char) \n\u001b[1;32m     12\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_parquet(processed_linkedin \u001b[38;5;241m+\u001b[39m roles_in_ctry)\u001b[38;5;241m.\u001b[39mto_parquet(output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15c_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m roles_in_ctry) \n\u001b[0;32m---> 13\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_parquet(processed_linkedin \u001b[38;5;241m+\u001b[39m all_matched_ids)\u001b[38;5;241m.\u001b[39mto_parquet(output_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15d_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m all_matched_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1) data/15_revelio_outputs/1_inputs/b_processed_data/linkedin/all_linkedin_matched_firmids_final.parquet'"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# Move all files to their final location\n",
    "########################################################################################\n",
    "for_emp = 'matched_firm_foreign_employment.parquet'\n",
    "all_char = 'matched_firm_empl_and_linkedin_characteristics.parquet'\n",
    "roles_in_ctry = 'data_roles_in_all_countries.parquet'\n",
    "all_matched_ids = 'all_linkedin_matched_firmids_final.parquet'\n",
    "output_dir = '1) data/15_revelio_outputs/2_outputs/'\n",
    "\n",
    "pd.read_parquet(processed_linkedin + for_emp).to_parquet(output_dir + '15a_'+ for_emp) \n",
    "pd.read_parquet(processed_linkedin + all_char).to_parquet(output_dir + '15b_'+ all_char) \n",
    "pd.read_parquet(processed_linkedin + roles_in_ctry).to_parquet(output_dir + '15c_'+ roles_in_ctry) \n",
    "pd.read_parquet(processed_linkedin + all_matched_ids).to_parquet(output_dir + '15d_'+ all_matched_ids) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
